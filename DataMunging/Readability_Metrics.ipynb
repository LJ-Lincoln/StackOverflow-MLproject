{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, StanfordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import entropy\n",
    "%matplotlib inline \n",
    "#from textstat.textstat import textstat\n",
    "sns.set_style(\"whitegrid\")\n",
    "os.chdir('/Users/RayLJazz//Dropbox/UCDavis/2016spring/STA208/Final/Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read sql data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM [posts]\"\n",
    "conn = sqlite3.connect('crossvalidated.db')\n",
    "post_df = pd.read_sql_query(query, conn)\n",
    "post_df.head()\n",
    "qpost_df = post_df[post_df.PostTypeId == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75067,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qpost_df['Body'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean body\n",
    "Remove:\n",
    "\n",
    "1.Code lines \n",
    "\n",
    "2.links \n",
    "\n",
    "3.Emails \n",
    "\n",
    "4.Formulas\n",
    "\n",
    "5.\\newline, \\tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Body_clean_puc= pd.read_pickle('Body_clean_puc.pickle')\n",
    "def exact_from_html(row, puc=0):\n",
    "    soup = BeautifulSoup(row, \"html.parser\")\n",
    "\n",
    "    #remove code\n",
    "    #chunck_num = len(soup.find_all('code'))\n",
    "    #i=1\n",
    "    #while True:\n",
    "      #  if i <= chunck_num:\n",
    "            #soup.code.extract()\n",
    "            #i = i + 1\n",
    "        #else:\n",
    "            #break\n",
    "    for tag in soup.find_all('code'):\n",
    "        tag.replaceWith(' ')\n",
    "    ######\n",
    "    raw = soup.get_text()\n",
    "    #remove link\n",
    "    raw_no_link = re.sub(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", '', raw)\n",
    "    #remove email\n",
    "    no_link_email = re.sub(r'[\\w\\.-]+@[\\w\\.-]+[\\.][com|org|ch|uk]{2,3}', \"\", raw_no_link)\n",
    "    #remove whitespace\n",
    "    tab_text = '\\t\\n\\r\\x0b\\x0c'\n",
    "    no_link_email_space = \"\".join([ch for ch in no_link_email if ch not in set(tab_text)])\n",
    "    #remove fomula\n",
    "    reg = '(\\$.+?\\$)|((\\\\\\\\begin\\{.+?\\})(.+?)(\\\\\\\\end\\{(.+?)\\}))'\n",
    "    raw = re.sub(reg, \"\", no_link_email_space, flags=re.IGNORECASE)\n",
    "    #remove numbers\n",
    "    raw = re.sub('[0-9]+?', ' ', raw)\n",
    "    #remove punctuation\n",
    "    #if puc == 1:\n",
    "     #   raw = \"\".join([ch for ch in raw if ch not in set(string.punctuation)])\n",
    "    # remove stop words\n",
    "    #raw = \" \".join([word for word in word_tokenize(raw.lower()) if word not in stopwords.words('english')])\n",
    "    return(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fun_clean = lambda i: exact_from_html(i)\n",
    "Body_clean_puc = qpost_df['Body'].map(fun_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    How should I elicit prior distributions from e...\n",
       "1    In many different statistical methods there is...\n",
       "2    What are some valuable Statistical Analysis op...\n",
       "3    I have two groups of data.  Each with a differ...\n",
       "5    Last year, I read a blog post from Brendan O'C...\n",
       "Name: Body, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Body_clean_puc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('Body_clean_puc.pickle', 'wb') as handle:\n",
    "    pickle.dump(Body_clean_puc, handle,protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact code and formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fomula_num = []\n",
    "Code = []\n",
    "def exact_formula(row):\n",
    "    soup = BeautifulSoup(row, \"html.parser\")\n",
    "    #remove code\n",
    "    chunck_num = len(soup.find_all('code'))\n",
    "    i=1\n",
    "    while True:\n",
    "        if chunck_num == 0:\n",
    "            Code.append(0)\n",
    "            break\n",
    "        elif i <= chunck_num:\n",
    "            code_num = 0\n",
    "            code = soup.code.get_text()\n",
    "            code = code.split('\\n')\n",
    "            codes = [x for x in code if x != '']\n",
    "            code_num = code_num + len(codes)\n",
    "            soup.code.extract()\n",
    "            i = i + 1\n",
    "        else:\n",
    "            Code.append(code_num)\n",
    "            break\n",
    "\n",
    "    ######\n",
    "    raw = soup.get_text()\n",
    "    #remove link\n",
    "    raw_no_link = re.sub(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", '', raw)\n",
    "    #remove email\n",
    "    no_link_email = re.sub(r'[\\w\\.-]+@[\\w\\.-]+[\\.][com|org|ch|uk]{2,3}', \"\", raw_no_link)\n",
    "    #remove whitespace\n",
    "    tab_text = '\\t\\n\\r\\x0b\\x0c'\n",
    "    no_link_email_space = \"\".join([ch for ch in no_link_email if ch not in set(tab_text)])\n",
    "    # remove fomula\n",
    "    reg = '(\\$.+?\\$)|((\\\\\\\\begin\\{.+?\\})(.+?)(\\\\\\\\end\\{(.+?)\\}))'\n",
    "    re_fomula = re.findall(reg, no_link_email_space, flags=re.IGNORECASE)\n",
    "    if re_fomula != []:\n",
    "        for item in re_fomula:\n",
    "            no_link_email_space = no_link_email_space.replace(str(item), '')\n",
    "        fomula_num.append(int(1))\n",
    "    else:\n",
    "        fomula_num.append(int(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fun_clean = lambda i: exact_formula(i)\n",
    "get_result = qpost_df['Body'].map(fun_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    75067.000000\n",
       "mean         2.250110\n",
       "std         16.899923\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          1.000000\n",
       "max       2160.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Code_df = pd.Series(data=Code)\n",
    "Code_df.describe()\n",
    "#Code_df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x136c440d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAECCAYAAADkaECYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122816c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(Code_df, rug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    54362\n",
       "1    20705\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fomula_num_df = pd.Series(data=fomula_num)\n",
    "fomula_num_df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body</th>\n",
       "      <th>Code number</th>\n",
       "      <th>formula</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How should I elicit prior distributions from e...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In many different statistical methods there is...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have two groups of data.  Each with a differ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Last year, I read a blog post from Brendan O'C...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Body  Code number  formula\n",
       "0  How should I elicit prior distributions from e...          0.0      0.0\n",
       "1  In many different statistical methods there is...          0.0      0.0\n",
       "2  What are some valuable Statistical Analysis op...          0.0      0.0\n",
       "3  I have two groups of data.  Each with a differ...          0.0      0.0\n",
       "5  Last year, I read a blog post from Brendan O'C...          0.0      0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Body_clean_puc_df = pd.DataFrame(Body_clean_puc)\n",
    "Body_clean_puc_df['Code number'] = Code_df\n",
    "Body_clean_puc_df['formula'] = fomula_num_df\n",
    "Body_clean_puc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Body_clean_puc_df[Body_clean_puc_df['Code number']>50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/RayLJazz/miniconda3/envs/python2/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/Users/RayLJazz/miniconda3/envs/python2/lib/python2.7/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>LastEditorDisplayName</th>\n",
       "      <th>ClosedDate</th>\n",
       "      <th>CommunityOwnedDate</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>ParentID</th>\n",
       "      <th>LastEditDate</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>...</th>\n",
       "      <th>Title</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>Tags</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>Id</th>\n",
       "      <th>LastActivityDate</th>\n",
       "      <th>Code number</th>\n",
       "      <th>formula</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;p&gt;How should I elicit prior distributions fro...</td>\n",
       "      <td>1850.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>&lt;bayesian&gt;&lt;prior&gt;&lt;elicitation&gt;</td>\n",
       "      <td>2010-07-19T19:12:12.510</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-09-15T21:08:26.077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;p&gt;In many different statistical methods there...</td>\n",
       "      <td>15519.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>88.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-08-07T17:56:44.800</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>What is normality?</td>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>&lt;distributions&gt;&lt;normality&gt;</td>\n",
       "      <td>2010-07-19T19:12:57.157</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2012-11-12T09:21:54.993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;p&gt;What are some valuable Statistical Analysis...</td>\n",
       "      <td>5162.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2010-07-19T19:13:28.577</td>\n",
       "      <td>183.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-02-12T05:50:03.667</td>\n",
       "      <td>4</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>&lt;software&gt;&lt;open-source&gt;</td>\n",
       "      <td>2010-07-19T19:13:28.577</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2013-05-27T14:48:36.927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;p&gt;I have two groups of data.  Each with a dif...</td>\n",
       "      <td>15443.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>&lt;distributions&gt;&lt;statistical-significance&gt;</td>\n",
       "      <td>2010-07-19T19:13:31.617</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010-09-08T03:00:19.690</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;p&gt;Last year, I read a blog post from &lt;a href=...</td>\n",
       "      <td>65657.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2010-08-09T13:05:50.603</td>\n",
       "      <td>22047.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-07T06:38:10.327</td>\n",
       "      <td>6</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learn...</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>&lt;machine-learning&gt;</td>\n",
       "      <td>2010-07-19T19:14:44.080</td>\n",
       "      <td>211.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2015-03-18T12:14:33.740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Body  ViewCount  \\\n",
       "0  <p>How should I elicit prior distributions fro...     1850.0   \n",
       "1  <p>In many different statistical methods there...    15519.0   \n",
       "2  <p>What are some valuable Statistical Analysis...     5162.0   \n",
       "3  <p>I have two groups of data.  Each with a dif...    15443.0   \n",
       "5  <p>Last year, I read a blog post from <a href=...    65657.0   \n",
       "\n",
       "  LastEditorDisplayName ClosedDate       CommunityOwnedDate  LastEditorUserId  \\\n",
       "0                  None       None                     None               NaN   \n",
       "1                  None       None                     None              88.0   \n",
       "2                  None       None  2010-07-19T19:13:28.577             183.0   \n",
       "3                  None       None                     None               NaN   \n",
       "5                  None       None  2010-08-09T13:05:50.603           22047.0   \n",
       "\n",
       "   ParentID             LastEditDate  CommentCount  AnswerCount   ...     \\\n",
       "0       NaN                     None             1          5.0   ...      \n",
       "1       NaN  2010-08-07T17:56:44.800             1          7.0   ...      \n",
       "2       NaN  2011-02-12T05:50:03.667             4         19.0   ...      \n",
       "3       NaN                     None             2          5.0   ...      \n",
       "5       NaN  2013-06-07T06:38:10.327             6         17.0   ...      \n",
       "\n",
       "                                               Title  PostTypeId OwnerUserId  \\\n",
       "0                      Eliciting priors from experts           1         8.0   \n",
       "1                                 What is normality?           1        24.0   \n",
       "2  What are some valuable Statistical Analysis op...           1        18.0   \n",
       "3  Assessing the significance of differences in d...           1        23.0   \n",
       "5  The Two Cultures: statistics vs. machine learn...           1         5.0   \n",
       "\n",
       "                                        Tags             CreationDate  \\\n",
       "0             <bayesian><prior><elicitation>  2010-07-19T19:12:12.510   \n",
       "1                 <distributions><normality>  2010-07-19T19:12:57.157   \n",
       "2                    <software><open-source>  2010-07-19T19:13:28.577   \n",
       "3  <distributions><statistical-significance>  2010-07-19T19:13:31.617   \n",
       "5                         <machine-learning>  2010-07-19T19:14:44.080   \n",
       "\n",
       "   FavoriteCount Id         LastActivityDate  Code number  formula  \n",
       "0           19.0  1  2010-09-15T21:08:26.077          0.0      0.0  \n",
       "1            9.0  2  2012-11-12T09:21:54.993          0.0      0.0  \n",
       "2           38.0  3  2013-05-27T14:48:36.927          0.0      0.0  \n",
       "3            4.0  4  2010-09-08T03:00:19.690          0.0      0.0  \n",
       "5          211.0  6  2015-03-18T12:14:33.740          0.0      0.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qpost_df['Code number'] = Code_df\n",
    "qpost_df['formula'] = fomula_num_df\n",
    "qpost_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = Body_clean_puc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'In many different statistical methods there is an \"assumption of normality\".  What is \"normality\" and how do I know if there is normality?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readablity Function--Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# words count\n",
    "def word_num(text):\n",
    "    text = \"\".join(x for x in text if x not in list(set(string.punctuation)))\n",
    "    text = word_tokenize(text)\n",
    "    return (len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_num(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68    552\n",
       "65    545\n",
       "51    545\n",
       "62    543\n",
       "69    539\n",
       "Name: Body, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Words_num = Body_clean_puc_df['Body'].apply(word_num)\n",
    "Words_num_df = pd.Series(data=Words_num)\n",
    "Words_num_df.value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sentence count\n",
    "def sent_num(text):\n",
    "    return (len(sent_tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    10754\n",
       "4    10176\n",
       "2     9447\n",
       "5     8749\n",
       "6     6880\n",
       "Name: Body, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sents_num = Body_clean_puc_df['Body'].apply(sent_num)\n",
    "Sents_num_df = pd.Series(data=Sents_num)\n",
    "Sents_num_df.value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## syllables count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# syllables count The \"Written Method\" Rules:http://www.howmanysyllables.com/howtocountsyllables\n",
    "def syllable_num(text):\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    text = text.lower()\n",
    "    text = \"\".join(x for x in text if x not in list(set(string.punctuation)))\n",
    "\n",
    "    if text == None:\n",
    "        return 0\n",
    "    elif len(text) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        if text[0] in vowels: \n",
    "            count += 1\n",
    "        for index in range(1, len(text)):\n",
    "            if text[index] in vowels and text[index-1] not in vowels:\n",
    "                count += 1\n",
    "        if text.endswith('e'): \n",
    "            count -= 1\n",
    "        if text.endswith('le'):\n",
    "            count += 1\n",
    "        if text.endswith('les'):\n",
    "            count += 1\n",
    "        if count == 0:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syllable_num(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## polysyllables count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# polysyllables count\n",
    "def polysyllab_num(text):\n",
    "    count = 0\n",
    "    for word in text.split():\n",
    "        wrds = syllable_num(word)\n",
    "        if wrds >= 3:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polysyllab_num(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characters count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# characters count\n",
    "def char_num(text, ignore_spaces=True):\n",
    "    if ignore_spaces:\n",
    "        text = text.replace(\" \", \"\")\n",
    "    text = \"\".join(x for x in text if x not in list(set(string.punctuation)))\n",
    "    return len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_num(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "easy_words = \"\"\"a able aboard about above absent accept accident account ache aching acorn acre across act acts add address admire adventure afar\tafraid after afternoon afterward afterwards again against age aged ago agree ah ahead aid aim air airfield airplane airport airship airy\talarm alike alive all alley alligator allow almost alone along aloud already also always am America American among amount an and\tangel anger angry animal another answer ant any anybody anyhow anyone anything anyway anywhere apart apartment ape apiece appear apple April\tapron are aren't arise arithmetic arm armful army arose around arrange arrive arrived arrow art artist as ash ashes aside ask\tasleep at ate attack attend attention August aunt author auto automobile autumn avenue awake awaken away awful awfully awhile ax axe baa babe babies back background backward backwards bacon bad badge badly bag bake baker bakery baking ball balloon banana band bandage bang banjo bank banker bar barber bare barefoot barely bark barn barrel base baseball basement basket bat batch bath bathe bathing bathroom bathtub\tbattle battleship bay be beach bead beam bean bear beard beast beat beating beautiful beautify beauty became because become becoming bed bedbug bedroom bedspread bedtime bee beech beef beefsteak beehive been beer beet before beg began beggar begged begin beginning begun behave behind being\tbelieve bell belong below belt bench bend beneath bent berries berry beside besides best bet better between bib bible bicycle bid big bigger bill billboard bin bind bird birth birthday biscuit bit bite biting bitter black blackberry blackbird blackboard blackness blacksmith blame blank blanket\tblast blaze bleed bless blessing blew blind blindfold blinds block blood bloom blossom blot blow blue blueberry bluebird blush board boast boat bob bobwhite bodies body boil boiler bold bone bonnet boo book bookcase bookkeeper boom boot born borrow boss both bother bottle bottom\tbought bounce bow bowl bow-wow box boxcar boxer boxes boy boyhood bracelet brain brake bran branch brass brave bread break breakfast breast breath breathe breeze brick bride bridge bright brightness bring broad broadcast broke broken brook broom brother brought brown brush bubble bucket buckle\tbud buffalo bug buggy build building built bulb bull bullet bum bumblebee bump bun bunch bundle bunny burn burst bury bus bush bushel business busy but butcher butt butter buttercup butterfly buttermilk butterscotch button buttonhole buy buzz by bye cab cabbage cabin cabinet cackle cage cake calendar calf call caller calling came camel camp campfire can canal canary candle candlestick candy cane cannon cannot canoe can't canyon cap cape capital captain car card cardboard care careful careless carelessness carload carpenter carpet carriage carrot carry cart\tcarve case cash cashier castle cat catbird catch catcher caterpillar catfish catsup cattle caught cause cave ceiling cell cellar cent center cereal certain certainly chain chair chalk champion chance change chap charge charm chart chase chatter cheap cheat check checkers cheek cheer cheese cherry chest chew\tchick chicken chief child childhood children chill chilly chimney chin china chip chipmunk chocolate choice choose chop chorus chose chosen christen Christmas church churn cigarette circle circus citizen city clang clap class classmate classroom claw clay clean cleaner clear clerk clever click cliff climb clip cloak\tclock close closet cloth clothes clothing cloud cloudy clover clown club cluck clump coach coal coast coat cob cobbler cocoa coconut cocoon cod codfish coffee coffeepot coin cold collar college color colored colt column comb come comfort comic coming company compare conductor cone connect coo cook\tcooked cooking cookie cookies cool cooler coop copper copy cord cork corn corner correct cost cot cottage cotton couch cough could couldn't count counter country county course court cousin cover cow coward cowardly cowboy cozy crab crack cracker cradle cramps cranberry crank cranky crash crawl crazy\tcream creamy creek creep crept cried croak crook crooked crop cross crossing cross-eyed crow crowd crowded crown cruel crumb crumble crush crust cry cries cub cuff cup cuff cup cupboard cupful cure curl curly curtain curve cushion custard customer cut cute cutting dab dad daddy daily dairy daisy dam damage dame damp dance dancer dancing dandy danger dangerous dare dark darkness darling darn dart dash date daughter\tdawn day daybreak daytime dead deaf deal dear death December decide deck deed deep deer defeat defend defense delight den dentist depend deposit describe desert\tdeserve desire desk destroy devil dew diamond did didn't die died dies difference different dig dim dime dine ding-dong dinner dip direct direction dirt dirty\tdiscover dish dislike dismiss ditch dive diver divide do dock doctor does doesn't dog doll dollar dolly done donkey don't door doorbell doorknob doorstep dope\tdot double dough dove down downstairs downtown dozen drag drain drank draw drawer draw drawing dream dress dresser dressmaker drew dried drift drill drink drip\tdrive driven driver drop drove drown drowsy drub drum drunk dry duck due dug dull dumb dump during dust dusty duty dwarf dwell dwelt dying each eager eagle ear early earn earth east eastern easy eat eaten\tedge egg eh eight eighteen eighth eighty either elbow elder eldest electric\telectricity elephant eleven elf elm else elsewhere empty end ending enemy engine\tengineer English enjoy enough enter envelope equal erase eraser errand escape eve\teven evening ever every everybody everyday everyone everything everywhere evil exact except\texchange excited exciting excuse exit expect explain extra eye eyebrow fable face facing fact factory fail faint fair fairy faith fake fall false family fan fancy far faraway fare farmer farm farming far-off farther fashion fast fasten fat father\tfault favor favorite fear feast feather February fed feed feel feet fell fellow felt fence fever few fib fiddle field fife fifteen fifth fifty fig fight figure file fill\tfilm finally find fine finger finish fire firearm firecracker fireplace fireworks firing first fish fisherman fist fit fits five fix flag flake flame flap flash flashlight flat flea flesh\tflew flies flight flip flip-flop float flock flood floor flop flour flow flower flowery flutter fly foam fog foggy fold folks follow following fond food fool foolish foot football\tfootprint for forehead forest forget forgive forgot forgotten fork form fort forth fortune forty forward fought found fountain four fourteen fourth fox frame free freedom freeze freight French fresh\tfret Friday fried friend friendly friendship frighten frog from front frost frown froze fruit fry fudge fuel full fully fun funny fur furniture further fuzzy gain gallon gallop game gang garage garbage garden gas gasoline gate gather gave gay gear geese general gentle gentleman gentlemen\tgeography get getting giant gift gingerbread girl give given giving glad gladly glance glass glasses gleam glide glory glove glow\tglue go going goes goal goat gobble God god godmother gold golden goldfish golf gone good goods goodbye good-by goodbye\tgood-bye good-looking goodness goody goose gooseberry got govern government gown grab gracious grade grain grand grandchild grandchildren granddaughter grandfather grandma\tgrandmother grandpa grandson grandstand grape grapes grapefruit grass grasshopper grateful grave gravel graveyard gravy gray graze grease great green greet\tgrew grind groan grocery ground group grove grow guard guess guest guide gulf gum gun gunpowder guy ha habit had hadn't hail hair haircut hairpin half hall halt ham hammer hand handful handkerchief handle handwriting hang happen happily happiness happy harbor hard hardly hardship hardware hare hark\tharm harness harp harvest has hasn't haste hasten hasty hat hatch hatchet hate haul have haven't having hawk hay hayfield haystack he head headache heal health healthy heap hear hearing\theard heart heat heater heaven heavy he'd heel height held hell he'll hello helmet help helper helpful hem hen henhouse her hers herd here here's hero herself he's hey hickory\thid hidden hide high highway hill hillside hilltop hilly him himself hind hint hip hire his hiss history hit hitch hive ho hoe hog hold holder hole holiday hollow holy\thome homely homesick honest honey honeybee honeymoon honk honor hood hoof hook hoop hop hope hopeful hopeless horn horse horseback horseshoe hose hospital host hot hotel hound hour house housetop\thousewife housework how however howl hug huge hum humble hump hundred hung hunger hungry hunk hunt hunter hurrah hurried hurry hurt husband hush hut hymn I ice icy I'd idea ideal if ill\tI'll I'm important impossible improve in inch inches\tincome indeed Indian indoors ink inn insect inside\tinstant instead insult intend interested interesting into invite\tiron is island isn't it its it's itself\tI've ivory ivy jacket jacks jail jam January jar\tjaw jay jelly jellyfish jerk jig\tjob jockey join joke joking jolly\tjourney joy joyful joyous judge jug\tjuice juicy July jump June junior\tjunk just keen keep kept kettle key\tkick kid kill killed kind\tkindly kindness king kingdom kiss\tkitchen kite kitten kitty knee\tkneel knew knife knit knives\tknob knock knot know known lace lad ladder ladies lady laid lake lamb lame lamp land lane language lantern lap lard large lash lass last\tlate laugh laundry law lawn lawyer lay lazy lead leader leaf leak lean leap learn learned least leather leave leaving\tled left leg lemon lemonade lend length less lesson let let's letter letting lettuce level liberty library lice lick lid\tlie life lift light lightness lightning like likely liking lily limb lime limp line linen lion lip list listen lit\tlittle live lives lively liver living lizard load loaf loan loaves lock locomotive log lone lonely lonesome long look lookout\tloop loose lord lose loser loss lost lot loud love lovely lover low luck lucky lumber lump lunch lying machine machinery mad made magazine magic maid mail mailbox mailman major make making male mama mamma man manager mane manger many map\tmaple marble march March mare mark market marriage married marry mask mast master mat match matter mattress may May maybe mayor maypole me\tmeadow meal mean means meant measure meat medicine meet meeting melt member men mend meow merry mess message met metal mew mice middle\tmidnight might mighty mile milk milkman mill miler million mind mine miner mint minute mirror mischief miss Miss misspell mistake misty mitt mitten\tmix moment Monday money monkey month moo moon moonlight moose mop more morning morrow moss most mostly mother motor mount mountain mouse mouth\tmove movie movies moving mow Mr. Mrs. much mud muddy mug mule multiply murder music must my myself nail name nap napkin narrow nasty naughty navy near nearby\tnearly neat neck necktie need needle needn't Negro neighbor neighborhood\tneither nerve nest net never nevermore new news newspaper next\tnibble nice nickel night nightgown nine nineteen ninety no nobody\tnod noise noisy none noon nor north northern nose not\tnote nothing notice November now nowhere number nurse nut oak oar oatmeal oats obey ocean o'clock October odd of off\toffer office officer often oh oil old old-fashioned on once one\tonion only onward open or orange orchard order ore organ other\totherwise ouch ought our ours ourselves out outdoors outfit outlaw outline\toutside outward oven over overalls overcoat overeat overhead overhear overnight overturn\towe owing owl own owner ox pa pace pack package pad page paid pail pain painful paint painter painting pair pal palace pale pan pancake pane pansy pants papa paper parade pardon parent park part partly partner party\tpass passenger past paste pasture pat patch path patter pave pavement paw pay payment pea peas peace peaceful peach peaches peak peanut pear pearl peck peek peel peep peg pen pencil penny\tpeople pepper peppermint perfume perhaps person pet phone piano pick pickle picnic picture pie piece pig pigeon piggy pile pill pillow pin pine pineapple pink pint pipe pistol pit pitch pitcher pity\tplace plain plan plane plant plate platform platter play player playground playhouse playmate plaything pleasant please pleasure plenty plow plug plum pocket pocketbook poem point poison poke pole police policeman polish polite\tpond ponies pony pool poor pop popcorn popped porch pork possible post postage postman pot potato potatoes pound pour powder power powerful praise pray prayer prepare present pretty price prick prince princess\tprint prison prize promise proper protect proud prove prune public puddle puff pull pump pumpkin punch punish pup pupil puppy pure purple purse push puss pussy pussycat put putting puzzle quack quart\tquarter queen\tqueer question\tquick quickly\tquiet quilt\tquit quite rabbit race rack radio radish rag rail railroad railway rain rainy rainbow raise raisin rake ram ran ranch rang rap rapidly\trat rate rather rattle raw ray reach read reader reading ready real really reap rear reason rebuild receive recess record red\tredbird redbreast refuse reindeer rejoice remain remember remind remove rent repair repay repeat report rest return review reward rib ribbon rice\trich rid riddle ride rider riding right rim ring rip ripe rise rising river road roadside roar roast rob robber robe\trobin rock rocky rocket rode roll roller roof room rooster root rope rose rosebud rot rotten rough round route row rowboat\troyal rub rubbed rubber rubbish rug rule ruler rumble run rung runner running rush rust rusty rye sack sad saddle sadness safe safety said sail sailboat sailor saint salad sale salt same sand sandy sandwich sang sank sap sash sat satin satisfactory Saturday sausage savage save savings saw say scab scales scare scarf school schoolboy schoolhouse schoolmaster schoolroom scorch score scrap scrape scratch scream screen screw scrub sea seal seam search season seat second secret see seeing seed seek seem seen seesaw select self selfish\tsell send sense sent sentence separate September servant serve service set setting settle settlement seven seventeen seventh seventy several sew shade shadow shady shake shaker shaking shall shame shan't shape share sharp shave she she'd she'll she's shear shears shed sheep sheet shelf shell shepherd shine shining shiny ship shirt shock shoe shoemaker shone shook shoot shop shopping shore short shot should shoulder shouldn't shout shovel show shower\tshut shy sick sickness side sidewalk sideways sigh sight sign silence silent silk sill silly silver simple sin since sing singer single sink sip sir sis sissy sister sit sitting six sixteen sixth sixty size skate skater ski skin skip skirt sky slam slap slate slave sled sleep sleepy sleeve sleigh slept slice slid slide sling slip slipped slipper slippery slit slow slowly sly smack small smart smell\tsmile smoke smooth snail snake snap snapping sneeze snow snowy snowball snowflake snuff snug so soak soap sob socks sod soda sofa soft soil sold soldier sole some somebody somehow someone something sometime sometimes somewhere son song soon sore sorrow sorry sort soul sound soup sour south southern space spade spank sparrow speak speaker spear speech speed spell spelling spend spent spider spike spill spin spinach spirit spit\tsplash spoil spoke spook spoon sport spot spread spring springtime sprinkle square squash squeak squeeze squirrel stable stack stage stair stall stamp stand star stare start starve state station stay steak steal steam steamboat steamer steel steep steeple steer stem step stepping stick sticky stiff still stillness sting stir stitch stock stocking stole stone stood stool stoop stop stopped stopping store stork stories storm stormy story stove straight\tstrange stranger strap straw strawberry stream street stretch string strip stripes strong stuck study stuff stump stung subject such suck sudden suffer sugar suit sum summer sun Sunday sunflower sung sunk sunlight sunny sunrise sunset sunshine supper suppose sure surely surface surprise swallow swam swamp swan swat swear sweat sweater sweep sweet sweetness sweetheart swell swept swift swim swimming swing switch sword swore table tablecloth tablespoon tablet tack tag tail tailor take taken taking tale talk talker tall tame tan tank tap tape tar tardy task taste taught tax tea teach teacher team tear\ttease teaspoon teeth telephone tell temper ten tennis tent term terrible test than thank thanks thankful Thanksgiving that that's the theater thee their them then there these they they'd they'll they're\tthey've thick thief thimble thin thing think third thirsty thirteen thirty this thorn those though thought thousand thread three threw throat throne through throw thrown thumb thunder Thursday thy tick ticket\ttickle tie tiger tight till time tin tinkle tiny tip tiptoe tire tired title to toad toadstool toast tobacco today toe together toilet told tomato tomorrow ton tone tongue tonight too\ttook tool toot tooth toothbrush toothpick top tore torn toss touch tow toward towards towel tower town toy trace track trade train tramp trap tray treasure treat tree trick tricycle tried\ttrim trip trolley trouble truck true truly trunk trust truth try tub Tuesday tug tulip tumble tune tunnel turkey turn turtle twelve twenty twice twig twin two ugly umbrella uncle under understand underwear\tundress unfair unfinished unfold unfriendly unhappy\tunhurt uniform United States unkind unknown\tunless unpleasant until unwilling up upon\tupper upset upside upstairs uptown upward\tus use used useful valentine valley valuable\tvalue vase vegetable\tvelvet very vessel\tvictory view village\tvine violet visit\tvisitor voice vote wag wagon waist wait wake waken walk wall walnut want war warm warn was wash washer washtub wasn't waste watch watchman water watermelon waterproof wave wax\tway wayside we weak weakness weaken wealth weapon wear weary weather weave web we'd wedding Wednesday wee weed week we'll weep weigh welcome well went were\twe're west western wet we've whale what what's wheat wheel when whenever where which while whip whipped whirl whisky whiskey whisper whistle white who who'd whole\twho'll whom who's whose why wicked wide wife wiggle wild wildcat will willing willow win wind windy windmill window wine wing wink winner winter wipe wire\twise wish wit witch with without woke wolf woman women won wonder wonderful won't wood wooden woodpecker woods wool woolen word wore work worker workman world\tworm worn worry worse worst worth would wouldn't wound wove wrap wrapped wreck wren wring write writing written wrong wrote wrung yard yarn year yell\tyellow yes yesterday yet\tyolk yonder you you'd\tyou'll young youngster your\tyours you're yourself yourselves\tyouth you've\"\"\"\n",
    "easy_word_set = set(easy_words.split())\n",
    "def complex_words_num(text):\n",
    "\ttext_list = text.split()\n",
    "\tdiff_words_set = set()\n",
    "\tfor value in text_list:\n",
    "\t\tif value not in easy_word_set:\n",
    "\t\t\tif syllable_num(value) > 1:\n",
    "\t\t\t\tif value not in diff_words_set:\n",
    "\t\t\t\t\tdiff_words_set.add(value)\n",
    "\treturn len(diff_words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_words_num(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some useful function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_sent(text): #avg_sentence_length\n",
    "    lc = word_num(text)\n",
    "    sc = sent_num(text)\n",
    "    return float(lc)/float(sc)\n",
    "\n",
    "\n",
    "def syll_word(text): #avg_syllables_per_word\n",
    "    syllable = syllable_num(text)\n",
    "    words = word_num(text)\n",
    "    ASPW = float(syllable)/float(words)\n",
    "    return ASPW\n",
    "\n",
    "\n",
    "\n",
    "def char_word(text): #avg_letter_per_word\n",
    "    ALPW = float(float(char_num(text))/float(word_num(text)))\n",
    "    return ALPW\n",
    "\n",
    "\n",
    "\n",
    "def word_sent(text):\n",
    "    AWPS = float(float(word_num(text))/float(sent_num(text)))\n",
    "    return AWPS\n",
    "\n",
    "def sent_word(text): #avg_sentence_per_word\n",
    "    ASPW = float(float(sent_num(text))/float(word_num(text)))\n",
    "    return ASPW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text=test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#word_sent(text)\n",
    "#syll_word(text)\n",
    "#char_word(text)\n",
    "#sent_word(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readablity Function--Advance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Automated Reading Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Automated_Readability_Index(text):\n",
    "    try:\n",
    "        a = char_word(text)\n",
    "        b = word_sent(text)\n",
    "        ARI = (4.71 * round(a, 2)) + (0.5*round(b, 2)) - 21.43\n",
    "        return round(ARI, 2)\n",
    "    except:\n",
    "        return 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.65"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Automated_Readability_Index(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ARI = Body_clean_puc_df['Body'].apply(Automated_Readability_Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.80     89\n",
       "10.23    89\n",
       "11.61    87\n",
       "9.91     85\n",
       "11.01    84\n",
       "Name: Body, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARI.value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coleman Liau Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Coleman_Liau_Index(text):\n",
    "    try:\n",
    "        L = char_word(text)*100\n",
    "        S = sent_word(text)*100\n",
    "        CLI = float((0.058 * L) - (0.296 * S) - 15.8)\n",
    "        return round(CLI, 2)\n",
    "    except:\n",
    "        return 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.11"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Coleman_Liau_Index(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CLI = Body_clean_puc_df['Body'].apply(Coleman_Liau_Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.27    156\n",
       "10.25    146\n",
       "7.37     145\n",
       "9.58     144\n",
       "10.52    143\n",
       "Name: Body, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLI.value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flesch Kincaid Grade Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Flesch_Kincaid_Grade(text):\n",
    "    try:\n",
    "        ASL = word_sent(text)\n",
    "        ASW = syll_word(text)\n",
    "        FKRA = float(0.39 * ASL) + float(11.8 * ASW) - 15.59\n",
    "        return round(FKRA, 2)\n",
    "    except:\n",
    "        return 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.96"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Flesch_Kincaid_Grade(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FKG = Body_clean_puc_df['Body'].apply(Flesch_Kincaid_Grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.83    150\n",
       "11.68    147\n",
       "12.27    147\n",
       "9.08     141\n",
       "13.37    131\n",
       "Name: Body, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FKG.value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flesch reading ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Flesch_Reading_Ease(text):\n",
    "    try:\n",
    "        ASL = word_sent(text)\n",
    "        ASW = syll_word(text)\n",
    "        FRE = 206.835 - float(1.015 * ASL) - float(84.6 * ASW)\n",
    "        return round(FRE, 2)\n",
    "    except:\n",
    "        return 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Flesch_Reading_Ease(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FRE = Body_clean_puc_df['Body'].apply(Flesch_Reading_Ease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.33    92\n",
       "47.83    86\n",
       "35.95    79\n",
       "48.64    71\n",
       "32.56    64\n",
       "Name: Body, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FRE.value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gunning fog index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Gunning_Fog(text):\n",
    "    try:\n",
    "        per_diff_words = (complex_words_num(test_data)/word_num(text)*100)\n",
    "        grade = 0.4*(word_sent(text) + per_diff_words)\n",
    "        return round(grade,2)\n",
    "    except:\n",
    "        return 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.6"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gunning_Fog(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GFI = Body_clean_puc_df['Body'].apply(Gunning_Fog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.4    885\n",
       "7.2    875\n",
       "7.6    863\n",
       "6.8    861\n",
       "6.0    815\n",
       "Name: Body, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GFI.value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Metric_Entropy(original): #original body with tags\n",
    "    try:\n",
    "        sentropy = entropy.shannon_entropy(original)\n",
    "        body_length = len(original)\n",
    "        ME = float(sentropy/body_length)\n",
    "        return ME\n",
    "    except:\n",
    "        return 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0036979012642914828"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Metric_Entropy(qpost_df['Body'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ME = qpost_df['Body'].apply(Metric_Entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN                  5249\n",
       "0.00134663514109        2\n",
       "0.000431743403253       2\n",
       "0.000899450906126       2\n",
       "0.00114828761518        2\n",
       "Name: Body, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ME.value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOC Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def LOC_ptg(data):\n",
    "    try:\n",
    "        LOC = data['Code number']\n",
    "        LOB = sent_num(data['Body'])\n",
    "        LP = float(LOC/(LOC+LOB))\n",
    "        return LP\n",
    "    except:\n",
    "        return 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NaN'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOC_ptg(qpost_df[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOC_Per = qpost_df.apply(LOC_ptg,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000000    24608\n",
       "0.250000      640\n",
       "0.333333      589\n",
       "0.500000      559\n",
       "0.200000      546\n",
       "dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOC_Per.value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Body_clean_puc_df['Id'] = qpost_df['Id']\n",
    "Body_clean_puc_df['word_count'] = Words_num_df\n",
    "Body_clean_puc_df['sentence_count'] = Sents_num_df\n",
    "Body_clean_puc_df['ARI'] = ARI\n",
    "Body_clean_puc_df['CLI'] = CLI\n",
    "Body_clean_puc_df['FKG'] = FKG\n",
    "Body_clean_puc_df['FRE'] = FRE\n",
    "Body_clean_puc_df['GFI'] = GFI\n",
    "Body_clean_puc_df['M_Entropy'] = ME\n",
    "Body_clean_puc_df['LOC_Per'] = LOC_Per\n",
    "Body_clean_puc_df['Score'] = qpost_df['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body</th>\n",
       "      <th>Code number</th>\n",
       "      <th>formula</th>\n",
       "      <th>Id</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>ARI</th>\n",
       "      <th>CLI</th>\n",
       "      <th>FKG</th>\n",
       "      <th>FRE</th>\n",
       "      <th>GFI</th>\n",
       "      <th>M_Entropy</th>\n",
       "      <th>LOC_Per</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How should I elicit prior distributions from e...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>10.41</td>\n",
       "      <td>13.15</td>\n",
       "      <td>9.45</td>\n",
       "      <td>50.47</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.00613826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In many different statistical methods there is...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>6.65</td>\n",
       "      <td>9.11</td>\n",
       "      <td>10.96</td>\n",
       "      <td>37</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.0036979</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>18.55</td>\n",
       "      <td>14.58</td>\n",
       "      <td>18.51</td>\n",
       "      <td>14.03</td>\n",
       "      <td>11.6</td>\n",
       "      <td>0.00275086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have two groups of data.  Each with a differ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>81</td>\n",
       "      <td>5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>9.94</td>\n",
       "      <td>10.98</td>\n",
       "      <td>45.21</td>\n",
       "      <td>6.48</td>\n",
       "      <td>0.00110337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Last year, I read a blog post from Brendan O'C...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>180</td>\n",
       "      <td>7</td>\n",
       "      <td>16.76</td>\n",
       "      <td>14.27</td>\n",
       "      <td>16.27</td>\n",
       "      <td>24.23</td>\n",
       "      <td>10.29</td>\n",
       "      <td>0.000372775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Body  Code number  formula  \\\n",
       "0  How should I elicit prior distributions from e...          0.0      0.0   \n",
       "1  In many different statistical methods there is...          0.0      0.0   \n",
       "2  What are some valuable Statistical Analysis op...          0.0      0.0   \n",
       "3  I have two groups of data.  Each with a differ...          0.0      0.0   \n",
       "5  Last year, I read a blog post from Brendan O'C...          0.0      0.0   \n",
       "\n",
       "   Id  word_count  sentence_count    ARI    CLI    FKG    FRE    GFI  \\\n",
       "0   1          13               1  10.41  13.15   9.45  50.47    5.2   \n",
       "1   2          23               2   6.65   9.11  10.96     37    4.6   \n",
       "2   3          29               1  18.55  14.58  18.51  14.03   11.6   \n",
       "3   4          81               5   9.04   9.94  10.98  45.21   6.48   \n",
       "5   6         180               7  16.76  14.27  16.27  24.23  10.29   \n",
       "\n",
       "     M_Entropy  LOC_Per  Score  \n",
       "0   0.00613826      0.0     31  \n",
       "1    0.0036979      0.0     26  \n",
       "2   0.00275086      0.0     63  \n",
       "3   0.00110337      0.0     15  \n",
       "5  0.000372775      0.0    226  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Body_clean_puc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('Readability_Metrics.pickle', 'wb') as handle:\n",
    "    pickle.dump(Body_clean_puc_df, handle,protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Code number</th>\n",
       "      <th>formula</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>ARI</th>\n",
       "      <th>CLI</th>\n",
       "      <th>FKG</th>\n",
       "      <th>FRE</th>\n",
       "      <th>GFI</th>\n",
       "      <th>M_Entropy</th>\n",
       "      <th>LOC_Per</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>10.41</td>\n",
       "      <td>13.15</td>\n",
       "      <td>9.45</td>\n",
       "      <td>50.47</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.00613826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>6.65</td>\n",
       "      <td>9.11</td>\n",
       "      <td>10.96</td>\n",
       "      <td>37</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.0036979</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>18.55</td>\n",
       "      <td>14.58</td>\n",
       "      <td>18.51</td>\n",
       "      <td>14.03</td>\n",
       "      <td>11.6</td>\n",
       "      <td>0.00275086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81</td>\n",
       "      <td>5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>9.94</td>\n",
       "      <td>10.98</td>\n",
       "      <td>45.21</td>\n",
       "      <td>6.48</td>\n",
       "      <td>0.00110337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>180</td>\n",
       "      <td>7</td>\n",
       "      <td>16.76</td>\n",
       "      <td>14.27</td>\n",
       "      <td>16.27</td>\n",
       "      <td>24.23</td>\n",
       "      <td>10.29</td>\n",
       "      <td>0.000372775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Code number  formula  word_count  sentence_count    ARI    CLI    FKG  \\\n",
       "0   1          0.0      0.0          13               1  10.41  13.15   9.45   \n",
       "1   2          0.0      0.0          23               2   6.65   9.11  10.96   \n",
       "2   3          0.0      0.0          29               1  18.55  14.58  18.51   \n",
       "3   4          0.0      0.0          81               5   9.04   9.94  10.98   \n",
       "5   6          0.0      0.0         180               7  16.76  14.27  16.27   \n",
       "\n",
       "     FRE    GFI    M_Entropy  LOC_Per  Score  \n",
       "0  50.47    5.2   0.00613826      0.0     31  \n",
       "1     37    4.6    0.0036979      0.0     26  \n",
       "2  14.03   11.6   0.00275086      0.0     63  \n",
       "3  45.21   6.48   0.00110337      0.0     15  \n",
       "5  24.23  10.29  0.000372775      0.0    226  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = ['Id','Code number','formula','word_count','sentence_count','ARI','CLI','FKG','FRE','GFI','M_Entropy','LOC_Per','Score']\n",
    "RM = Body_clean_puc_df[target]\n",
    "RM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('Readability_Metrics_wtbody.pickle', 'wb') as handle:\n",
    "    pickle.dump(RM, handle,protocol=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
