{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.core.display import display, HTML\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('../Data/crossvalidated.db')\n",
    "# return all the records for questions posts from posts table\n",
    "ques_query = \"SELECT * FROM [posts] WHERE PostTypeId==2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apost_df = pd.read_sql_query(ques_query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apost_df.drop(['LastEditorDisplayName','CommunityOwnedDate','LastEditorUserId','LastEditDate',\n",
    "             'LastActivityDate'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53189    <p>What a great question- it's a chance to show how one would inspect the drawbacks and assumptions of any statistical method.  Namely: make up some data and try the algorithm on it!</p>\\n\\n<p>We'll consider two of your assumptions, and we'll see what happens to the k-means algorithm when those assumptions are broken. We'll stick to 2-dimensional data since it's easy to visualize. (Thanks to the <a href=\"http://en.wikipedia.org/wiki/Curse_of_dimensionality\">curse of dimensionality</a>, adding additional dimensions is likely to make these problems more severe, not less). We'll work with the statistical programming language R: you can find the full code <a href=\"https://github.com/dgrtwo/dgrtwo.github.com/blob/master/_R/2015-01-16-kmeans-free-lunch.Rmd\">here</a> (and the post in blog form <a href=\"http://varianceexplained.org/r/kmeans-free-lunch/\">here</a>).</p>\\n\\n<h3>Diversion: Anscombe's Quartet</h3>\\n\\n<p>First, an analogy. Imagine someone argued the following:</p>\\n\\n<blockquote>\\n  <p>I read some material about the drawbacks of linear regression- that it expects a linear trend, that the residuals are normally distributed, and that there are no outliers. But all linear regression is doing is minimizing the sum of squared errors (SSE) from the predicted line. That's an optimization problem that can be solved no matter what the shape of the curve or the distribution of the residuals is. Thus, linear regression requires no assumptions to work.</p>\\n</blockquote>\\n\\n<p>Well, yes, linear regression works by minimizing the sum of squared residuals. But that by itself is not the goal of a regression: what we're <em>trying</em> to do is draw a line that serves as a reliable, unbiased predictor of <em>y</em> based on <em>x</em>. The <a href=\"http://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem\">Gauss-Markov theorem</a> tells us that minimizing the SSE accomplishes that goal- but that theorem rests on some very specific assumptions. If those assumptions are broken, you can still minimize the SSE, but it might not <em>do</em> anything. Imagine saying \"You drive a car by pushing the pedal: driving is essentially a 'pedal-pushing process.' The pedal can be pushed no matter how much gas in the tank. Therefore, even if the tank is empty, you can still push the pedal and drive the car.\"</p>\\n\\n<p>But talk is cheap. Let's look at the cold, hard, data. Or actually, made-up data.</p>\\n\\n<p><img src=\"http://varianceexplained.org/figs/2015-01-16-kmeans-free-lunch/anscombe-1.png\" alt=\"center\"> </p>\\n\\n<p>This is in fact my <em>favorite</em> made-up data: <a href=\"http://en.wikipedia.org/wiki/Anscombe%27s_quartet\">Anscombe's Quartet</a>. Created in 1973 by statistician Francis Anscombe, this delightful concoction illustrates the folly of trusting statistical methods blindly. Each of the datasets has the same linear regression slope, intercept, p-value and $R^2$- and yet at a glance we can see that only one of them, <strong>I</strong>, is appropriate for linear regression. In <strong>II</strong> it suggests the wrong shape, in <strong>III</strong> it is skewed by a single outlier- and in <strong>IV</strong> there is clearly no trend at all!</p>\\n\\n<p>One could say \"Linear regression is still <em>working</em> in those cases, because it's minimizing the sum of squares of the residuals.\" But what a <a href=\"http://en.wikipedia.org/wiki/Pyrrhic_victory\">Pyrrhic victory</a>! Linear regression will always draw a line, but if it's a meaningless line, who cares?</p>\\n\\n<p>So now we see that just because an optimization can be performed doesn't mean we're accomplishing our goal. And we see that making up data, and visualizing it, is a good way to inspect the assumptions of a model. Hang on to that intuition, we're going to need it in a minute.</p>\\n\\n<h3>Broken Assumption: Non-Spherical Data</h3>\\n\\n<p>You argue that the k-means algorithm will work fine on non-spherical clusters. Non-spherical clusters like... these?</p>\\n\\n<p><img src=\"http://varianceexplained.org/figs/2015-01-16-kmeans-free-lunch/non_spherical-1.png\" alt=\"center\"> </p>\\n\\n<p>Maybe this isn't what you were expecting- but it's a perfectly reasonable way to construct clusters. Looking at this image, we humans <em>immediately</em> recognize two natural groups of points- there's no mistaking them. So let's see how k-means does: assignments are shown in color, imputed centers are shown as X's.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/SlpL1.png\" alt=\"enter image description here\"></p>\\n\\n<p>Well, <em>that</em>'s not right. K-means was trying to fit a <a href=\"http://en.wikipedia.org/wiki/Square_peg_in_a_round_hole\">square peg in a round hole</a>- trying to find nice centers with neat spheres around them- and it failed. Yes, it's still minimizing the within-cluster sum of squares- but just like in Anscombe's Quartet above, it's a Pyrrhic victory!</p>\\n\\n<p>You might say \"That's not a fair example... <em>no</em> clustering method could correctly find clusters that are that weird.\" Not true! Try <a href=\"http://en.wikipedia.org/wiki/Single-linkage_clustering\">single linkage</a> <a href=\"http://en.wikipedia.org/wiki/Hierarchical_clustering\">hierachical clustering</a>:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/vBuTf.png\" alt=\"enter image description here\"></p>\\n\\n<p>Nailed it! This is because single-linkage hierarchical clustering makes the <em>right</em> assumptions for this dataset. (There's a whole <em>other</em> class of situations where it fails).</p>\\n\\n<p>You might say \"That's a single, extreme, pathological case.\" But it's not! For instance, you can make the outer group a semi-circle instead of a circle, and you'll see k-means still does terribly (and hierarchical clustering still does well). I could come up with other problematic situations easily, and that's just in two dimensions. When you're clustering 16-dimensional data, there's all kinds of pathologies that could arise.</p>\\n\\n<p>Lastly, I should note that k-means is still salvagable! If you start by transforming your data into <a href=\"http://en.wikipedia.org/wiki/Polar_coordinate_system\">polar coordinates</a>, the clustering now works:</p>\\n\\n<p><img src=\"http://varianceexplained.org/figs/2015-01-16-kmeans-free-lunch/polar-1.png\" alt=\"center\"> </p>\\n\\n<p>That's why understanding the assumptions underlying a method is essential: <em>it doesn't just tell you when a method has drawbacks, it tells you how to fix them.</em></p>\\n\\n<h3>Broken Assumption: Unevenly Sized Clusters</h3>\\n\\n<p>What if the clusters have an uneven number of points- does that also break k-means clustering? Well, consider this set of clusters, of sizes 20, 100, 500. I've generated each from a multivariate Gaussian: </p>\\n\\n<p><img src=\"http://varianceexplained.org/figs/2015-01-16-kmeans-free-lunch/different_sizes-1.png\" alt=\"center\"> </p>\\n\\n<p>This looks like k-means could probably find those clusters, right? Everything seems to be generated into neat and tidy groups. So let's try k-means:</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/zAI1g.png\" alt=\"enter image description here\"></p>\\n\\n<p>Ouch. What happened here is a bit subtler. In its quest to minimize the within-cluster sum of squares, the k-means algorithm gives more \"weight\" to larger clusters. In practice, that means it's happy to let that small cluster end up far away from any center, while it uses those centers to \"split up\" a much larger cluster.</p>\\n\\n<p>If you play with these examples a little (<a href=\"https://github.com/dgrtwo/dgrtwo.github.com/blob/master/_R/2015-01-16-kmeans-free-lunch.Rmd\">R code here!</a>), you'll see that you can construct far more scenarios where k-means gets it embarrassingly wrong.</p>\\n\\n<h3>Conclusion: No Free Lunch</h3>\\n\\n<p>There's a charming construction in mathematical folklore, formalized by <a href=\"http://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf\">Wolpert and Macready</a>, called the \"No Free Lunch Theorem.\" It's probably my favorite theorem in machine learning philosophy, and I relish any chance to bring it up (did I mention I love this question?) The basic idea is stated (non-rigorously) as this: <strong>\"When averaged across all possible situations, every algorithm performs equally well.\"</strong></p>\\n\\n<p>Sound counterintuitive? Consider that for every case where an algorithm works, I could construct a situation where it fails terribly. Linear regression assumes your data falls along a line- but what if it follows a sinusoidal wave? A t-test assumes each sample comes from a normal distribution: what if you throw in an outlier? Any gradient ascent algorithm can get trapped in local maxima, and any supervised classification can be tricked into overfitting.</p>\\n\\n<p>What does this mean? It means that assumptions <em>are where your power comes from!</em> When Netflix recommends movies to you, it's assuming that if you like one movie, you'll like similar ones (and vice versa). Imagine a world where that wasn't true, and your tastes are perfectly random- scattered haphazardly across genres, actors and directors. Their recommendation algorithm would fail terribly. Would it make sense to say \"Well, it's still minimizing some expected squared error, so the algorithm is still working\"? You can't make a recommendation algorithm without making some assumptions about users' tastes- just like you can't make a clustering algorithm without making some assumptions about the nature of those clusters.</p>\\n\\n<p>So don't just accept these drawbacks. Know them, so they can inform your choice of algorithms. Understand them, so you can tweak your algorithm and transform your data to solve them. And love them, because if your model could never be wrong, that means it will never be right.</p>\\n\\n<hr>\\n\n",
       "Name: Body, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(apost_df.Body[apost_df.Id==133694])#apost_df.Id==133694"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_html_text(row):\n",
    "    soup = BeautifulSoup(row, 'html.parser')\n",
    "    #denote code\n",
    "    for tag in soup.find_all('code'):\n",
    "        tag.replaceWith(' refcode ')\n",
    "    #denote link\n",
    "    for tag in soup.find_all('a'):\n",
    "        content = tag.text\n",
    "        tag.replaceWith(content +' (reflink) ')\n",
    "    #denote image\n",
    "    for tag in soup.find_all('img'):\n",
    "        tag.replaceWith(' refimage ')\n",
    "        \n",
    "    raw = soup.get_text().lower()\n",
    "    #remove whitespace and /\n",
    "    raw = re.sub('[\\t\\n\\r\\x0b\\x0c/]+?', ' ', raw) \n",
    "    #denote mention\n",
    "    # replace twitter @mentions\n",
    "    mentionFinder = re.compile(r\"@[a-z0-9_]{1,15}\", re.IGNORECASE)\n",
    "    raw = mentionFinder.sub(\"@mention\", raw)\n",
    "    \n",
    "    #denote email\n",
    "    raw = re.sub(r'[\\w\\.-]+@[\\w\\.-]+[\\.][com|org|ch|uk]{2,3}', \" refemail \", raw)\n",
    "    #denote fomula\n",
    "    reg = '(\\$\\$.+?\\$\\$)|((\\\\\\\\begin\\{.+?\\})(.+?)(\\\\\\\\end\\{(.+?)\\}))'\n",
    "    raw = re.sub(reg, \" refformula \", raw, flags=re.IGNORECASE)  \n",
    "    #denote variable\n",
    "    raw = re.sub('(\\$.+?\\$)|([a-z]\\d)',' refvariable ', raw)\n",
    "    #denote number\n",
    "    raw = re.sub('[-+]?(\\d*[.])?\\d+',' refnumber ', raw)\n",
    "    \n",
    "    return(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apost_df['Body_Text'] = apost_df.Body.map(lambda i: clean_html_text(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = [-36, 1, 260]\n",
    "group_names = ['bad','good']\n",
    "apost_df['AnsQuality']= pd.cut(apost_df['Score'],bins,labels=group_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "good    40546\n",
       "bad     33785\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apost_df.AnsQuality.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../Data/ans_clean_forDL.pickle', 'wb') as handle:\n",
    "    pickle.dump(apost_df[['Id','Body_Text','AnsQuality']], handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"one at a somewhat lower level of mathematical sophistication than wooldridge (less dense, more pictures), but a bit more up to date on some of the fast-moving areas: murray, michael p. econometrics: a modern introduction. addison wesley,  refnumber .  refnumber  pp. isbn  refnumber  (reflink)  seems that it's not available for preview on the web and the publisher is out of stock, but you can view pdfs of  refnumber  web extensions (reflink)  to get an idea of its style. \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(clean_html_text(apost_df.Body[apost_df.Id==4632].iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Automatic Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.summarization import summarize\n",
    "from gensim.summarization import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = clean_html_text(apost_df.Body[apost_df.Id==1632].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"it's hard to ignore the wealth of statistical packages available in r cran.  that said, i spend a lot of time in python land and would never dissuade anyone from having as much fun as i do.  :)  here are some libraries links you might find useful for statistical work.    numpy scipy (reflink)  you probably know about these already.  but let me point out the cookbook (reflink)  where you can read about many statistical facilities already available and the example list (reflink)  which is a great reference for functions (including data manipulation and other operations).  another handy reference is john cook's distributions in scipy (reflink) . pandas (reflink)  this is a really nice library for working with statistical data -- tabular data, time series, panel data.  includes many builtin functions for data summaries, grouping aggregation, pivoting.  also has a statistics econometrics library. larry (reflink)   labeled array that plays nice with numpy.  provides statistical functions not present in numpy and good for data manipulation. python-statlib (reflink)  a fairly recent effort which combined a number of scattered statistics libraries.  useful for basic and descriptive statistics if you're not using numpy or pandas. statsmodels (reflink)  statistical modeling: linear models, glms, among others.   scikits (reflink)   statistical and scientific computing packages -- notably smoothing, optimization and machine learning. pymc (reflink)  for your bayesian mcmc hierarchical modeling needs. highly recommended. pymix (reflink)  mixture models.  if speed becomes a problem, consider theano (reflink)  -- used with good success by the deep learning people. there's plenty of other stuff out there, but this is what i find the most useful along the lines you mentioned. \""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "but let me point out the cookbook (reflink)  where you can read about many statistical facilities already available and the example list (reflink)  which is a great reference for functions (including data manipulation and other operations).\n"
     ]
    }
   ],
   "source": [
    "print 'Summary:'\n",
    "print summarize(text, word_count=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords:\n",
      "reflink\n",
      "statistics\n",
      "data\n",
      "models\n",
      "libraries\n",
      "learning\n",
      "packages\n"
     ]
    }
   ],
   "source": [
    "print 'Keywords:'\n",
    "print keywords(text,pos_filter=['NN'],ratio=0.1,lemmatize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Word2vec/Doc2vec prepare: Train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(apost_df[['Id','Body_Text']], apost_df['AnsQuality'], test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train['source'] = 'train'\n",
    "X_test['source'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp_df = pd.concat([X_train, X_test],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apost_df = apost_df.merge(tmp_df, left_on=apost_df.Id,right_on=tmp_df.Id,suffixes=['_post', '_tmp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Body', u'ViewCount', u'ClosedDate', u'ParentID', u'CommentCount',\n",
       "       u'AnswerCount', u'AcceptedAnswerId', u'Score', u'OwnerDisplayName',\n",
       "       u'Title', u'PostTypeId', u'OwnerUserId', u'Tags', u'CreationDate',\n",
       "       u'FavoriteCount', u'Id_post', u'Body_Text_post', u'AnsQuality',\n",
       "       u'Id_tmp', u'Body_Text_tmp', u'source'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apost_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdata = open(\"../data/allans_forDL.csv\", 'wb')\n",
    "for index,row in apost_df.iterrows():\n",
    "    ID = row['Id_tmp']\n",
    "    split = row['source']\n",
    "    sentiment = row['AnsQuality']     \n",
    "    ans = row[\"Body_Text_post\"].encode(\"ascii\", \"ignore\")\n",
    "    \n",
    "    fdata.write(\"AnsID:%s\\t%s\\t%s\\t%s\\n\" % (str(ID),ans,split,sentiment))\n",
    "fdata.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74331 docs: 44598 train-sentiment, 29733 test-sentiment\n"
     ]
    }
   ],
   "source": [
    "alldocs = []  # will hold all docs in original order\n",
    "with open('../Data/allans_forDL.csv') as alldata:\n",
    "    for line_no, line in enumerate(alldata):\n",
    "        tags, words_texts, split, sentiment = line.strip().split('\\t')\n",
    "        words = nltk.word_tokenize(words_texts)\n",
    "        if sentiment == \"good\":\n",
    "            sentiment = 1\n",
    "        else:\n",
    "            sentiment = 0\n",
    "            \n",
    "        alldocs.append(SentimentDocument(words, tags, split, sentiment))\n",
    "\n",
    "train_docs = [doc for doc in alldocs if doc.split == 'train']\n",
    "test_docs = [doc for doc in alldocs if doc.split == 'test']\n",
    "doc_list = alldocs[:]  # for reshuffling per pass\n",
    "\n",
    "print('%d docs: %d train-sentiment, %d test-sentiment' % (len(doc_list), len(train_docs), len(test_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentDocument(words=['this', 'paper', 'by', 'massey', 'and', 'denton', 'refnumber', '(', 'reflink', ')', 'is', 'a', 'fairly', 'prolific', 'overview', 'of', 'commonly', 'used', 'indices', 'in', 'sociology', 'demography', '.', 'it', 'would', 'also', 'be', 'useful', 'for', 'some', 'other', 'key', 'terms', 'used', 'for', 'searching', 'articles', '.', 'frequently', 'in', 'sociology', 'the', 'indices', 'are', 'labelled', 'with', 'names', 'such', 'as', '``', 'heterogeneity', \"''\", 'and', '``', 'segregation', \"''\", 'as', 'well', 'as', '``', 'diversity', \"''\", '.', 'part', 'of', 'the', 'reason', 'no', 'absolute', 'right', 'answer', 'exists', 'to', 'your', 'question', 'is', 'that', 'people', 'frequently', 'only', 'use', 'epistemic', 'logic', 'to', 'reason', 'why', 'one', 'index', 'is', 'a', 'preferred', 'measurement', '.', 'infrequently', 'are', 'those', 'arguments', 'so', 'strong', 'that', 'one', 'should', 'entirely', 'discount', 'other', 'suggested', 'measures', '.', 'the', 'work', 'of', 'massey', 'and', 'denton', 'is', 'useful', 'to', 'highlight', 'what', 'many', 'of', 'these', 'indices', 'theoretically', 'measure', 'and', 'when', 'they', 'differ', 'to', 'a', 'substantively', 'noticeable', 'extent', '(', 'in', 'large', 'cities', 'in', 'the', 'us', ')', '.'], tags='AnsID:7704', split='train', sentiment=1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_list[5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'this paper by massey and denton  refnumber  (reflink)  is a fairly prolific overview of commonly used indices in sociology demography. it would also be useful for some other key terms used for searching articles. frequently in sociology the indices are labelled with names such as \"heterogeneity\" and \"segregation\" as well as \"diversity\". part of the reason no absolute right answer exists to your question is that people frequently only use epistemic logic to reason why one index is a preferred measurement. infrequently are those arguments so strong that one should entirely discount other suggested measures. the work of massey and denton is useful to highlight what many of these indices theoretically measure and when they differ to a substantively noticeable extent (in large cities in the us). '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apost_df['Body_Text_post'][5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Set-up Doc2Vec Training & Evaluation Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_models = [\n",
    "    # PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "    #Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, size=100, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DM w/average\n",
    "    Doc2Vec(dm=1, dm_mean=1, size=100, window=10, negative=5, hs=0, min_count=2, workers=cores),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# speed setup by sharing results of 1st model's vocabulary scan\n",
    "simple_models[0].build_vocab(alldocs)  # PV-DM/concat requires one special NULL word so it serves as template\n",
    "print(simple_models[0])\n",
    "for model in simple_models[1:]:\n",
    "    model.reset_from(simple_models[0])\n",
    "    print(model)\n",
    "\n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Predictive Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from random import sample\n",
    "\n",
    "# for timing\n",
    "from contextlib import contextmanager\n",
    "from timeit import default_timer\n",
    "import time \n",
    "\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "    \n",
    "def logistic_predictor_from_data(train_targets, train_regressors):\n",
    "    logit = sm.Logit(train_targets, train_regressors)\n",
    "    predictor = logit.fit(disp=0)\n",
    "    #print(predictor.summary())\n",
    "    return predictor\n",
    "\n",
    "def error_rate_for_model(test_model, train_set, test_set, infer=False, infer_steps=3, infer_alpha=0.1, infer_subsample=0.1):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "\n",
    "    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
    "    train_regressors = sm.add_constant(train_regressors)\n",
    "    predictor = logistic_predictor_from_data(train_targets, train_regressors)\n",
    "\n",
    "    test_data = test_set\n",
    "    if infer:\n",
    "        if infer_subsample < 1.0:\n",
    "            test_data = sample(test_data, int(infer_subsample * len(test_data)))\n",
    "        test_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in test_data]\n",
    "    else:\n",
    "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_docs]\n",
    "    test_regressors = sm.add_constant(test_regressors)\n",
    "    \n",
    "    # predict & evaluate\n",
    "    test_predictions = predictor.predict(test_regressors)\n",
    "    corrects = sum(np.rint(test_predictions) == [doc.sentiment for doc in test_data])\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    return (error_rate, errors, len(test_predictions), predictor) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Bulk Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using explicit multiple-pass, alpha-reduction approach as sketched in gensim doc2vec blog post – with added shuffling of corpus on each pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of each model's sentiment-predictive power is repeated after each pass, as an error rate (lower is better), to see the rates-of-relative-improvement. The base numbers reuse the TRAIN and TEST vectors stored in the models for the logistic regression, while the inferred results use newly-inferred TEST vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import datetime\n",
    "\n",
    "alpha, min_alpha, passes = (0.025, 0.001, 20)\n",
    "alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "print(\"START %s\" % datetime.datetime.now())\n",
    "\n",
    "for epoch in range(passes):\n",
    "    shuffle(doc_list)  # shuffling gets best results\n",
    "    \n",
    "    for name, train_model in models_by_name.items():\n",
    "        # train\n",
    "        duration = 'na'\n",
    "        train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "        with elapsed_timer() as elapsed:\n",
    "            train_model.train(doc_list)\n",
    "            duration = '%.1f' % elapsed()\n",
    "            \n",
    "        # evaluate\n",
    "        eval_duration = ''\n",
    "        with elapsed_timer() as eval_elapsed:\n",
    "            err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs)\n",
    "        eval_duration = '%.1f' % eval_elapsed()\n",
    "        best_indicator = ' '\n",
    "        if err <= best_error[name]:\n",
    "            best_error[name] = err\n",
    "            best_indicator = '*' \n",
    "        print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, err, epoch + 1, name, duration, eval_duration))\n",
    "\n",
    "        if ((epoch + 1) % 5) == 0 or epoch == 0:\n",
    "            eval_duration = ''\n",
    "            with elapsed_timer() as eval_elapsed:\n",
    "                infer_err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs, infer=True)\n",
    "            eval_duration = '%.1f' % eval_elapsed()\n",
    "            best_indicator = ' '\n",
    "            if infer_err < best_error[name + '_inferred']:\n",
    "                best_error[name + '_inferred'] = infer_err\n",
    "                best_indicator = '*'\n",
    "            print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, infer_err, epoch + 1, name + '_inferred', duration, eval_duration))\n",
    "\n",
    "    print('completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
    "    alpha -= alpha_delta\n",
    "    \n",
    "print(\"END %s\" % str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Achieved Sentiment-Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print best error rates achieved\n",
    "for rate, name in sorted((rate, name) for name, rate in best_error.items()):\n",
    "    print(\"%f %s\" % (rate, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Are inferred vectors close to the precalculated ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc; re-run cell for more examples\n",
    "print('for doc %d...' % doc_id)\n",
    "for model in simple_models:\n",
    "    inferred_docvec = model.infer_vector(alldocs[doc_id].words)\n",
    "    print('%s:\\n %s' % (model, model.docvecs.most_similar([inferred_docvec], topn=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Do close documents seem more related than distant ones?¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc, re-run cell for more examples\n",
    "model = random.choice(simple_models)  # and a random model\n",
    "sims = model.docvecs.most_similar(doc_id, topn=model.docvecs.count)  # get *all* similar documents\n",
    "print(u'TARGET (%d): «%s»\\n' % (doc_id, ' '.join(alldocs[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(alldocs[sims[index][0]].words)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
