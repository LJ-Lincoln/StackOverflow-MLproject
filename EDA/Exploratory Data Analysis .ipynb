{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from IPython.core.display import display, HTML\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "plt.rcdefaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('../Data/crossvalidated.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return all the records for questions posts from posts table\n",
    "ques_query = \"SELECT * FROM [posts]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post_df = pd.read_sql_query(ques_query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151044, 21)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "post_df.drop(['LastEditorDisplayName','CommunityOwnedDate','LastEditorUserId','LastEditDate',\n",
    "             'LastActivityDate'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Body', u'ViewCount', u'ClosedDate', u'ParentID', u'CommentCount',\n",
       "       u'AnswerCount', u'AcceptedAnswerId', u'Score', u'OwnerDisplayName',\n",
       "       u'Title', u'PostTypeId', u'OwnerUserId', u'Tags', u'CreationDate',\n",
       "       u'FavoriteCount', u'Id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeplain(html):\n",
    "    text = BeautifulSoup(html, 'html.parser').get_text()\n",
    "    text = text.replace(u'\\u200b','')\n",
    "    text = text.replace(u'\\xa0','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "post_df['Body_Text'] = post_df.apply(lambda row: makeplain(row['Body']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = post_df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Top Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/IPython/kernel/__main__.py:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "top_100ques = post_df[(post_df.PostTypeId==1)].sort(['Score','ViewCount','AnswerCount','FavoriteCount','CommentCount'],\\\n",
    "                                      ascending=[False,False,False,False,False]).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Body_Text</th>\n",
       "      <th>Title</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>CommentCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>334</td>\n",
       "      <td>In today's pattern recognition class my professor talked about PCA, eigenvectors &amp; eigenvalues. \\nI got the mathematics of it. If I'm asked to find eigenvalues etc. I'll do it correctly like a machine. But I didn't understand it. I didn't get the purpose of it. I didn't get the feel of it.  I strongly believe in \\n\\nyou do not really understand something unless you can explain it to your grandmother -- Albert Einstein\\n\\nWell, I can't explain these concepts to a layman or grandma.\\n\\nWhy PCA, eigenvectors &amp; eigenvalues? What was the need for these concepts?\\nHow would you explain these to a layman?\\n\\n</td>\n",
       "      <td>Making sense of principal component analysis, eigenvectors &amp; eigenvalues</td>\n",
       "      <td>134213.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>389.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>272</td>\n",
       "      <td>Lots of people use a main tool like Excel or another spreadsheet, SPSS, Stata, or R for their statistics needs. They might turn to some specific package for very special needs, but a lot of things can be done with a simple spreadsheet or a general stats package or stats programming environment.\\nI've always liked Python as a programming language, and for simple needs, it's easy to write a short program that calculates what I need. Matplotlib allows me to plot it.\\nHas anyone switched completely from, say R, to Python? R (or any other statistics package) has a lot of functionality specific to statistics, and it has data structures that allow you to think about the statistics you want to perform and less about the internal representation of your data. Python (or some other dynamic language) has the benefit of allowing me to program in a familiar, high-level language, and it lets me programmatically interact with real-world systems in which the data resides or from which I can take measurements. But I haven't found any Python package that would allow me to express things with \"statistical terminology\" – from simple descriptive statistics to more complicated multivariate methods.\\nWhat can you recommend if I wanted to use Python as a \"statistics workbench\" to replace R, SPSS, etc.?\\nWhat would I gain and lose, based on your experience?\\n</td>\n",
       "      <td>Python as a statistics workbench</td>\n",
       "      <td>95622.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>226</td>\n",
       "      <td>Last year, I read a blog post from Brendan O'Connor entitled \"Statistics vs. Machine Learning, fight!\" that discussed some of the differences between the two fields.  Andrew Gelman responded favorably to this:\\nSimon Blomberg: \\n\\nFrom R's fortunes\\n  package: To paraphrase provocatively,\\n  'machine learning is statistics minus\\n  any checking of models and\\n  assumptions'.\\n  -- Brian D. Ripley (about the difference between machine learning\\n  and statistics) useR! 2004, Vienna\\n  (May 2004) :-) Season's Greetings!\\n\\nAndrew Gelman:\\n\\nIn that case, maybe we should get rid\\n  of checking of models and assumptions\\n  more often. Then maybe we'd be able to\\n  solve some of the problems that the\\n  machine learning people can solve but\\n  we can't!\\n\\nThere was also the \"Statistical Modeling: The Two Cultures\" paper by Leo Breiman in 2001 which argued that statisticians rely too heavily on data modeling, and that machine learning techniques are making progress by instead relying on the predictive accuracy of models.\\nHas the statistics field changed over the last decade in response to these critiques?  Do the two cultures still exist or has statistics grown to embrace machine learning techniques such as neural networks and support vector machines?\\n</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learning?</td>\n",
       "      <td>65657.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>217</td>\n",
       "      <td>This is one of my favorites:\\n\\nOne entry per answer. This is in the vein of the Stack Overflow question What’s your favorite “programmer” cartoon?.\\nP.S. Do not hotlink the cartoon without the site's permission please.\\n</td>\n",
       "      <td>What is your favorite \"data analysis\" cartoon?</td>\n",
       "      <td>103708.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>208</td>\n",
       "      <td>In the definition of standard deviation, why do we have to square the difference from the mean to get the mean (E) and take the square root back at the end? Can't we just simply take the absolute value of the difference instead and get the expected value (mean) of those, and wouldn't that also show the variation of the data? The number is going to be different from square method (the absolute-value method will be smaller), but it should still show the spread of data. Anybody know why we take this square approach as a standard?\\nThe definition of standard deviation:\\n$\\sigma = \\sqrt{E\\left[\\left(X - \\mu\\right)^2\\right]}.$\\r\\nCan't we just take the absolute value instead and still be a good measurement?\\n$\\sigma = E\\left[|X - \\mu|\\right]$\\r\\n</td>\n",
       "      <td>Why square the difference instead of taking the absolute value in standard deviation?</td>\n",
       "      <td>88420.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Score  \\\n",
       "2504  334     \n",
       "1489  272     \n",
       "5     226     \n",
       "398   217     \n",
       "108   208     \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Body_Text  \\\n",
       "2504  In today's pattern recognition class my professor talked about PCA, eigenvectors & eigenvalues. \\nI got the mathematics of it. If I'm asked to find eigenvalues etc. I'll do it correctly like a machine. But I didn't understand it. I didn't get the purpose of it. I didn't get the feel of it.  I strongly believe in \\n\\nyou do not really understand something unless you can explain it to your grandmother -- Albert Einstein\\n\\nWell, I can't explain these concepts to a layman or grandma.\\n\\nWhy PCA, eigenvectors & eigenvalues? What was the need for these concepts?\\nHow would you explain these to a layman?\\n\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "1489  Lots of people use a main tool like Excel or another spreadsheet, SPSS, Stata, or R for their statistics needs. They might turn to some specific package for very special needs, but a lot of things can be done with a simple spreadsheet or a general stats package or stats programming environment.\\nI've always liked Python as a programming language, and for simple needs, it's easy to write a short program that calculates what I need. Matplotlib allows me to plot it.\\nHas anyone switched completely from, say R, to Python? R (or any other statistics package) has a lot of functionality specific to statistics, and it has data structures that allow you to think about the statistics you want to perform and less about the internal representation of your data. Python (or some other dynamic language) has the benefit of allowing me to program in a familiar, high-level language, and it lets me programmatically interact with real-world systems in which the data resides or from which I can take measurements. But I haven't found any Python package that would allow me to express things with \"statistical terminology\" – from simple descriptive statistics to more complicated multivariate methods.\\nWhat can you recommend if I wanted to use Python as a \"statistics workbench\" to replace R, SPSS, etc.?\\nWhat would I gain and lose, based on your experience?\\n   \n",
       "5     Last year, I read a blog post from Brendan O'Connor entitled \"Statistics vs. Machine Learning, fight!\" that discussed some of the differences between the two fields.  Andrew Gelman responded favorably to this:\\nSimon Blomberg: \\n\\nFrom R's fortunes\\n  package: To paraphrase provocatively,\\n  'machine learning is statistics minus\\n  any checking of models and\\n  assumptions'.\\n  -- Brian D. Ripley (about the difference between machine learning\\n  and statistics) useR! 2004, Vienna\\n  (May 2004) :-) Season's Greetings!\\n\\nAndrew Gelman:\\n\\nIn that case, maybe we should get rid\\n  of checking of models and assumptions\\n  more often. Then maybe we'd be able to\\n  solve some of the problems that the\\n  machine learning people can solve but\\n  we can't!\\n\\nThere was also the \"Statistical Modeling: The Two Cultures\" paper by Leo Breiman in 2001 which argued that statisticians rely too heavily on data modeling, and that machine learning techniques are making progress by instead relying on the predictive accuracy of models.\\nHas the statistics field changed over the last decade in response to these critiques?  Do the two cultures still exist or has statistics grown to embrace machine learning techniques such as neural networks and support vector machines?\\n                                                                                          \n",
       "398   This is one of my favorites:\\n\\nOne entry per answer. This is in the vein of the Stack Overflow question What’s your favorite “programmer” cartoon?.\\nP.S. Do not hotlink the cartoon without the site's permission please.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "108   In the definition of standard deviation, why do we have to square the difference from the mean to get the mean (E) and take the square root back at the end? Can't we just simply take the absolute value of the difference instead and get the expected value (mean) of those, and wouldn't that also show the variation of the data? The number is going to be different from square method (the absolute-value method will be smaller), but it should still show the spread of data. Anybody know why we take this square approach as a standard?\\nThe definition of standard deviation:\\n$\\sigma = \\sqrt{E\\left[\\left(X - \\mu\\right)^2\\right]}.$\\r\\nCan't we just take the absolute value instead and still be a good measurement?\\n$\\sigma = E\\left[|X - \\mu|\\right]$\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "\n",
       "                                                                                      Title  \\\n",
       "2504  Making sense of principal component analysis, eigenvectors & eigenvalues                \n",
       "1489  Python as a statistics workbench                                                        \n",
       "5     The Two Cultures: statistics vs. machine learning?                                      \n",
       "398   What is your favorite \"data analysis\" cartoon?                                          \n",
       "108   Why square the difference instead of taking the absolute value in standard deviation?   \n",
       "\n",
       "      ViewCount  AnswerCount  FavoriteCount  CommentCount  \n",
       "2504  134213.0   26.0         389.0          15            \n",
       "1489  95622.0    25.0         328.0          1             \n",
       "5     65657.0    17.0         211.0          6             \n",
       "398   103708.0   68.0         196.0          10            \n",
       "108   88420.0    20.0         127.0          19            "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_100ques[['Score','Body_Text','Title','ViewCount','AnswerCount','FavoriteCount','CommentCount']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_ques = top_100ques.Title.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Making sense of principal component analysis, eigenvectors & eigenvalues"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Python as a statistics workbench"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The Two Cultures: statistics vs. machine learning?"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "What is your favorite \"data analysis\" cartoon?"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Why square the difference instead of taking the absolute value in standard deviation?"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "What is the difference between \"likelihood\" and \"probability\"?"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "How to understand the drawbacks of K-means"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "What is the intuition behind beta distribution?"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Bayesian and frequentist reasoning in plain English"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "What are common statistical sins?"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Famous statistical quotations"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Difference between logit and probit models"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Is $R^2$ useful or dangerous?"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Is normality testing 'essentially useless'?"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Explaining to laypeople why bootstrapping works "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Interpretation of R's lm() output"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "What happens if the explanatory and response variables are sorted independently before regression?"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "How to understand degrees of freedom?"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Why does a 95% CI not imply a 95% chance of containing the mean?"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "What is the meaning of p values and t values in statistical tests?"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for ques in best_ques[:20]:\n",
    "    display(HTML(ques))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Top Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/IPython/kernel/__main__.py:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "top_100ans = post_df[(post_df.PostTypeId==2)].sort(['Score'],ascending=[False]).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Body_Text</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99888</th>\n",
       "      <td>259</td>\n",
       "      <td>What a great question- it's a chance to show how one would inspect the drawbacks and assumptions of any statistical method.  Namely: make up some data and try the algorithm on it!\\nWe'll consider two of your assumptions, and we'll see what happens to the k-means algorithm when those assumptions are broken. We'll stick to 2-dimensional data since it's easy to visualize. (Thanks to the curse of dimensionality, adding additional dimensions is likely to make these problems more severe, not less). We'll work with the statistical programming language R: you can find the full code here (and the post in blog form here).\\nDiversion: Anscombe's Quartet\\nFirst, an analogy. Imagine someone argued the following:\\n\\nI read some material about the drawbacks of linear regression- that it expects a linear trend, that the residuals are normally distributed, and that there are no outliers. But all linear regression is doing is minimizing the sum of squared errors (SSE) from the predicted line. That's an optimization problem that can be solved no matter what the shape of the curve or the distribution of the residuals is. Thus, linear regression requires no assumptions to work.\\n\\nWell, yes, linear regression works by minimizing the sum of squared residuals. But that by itself is not the goal of a regression: what we're trying to do is draw a line that serves as a reliable, unbiased predictor of y based on x. The Gauss-Markov theorem tells us that minimizing the SSE accomplishes that goal- but that theorem rests on some very specific assumptions. If those assumptions are broken, you can still minimize the SSE, but it might not do anything. Imagine saying \"You drive a car by pushing the pedal: driving is essentially a 'pedal-pushing process.' The pedal can be pushed no matter how much gas in the tank. Therefore, even if the tank is empty, you can still push the pedal and drive the car.\"\\nBut talk is cheap. Let's look at the cold, hard, data. Or actually, made-up data.\\n \\nThis is in fact my favorite made-up data: Anscombe's Quartet. Created in 1973 by statistician Francis Anscombe, this delightful concoction illustrates the folly of trusting statistical methods blindly. Each of the datasets has the same linear regression slope, intercept, p-value and $R^2$- and yet at a glance we can see that only one of them, I, is appropriate for linear regression. In II it suggests the wrong shape, in III it is skewed by a single outlier- and in IV there is clearly no trend at all!\\nOne could say \"Linear regression is still working in those cases, because it's minimizing the sum of squares of the residuals.\" But what a Pyrrhic victory! Linear regression will always draw a line, but if it's a meaningless line, who cares?\\nSo now we see that just because an optimization can be performed doesn't mean we're accomplishing our goal. And we see that making up data, and visualizing it, is a good way to inspect the assumptions of a model. Hang on to that intuition, we're going to need it in a minute.\\nBroken Assumption: Non-Spherical Data\\nYou argue that the k-means algorithm will work fine on non-spherical clusters. Non-spherical clusters like... these?\\n \\nMaybe this isn't what you were expecting- but it's a perfectly reasonable way to construct clusters. Looking at this image, we humans immediately recognize two natural groups of points- there's no mistaking them. So let's see how k-means does: assignments are shown in color, imputed centers are shown as X's.\\n\\nWell, that's not right. K-means was trying to fit a square peg in a round hole- trying to find nice centers with neat spheres around them- and it failed. Yes, it's still minimizing the within-cluster sum of squares- but just like in Anscombe's Quartet above, it's a Pyrrhic victory!\\nYou might say \"That's not a fair example... no clustering method could correctly find clusters that are that weird.\" Not true! Try single linkage hierachical clustering:\\n\\nNailed it! This is because single-linkage hierarchical clustering makes the right assumptions for this dataset. (There's a whole other class of situations where it fails).\\nYou might say \"That's a single, extreme, pathological case.\" But it's not! For instance, you can make the outer group a semi-circle instead of a circle, and you'll see k-means still does terribly (and hierarchical clustering still does well). I could come up with other problematic situations easily, and that's just in two dimensions. When you're clustering 16-dimensional data, there's all kinds of pathologies that could arise.\\nLastly, I should note that k-means is still salvagable! If you start by transforming your data into polar coordinates, the clustering now works:\\n \\nThat's why understanding the assumptions underlying a method is essential: it doesn't just tell you when a method has drawbacks, it tells you how to fix them.\\nBroken Assumption: Unevenly Sized Clusters\\nWhat if the clusters have an uneven number of points- does that also break k-means clustering? Well, consider this set of clusters, of sizes 20, 100, 500. I've generated each from a multivariate Gaussian: \\n \\nThis looks like k-means could probably find those clusters, right? Everything seems to be generated into neat and tidy groups. So let's try k-means:\\n\\nOuch. What happened here is a bit subtler. In its quest to minimize the within-cluster sum of squares, the k-means algorithm gives more \"weight\" to larger clusters. In practice, that means it's happy to let that small cluster end up far away from any center, while it uses those centers to \"split up\" a much larger cluster.\\nIf you play with these examples a little (R code here!), you'll see that you can construct far more scenarios where k-means gets it embarrassingly wrong.\\nConclusion: No Free Lunch\\nThere's a charming construction in mathematical folklore, formalized by Wolpert and Macready, called the \"No Free Lunch Theorem.\" It's probably my favorite theorem in machine learning philosophy, and I relish any chance to bring it up (did I mention I love this question?) The basic idea is stated (non-rigorously) as this: \"When averaged across all possible situations, every algorithm performs equally well.\"\\nSound counterintuitive? Consider that for every case where an algorithm works, I could construct a situation where it fails terribly. Linear regression assumes your data falls along a line- but what if it follows a sinusoidal wave? A t-test assumes each sample comes from a normal distribution: what if you throw in an outlier? Any gradient ascent algorithm can get trapped in local maxima, and any supervised classification can be tricked into overfitting.\\nWhat does this mean? It means that assumptions are where your power comes from! When Netflix recommends movies to you, it's assuming that if you like one movie, you'll like similar ones (and vice versa). Imagine a world where that wasn't true, and your tastes are perfectly random- scattered haphazardly across genres, actors and directors. Their recommendation algorithm would fail terribly. Would it make sense to say \"Well, it's still minimizing some expected squared error, so the algorithm is still working\"? You can't make a recommendation algorithm without making some assumptions about users' tastes- just like you can't make a clustering algorithm without making some assumptions about the nature of those clusters.\\nSo don't just accept these drawbacks. Know them, so they can inform your choice of algorithms. Understand them, so you can tweak your algorithm and transform your data to solve them. And love them, because if your model could never be wrong, that means it will never be right.\\n\\n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27635</th>\n",
       "      <td>251</td>\n",
       "      <td>A standard linear model (e.g., a simple regression model) can be thought of as having two 'parts'. These are called the structural component and the random component. For example:\\n$$\\nY=\\beta_0+\\beta_1X+\\varepsilon  \\\\\\n\\text{where }  \\varepsilon\\sim\\mathcal{N}(0,\\sigma^2)\\n$$\\nThe first two terms (that is, $\\beta_0+\\beta_1X$) constitute the structural component, and the $\\varepsilon$ (which indicates a normally distributed error term) is the random component. When the response variable is not normally distributed (for example, if your response variable is binary) this approach may no longer be valid. The generalized linear model (GLiM) was developed to address such cases, and logit and probit models are special cases of GLiMs that are appropriate for binary variables (or multi-category response variables with some adaptations to the process). A GLiM has three parts, a structural component, a link function, and a response distribution. For example:\\n$$\\ng(\\mu)=\\beta_0+\\beta_1X\\n$$\\nHere $\\beta_0+\\beta_1X$ is again the structural component, $g()$ is the link function, and $\\mu$ is a mean of a conditional response distribution at a given point in the covariate space. The way we think about the structural component here doesn't really differ from how we think about it with standard linear models; in fact, that's one of the great advantages of GLiMs. Because for many distributions the variance is a function of the mean, having fit a conditional mean (and given that you stipulated a response distribution), you have automatically accounted for the analog of the random component in a linear model (N.B.: this can be more complicated in practice).  \\nThe link function is the key to GLiMs: since the distribution of the response variable is non-normal, it's what lets us connect the structural component to the response--it 'links' them (hence the name). It's also the key to your question, since the logit and probit are links (as @vinux explained), and understanding link functions will allow us to intelligently choose when to use which one. Although there can be many link functions that can be acceptable, often there is one that is special. Without wanting to get too far into the weeds (this can get very technical) the predicted mean, $\\mu$, will not necessarily be mathematically the same as the response distribution's canonical location parameter; the link function that does equate them is the canonical link function. The advantage of this \"is that a minimal sufficient statistic for $\\beta$ exists\" (German Rodriguez). The canonical link for binary response data (more specifically, the binomial distribution) is the logit.  However, there are lots of functions that can map the structural component onto the interval $(0,1)$, and thus be acceptable; the probit is also popular, but there are yet other options that are sometimes used (such as the complementary log log, $\\ln(-\\ln(1-\\mu))$, often called 'cloglog'). Thus, there are lots of possible link functions and the choice of link function can be very important.  The choice should be made based on some combination of:  \\n\\nKnowledge of the response distribution,  \\nTheoretical considerations, and  \\nEmpirical fit to the data.  \\n\\nHaving covered a little of conceptual background needed to understand these ideas more clearly (forgive me), I will explain how these considerations can be used to guide your choice of link. (Let me note that I think @David's comment accurately captures why different links are chosen in practice.)  To start with, if your response variable is the outcome of a Bernoulli trial (that is, $0$ or $1$), your response distribution will be binomial, and what you are actually modeling is the probability of an observation being a $1$ (that is, $\\pi(Y=1)$). As a result, any function that maps the real number line, $(-\\infty,+\\infty)$, to the interval $(0,1)$ will work.  \\nFrom the point of view of your substantive theory, if you are thinking of your covariates as directly connected to the probability of success, then you would typically choose logistic regression because it is the canonical link. However, consider the following example: You are asked to model high_Blood_Pressure as a function of some covariates. Blood pressure itself is normally distributed in the population (I don't actually know that, but it seems reasonable prima fascie), nonetheless, clinicians dichotomized it during the study (that is, they only recorded 'high-BP' or 'normal'). In this case, probit would be preferable a-priori for theoretical reasons. This is what @Elvis meant by \"your binary outcome depends on a hidden Gaussian variable\". Another consideration is that both logit and probit are symmetrical, if you believe that the probability of success rises slowly from zero, but then tapers off more quickly as it approaches one, the cloglog is called for, etc.  \\nLastly, note that the empirical fit of the model to the data is unlikely to be of assistance in selecting a link, unless the shapes of the link functions in question differ substantially (of which, the logit and probit do not). For instance, consider the following simulation:  \\nset.seed(1)\\nprobLower = vector(length=1000)\\n\\nfor(i in 1:1000){      \\n    x = rnorm(1000)\\n    y = rbinom(n=1000, size=1, prob=pnorm(x))\\n\\n    logitModel  = glm(y~x, family=binomial(link=\"logit\"))\\n    probitModel = glm(y~x, family=binomial(link=\"probit\"))\\n\\n    probLower[i] = deviance(probitModel)&lt;deviance(logitModel)\\n}\\n\\nsum(probLower)/1000\\n[1] 0.695\\n\\nEven when we know the data were generated by a probit model, and we have 1000 data points, the probit model only yields a better fit 70% of the time, and even then, often by only a trivial amount. Consider the last iteration:  \\ndeviance(probitModel)\\n[1] 1025.759\\ndeviance(logitModel)\\n[1] 1026.366\\ndeviance(logitModel)-deviance(probitModel)\\n[1] 0.6076806\\n\\nThe reason for this is simply that the logit and probit link functions yield very similar outputs when given the same inputs.  \\n \\nThe logit and probit functions are practically identical, except that the logit is slightly further from the bounds when they 'turn the corner', as @vinux stated. (Note that to get the logit and the probit to align optimally, the logit's $\\beta_1$ must be $\\approx 1.7$ times the corresponding slope value for the probit. In addition, I could have shifted the cloglog over slightly so that they would lay on top of each other more, but I left it to the side to keep the figure more readable.) Notice that the cloglog is asymmetrical whereas the others are not; it starts pulling away from 0 earlier, but more slowly, and approaches close to 1 and then turns sharply.  \\nA couple more things can be said about link functions. First, considering the identity function ($g(\\eta)=\\eta$) as a link function allows us to understand the standard linear model as a special case of the generalized linear model (that is, the response distribution is normal, and the link is the identity function). It's also important to recognize that whatever transformation the link instantiates is properly applied to the parameter governing the response distribution (that is, $\\mu$), not the actual response data. Finally, because in practice we never have the underlying parameter to transform, in discussions of these models, often what is considered to be the actual link is left implicit and the model is represented by the inverse of the link function applied to the structural component instead. That is:\\n$$\\n\\mu=g^{-1}(\\beta_0+\\beta_1X)\\n$$\\nFor instance, logistic regression is usually represented:\\n$$\\n\\pi(Y)=\\frac{\\exp(\\beta_0+\\beta_1X)}{1+\\exp(\\beta_0+\\beta_1X)}\\n$$\\ninstead of:\\n$$\\n\\ln\\left(\\frac{\\pi(Y)}{1-\\pi(Y)}\\right)=\\beta_0+\\beta_1X\\n$$  \\nFor a quick and clear, but solid, overview of the generalized linear model, see chapter 10 of Fitzmaurice, Laird, &amp; Ware (2004), (on which I leaned for parts of this answer, although since this is my own adaptation of that--and other--material, any mistakes would be my own). For how to fit these models in R, check out the documentation for the function ?glm in the base package.  \\n(One final note added later:) I occasionally hear people say that you shouldn't use the probit, because it can't be interpreted. This is not true, although the interpretation of the betas is less intuitive. With logistic regression, a one unit change in $X_1$ is associated with a $\\beta_1$ change in the log odds of 'success' (alternatively, an $\\exp(\\beta_1)$-fold change in the odds), all else being equal. With a probit, this would be a change of $\\beta_1\\text{ }z$'s. (Think of two observations in a dataset with $z$-scores of 1 and 2, for example.) To convert these into predicted probabilities, you can pass them through the normal CDF, or look them up on a $z$-table.  \\n(+1 to both @vinux and @Elvis. Here I have tried to provide a broader framework within which to think about these things and then using that to address the choice between logit and probit.)\\n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>231</td>\n",
       "      <td>It's hard to ignore the wealth of statistical packages available in R/CRAN.  That said, I spend a lot of time in Python land and would never dissuade anyone from having as much fun as I do.  :)  Here are some libraries/links you might find useful for statistical work.  \\n\\nNumPy/Scipy You probably know about these already.  But let me point out the Cookbook where you can read about many statistical facilities already available and the Example List which is a great reference for functions (including data manipulation and other operations).  Another handy reference is John Cook's Distributions in Scipy.\\npandas This is a really nice library for working with statistical data -- tabular data, time series, panel data.  Includes many builtin functions for data summaries, grouping/aggregation, pivoting.  Also has a statistics/econometrics library.\\nlarry  Labeled array that plays nice with NumPy.  Provides statistical functions not present in NumPy and good for data manipulation.\\npython-statlib A fairly recent effort which combined a number of scattered statistics libraries.  Useful for basic and descriptive statistics if you're not using NumPy or pandas.\\nstatsmodels Statistical modeling: Linear models, GLMs, among others.  \\nscikits  Statistical and scientific computing packages -- notably smoothing, optimization and machine learning.\\nPyMC For your Bayesian/MCMC/hierarchical modeling needs. Highly recommended.\\nPyMix Mixture models.\\n\\nIf speed becomes a problem, consider Theano -- used with good success by the deep learning people.\\nThere's plenty of other stuff out there, but this is what I find the most useful along the lines you mentioned.\\n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39438</th>\n",
       "      <td>225</td>\n",
       "      <td>The short version is that the Beta distribution can be understood as representing a distribution of probabilities- that is, it represents all the possible values of a probability when we don't know what that probability is. Here is my favorite intuitive explanation of this:\\nAnyone who follows baseball is familiar with batting averages- simply the number of times a player gets a base hit divided by the number of times he goes up at bat (so it's just a percentage between 0 and 1). .266 is in general considered an average batting average, while .300 is considered an excellent one.\\nImagine we have a baseball player, and we want to predict what his season-long batting average will be. You might say we can just use his batting average so far- but this will be a very poor measure at the start of a season! If a player goes up to bat once and gets a single, his batting average is briefly 1.000, while if he strikes out or walks, his batting average is 0.000. It doesn't get much better if you go up to bat five or six times- you could get a lucky streak and get an average of 1.000, or an unlucky streak and get an average of 0, neither of which are a remotely good predictor of how you will bat that season.\\nWhy is your batting average in the first few hits not a good predictor of your eventual batting average? When a player's first at-bat is a strikeout, why does no one predict that he'll never get a hit all season? Because we're going in with prior expectations. We know that in history, most batting averages over a season have hovered between something like .215 and .360, with some extremely rare exceptions on either side. We know that if a player gets a few strikeouts in a row at the start, that might indicate he'll end up a bit worse than average, but we know he probably won't deviate from that range.\\nGiven our batting average problem, which can be represented with a binomial distribution (a series of successes and failures), the best way to represent these prior expectations (what we in statistics just call a prior) is with the Beta distribution- it's saying, before we've seen the player take his first swing, what we roughly expect his batting average to be. The domain of the Beta distribution is (0, 1), just like a probability, so we already know we're on the right track- but the appropriateness of the Beta for this task goes far beyond that.\\nWe expect that the player's season-long batting average will be most likely around .27, but that it could reasonably range from .21 to .35. This can be represented with a Beta distribution with parameters $\\alpha=81$ and $\\beta=219$:\\ncurve(dbeta(x, 81, 219))\\n\\n\\nI came up with these parameters for two reasons:\\n\\nThe mean is $\\frac{\\alpha}{\\alpha+\\beta}=\\frac{81}{81+219}=.270$\\nAs you can see in the plot, this distribution lies almost entirely within (.2, .35)- the reasonable range for a batting average.\\n\\nYou asked what the x axis represents in a beta distribution density plot- here it represents his batting average. Thus notice that in this case, not only is the y-axis a probability (or more precisely a probability density), but the x-axis is as well (batting average is just a probability of a hit, after all)! The Beta distribution is representing a probability distribution of probabilities.\\nBut here's why the Beta distribution is so appropriate. Imagine the player gets a single hit. His record for the season is now 1 hit; 1 at bat. We have to then update our probabilities- we want to shift this entire curve over just a bit to reflect our new information. While the math for proving this is a bit involved (it's shown here), the result is very simple. The new Beta distribution will be:\\n$\\mbox{Beta}(\\alpha_0+\\mbox{hits}, \\beta_0+\\mbox{misses})$\\nWhere $\\alpha_0$ and $\\beta_0$ are the parameters we started with- that is, 81 and 219. Thus, in this case, $\\alpha$  has increased by 1 (his one hit), while $\\beta$ has not increased at all (no misses yet). That means our new distribution is $\\mbox{Beta}(81+1, 219)$, or:\\ncurve(dbeta(x, 82, 219))\\n\\n\\nNotice that it has barely changed at all- the change is indeed invisible to the naked eye! (That's because one hit doesn't really mean anything).\\nHowever, the more the player hits over the course of the season, the more the curve will shift to accommodate the new evidence, and furthermore the more it will narrow based on the fact that we have more proof. Let's say halfway through the season he has been up to bat 300 times, hitting 100 out of those times. The new distribution would be $\\mbox{Beta}(81+100, 219+200)$, or:\\ncurve(dbeta(x, 81+100, 219+200))\\n\\n\\nNotice the curve is now both thinner and shifted to the right (higher batting average) than it used to be- we have a better sense of what the player's batting average is.\\nOne of the most interesting outputs of this formula is the expected value of the resulting Beta distribution, which is basically your new estimate. Recall that the expected value of the Beta distribution is $\\frac{\\alpha}{\\alpha+\\beta}$. Thus, after 100 hits of 300 real at-bats, the expected value of the new Beta distribution is $\\frac{81+100}{81+100+219+200}=.303$- notice that it is lower than the naive estimate of $\\frac{100}{100+200}=.333$, but higher than the estimate you started the season with ($\\frac{81}{81+219}=.270$). You might notice that this formula is equivalent to adding a \"head start\" to the number of hits and non-hits of a player- you're saying \"start him off in the season with 81 hits and 219 non hits on his record\").\\nThus, the Beta distribution is best for representing a probabilistic distribution of probabilities- the case where we don't know what a probability is in advance, but we have some reasonable guesses.\\n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>220</td>\n",
       "      <td>This manuscript really helped me grok PCA. I think it's still too complex for explaining to your grandmother, but it's not bad. You should skip first few bits on calculating eigens, etc. Jump down to the example in chapter 3 and look at the graphs. \\nI have some examples where I worked through some toy examples so I could understand PCA vs. OLS linear regression. I'll try to dig those up and post them as well. \\nedit:\\nYou didn't really ask about the difference between Ordinary Least Squares (OLS) and PCA but since I dug up my notes I did a blog post about it. The very short version is OLS of y ~ x minimizes error perpendicular to the independent axis like this (yellow lines are examples of two errors):\\n\\nIf you were to regress x ~ y (as opposed to y ~ x in the first example) it would minimize error like this:\\n\\nand PCA effectively minimizes error orthogonal to the model itself, like so:\\n\\nMore importantly, as others have said, in a situation where you have a WHOLE BUNCH of independent variables, PCA helps you figure out which ones matter the most. The examples above just help visualize what the first principal component looks like in a really simple case. \\nIn my blog post I have the R code for creating the above graphs and for calculating the first principal component. It might be worth playing with to build your intuition around PCA. I tend to not really own something until I write code that reproduces it.\\n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Score  \\\n",
       "99888  259     \n",
       "27635  251     \n",
       "1525   231     \n",
       "39438  225     \n",
       "2512   220     \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Body_Text  \\\n",
       "99888  What a great question- it's a chance to show how one would inspect the drawbacks and assumptions of any statistical method.  Namely: make up some data and try the algorithm on it!\\nWe'll consider two of your assumptions, and we'll see what happens to the k-means algorithm when those assumptions are broken. We'll stick to 2-dimensional data since it's easy to visualize. (Thanks to the curse of dimensionality, adding additional dimensions is likely to make these problems more severe, not less). We'll work with the statistical programming language R: you can find the full code here (and the post in blog form here).\\nDiversion: Anscombe's Quartet\\nFirst, an analogy. Imagine someone argued the following:\\n\\nI read some material about the drawbacks of linear regression- that it expects a linear trend, that the residuals are normally distributed, and that there are no outliers. But all linear regression is doing is minimizing the sum of squared errors (SSE) from the predicted line. That's an optimization problem that can be solved no matter what the shape of the curve or the distribution of the residuals is. Thus, linear regression requires no assumptions to work.\\n\\nWell, yes, linear regression works by minimizing the sum of squared residuals. But that by itself is not the goal of a regression: what we're trying to do is draw a line that serves as a reliable, unbiased predictor of y based on x. The Gauss-Markov theorem tells us that minimizing the SSE accomplishes that goal- but that theorem rests on some very specific assumptions. If those assumptions are broken, you can still minimize the SSE, but it might not do anything. Imagine saying \"You drive a car by pushing the pedal: driving is essentially a 'pedal-pushing process.' The pedal can be pushed no matter how much gas in the tank. Therefore, even if the tank is empty, you can still push the pedal and drive the car.\"\\nBut talk is cheap. Let's look at the cold, hard, data. Or actually, made-up data.\\n \\nThis is in fact my favorite made-up data: Anscombe's Quartet. Created in 1973 by statistician Francis Anscombe, this delightful concoction illustrates the folly of trusting statistical methods blindly. Each of the datasets has the same linear regression slope, intercept, p-value and $R^2$- and yet at a glance we can see that only one of them, I, is appropriate for linear regression. In II it suggests the wrong shape, in III it is skewed by a single outlier- and in IV there is clearly no trend at all!\\nOne could say \"Linear regression is still working in those cases, because it's minimizing the sum of squares of the residuals.\" But what a Pyrrhic victory! Linear regression will always draw a line, but if it's a meaningless line, who cares?\\nSo now we see that just because an optimization can be performed doesn't mean we're accomplishing our goal. And we see that making up data, and visualizing it, is a good way to inspect the assumptions of a model. Hang on to that intuition, we're going to need it in a minute.\\nBroken Assumption: Non-Spherical Data\\nYou argue that the k-means algorithm will work fine on non-spherical clusters. Non-spherical clusters like... these?\\n \\nMaybe this isn't what you were expecting- but it's a perfectly reasonable way to construct clusters. Looking at this image, we humans immediately recognize two natural groups of points- there's no mistaking them. So let's see how k-means does: assignments are shown in color, imputed centers are shown as X's.\\n\\nWell, that's not right. K-means was trying to fit a square peg in a round hole- trying to find nice centers with neat spheres around them- and it failed. Yes, it's still minimizing the within-cluster sum of squares- but just like in Anscombe's Quartet above, it's a Pyrrhic victory!\\nYou might say \"That's not a fair example... no clustering method could correctly find clusters that are that weird.\" Not true! Try single linkage hierachical clustering:\\n\\nNailed it! This is because single-linkage hierarchical clustering makes the right assumptions for this dataset. (There's a whole other class of situations where it fails).\\nYou might say \"That's a single, extreme, pathological case.\" But it's not! For instance, you can make the outer group a semi-circle instead of a circle, and you'll see k-means still does terribly (and hierarchical clustering still does well). I could come up with other problematic situations easily, and that's just in two dimensions. When you're clustering 16-dimensional data, there's all kinds of pathologies that could arise.\\nLastly, I should note that k-means is still salvagable! If you start by transforming your data into polar coordinates, the clustering now works:\\n \\nThat's why understanding the assumptions underlying a method is essential: it doesn't just tell you when a method has drawbacks, it tells you how to fix them.\\nBroken Assumption: Unevenly Sized Clusters\\nWhat if the clusters have an uneven number of points- does that also break k-means clustering? Well, consider this set of clusters, of sizes 20, 100, 500. I've generated each from a multivariate Gaussian: \\n \\nThis looks like k-means could probably find those clusters, right? Everything seems to be generated into neat and tidy groups. So let's try k-means:\\n\\nOuch. What happened here is a bit subtler. In its quest to minimize the within-cluster sum of squares, the k-means algorithm gives more \"weight\" to larger clusters. In practice, that means it's happy to let that small cluster end up far away from any center, while it uses those centers to \"split up\" a much larger cluster.\\nIf you play with these examples a little (R code here!), you'll see that you can construct far more scenarios where k-means gets it embarrassingly wrong.\\nConclusion: No Free Lunch\\nThere's a charming construction in mathematical folklore, formalized by Wolpert and Macready, called the \"No Free Lunch Theorem.\" It's probably my favorite theorem in machine learning philosophy, and I relish any chance to bring it up (did I mention I love this question?) The basic idea is stated (non-rigorously) as this: \"When averaged across all possible situations, every algorithm performs equally well.\"\\nSound counterintuitive? Consider that for every case where an algorithm works, I could construct a situation where it fails terribly. Linear regression assumes your data falls along a line- but what if it follows a sinusoidal wave? A t-test assumes each sample comes from a normal distribution: what if you throw in an outlier? Any gradient ascent algorithm can get trapped in local maxima, and any supervised classification can be tricked into overfitting.\\nWhat does this mean? It means that assumptions are where your power comes from! When Netflix recommends movies to you, it's assuming that if you like one movie, you'll like similar ones (and vice versa). Imagine a world where that wasn't true, and your tastes are perfectly random- scattered haphazardly across genres, actors and directors. Their recommendation algorithm would fail terribly. Would it make sense to say \"Well, it's still minimizing some expected squared error, so the algorithm is still working\"? You can't make a recommendation algorithm without making some assumptions about users' tastes- just like you can't make a clustering algorithm without making some assumptions about the nature of those clusters.\\nSo don't just accept these drawbacks. Know them, so they can inform your choice of algorithms. Understand them, so you can tweak your algorithm and transform your data to solve them. And love them, because if your model could never be wrong, that means it will never be right.\\n\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "27635  A standard linear model (e.g., a simple regression model) can be thought of as having two 'parts'. These are called the structural component and the random component. For example:\\n$$\\nY=\\beta_0+\\beta_1X+\\varepsilon  \\\\\\n\\text{where }  \\varepsilon\\sim\\mathcal{N}(0,\\sigma^2)\\n$$\\nThe first two terms (that is, $\\beta_0+\\beta_1X$) constitute the structural component, and the $\\varepsilon$ (which indicates a normally distributed error term) is the random component. When the response variable is not normally distributed (for example, if your response variable is binary) this approach may no longer be valid. The generalized linear model (GLiM) was developed to address such cases, and logit and probit models are special cases of GLiMs that are appropriate for binary variables (or multi-category response variables with some adaptations to the process). A GLiM has three parts, a structural component, a link function, and a response distribution. For example:\\n$$\\ng(\\mu)=\\beta_0+\\beta_1X\\n$$\\nHere $\\beta_0+\\beta_1X$ is again the structural component, $g()$ is the link function, and $\\mu$ is a mean of a conditional response distribution at a given point in the covariate space. The way we think about the structural component here doesn't really differ from how we think about it with standard linear models; in fact, that's one of the great advantages of GLiMs. Because for many distributions the variance is a function of the mean, having fit a conditional mean (and given that you stipulated a response distribution), you have automatically accounted for the analog of the random component in a linear model (N.B.: this can be more complicated in practice).  \\nThe link function is the key to GLiMs: since the distribution of the response variable is non-normal, it's what lets us connect the structural component to the response--it 'links' them (hence the name). It's also the key to your question, since the logit and probit are links (as @vinux explained), and understanding link functions will allow us to intelligently choose when to use which one. Although there can be many link functions that can be acceptable, often there is one that is special. Without wanting to get too far into the weeds (this can get very technical) the predicted mean, $\\mu$, will not necessarily be mathematically the same as the response distribution's canonical location parameter; the link function that does equate them is the canonical link function. The advantage of this \"is that a minimal sufficient statistic for $\\beta$ exists\" (German Rodriguez). The canonical link for binary response data (more specifically, the binomial distribution) is the logit.  However, there are lots of functions that can map the structural component onto the interval $(0,1)$, and thus be acceptable; the probit is also popular, but there are yet other options that are sometimes used (such as the complementary log log, $\\ln(-\\ln(1-\\mu))$, often called 'cloglog'). Thus, there are lots of possible link functions and the choice of link function can be very important.  The choice should be made based on some combination of:  \\n\\nKnowledge of the response distribution,  \\nTheoretical considerations, and  \\nEmpirical fit to the data.  \\n\\nHaving covered a little of conceptual background needed to understand these ideas more clearly (forgive me), I will explain how these considerations can be used to guide your choice of link. (Let me note that I think @David's comment accurately captures why different links are chosen in practice.)  To start with, if your response variable is the outcome of a Bernoulli trial (that is, $0$ or $1$), your response distribution will be binomial, and what you are actually modeling is the probability of an observation being a $1$ (that is, $\\pi(Y=1)$). As a result, any function that maps the real number line, $(-\\infty,+\\infty)$, to the interval $(0,1)$ will work.  \\nFrom the point of view of your substantive theory, if you are thinking of your covariates as directly connected to the probability of success, then you would typically choose logistic regression because it is the canonical link. However, consider the following example: You are asked to model high_Blood_Pressure as a function of some covariates. Blood pressure itself is normally distributed in the population (I don't actually know that, but it seems reasonable prima fascie), nonetheless, clinicians dichotomized it during the study (that is, they only recorded 'high-BP' or 'normal'). In this case, probit would be preferable a-priori for theoretical reasons. This is what @Elvis meant by \"your binary outcome depends on a hidden Gaussian variable\". Another consideration is that both logit and probit are symmetrical, if you believe that the probability of success rises slowly from zero, but then tapers off more quickly as it approaches one, the cloglog is called for, etc.  \\nLastly, note that the empirical fit of the model to the data is unlikely to be of assistance in selecting a link, unless the shapes of the link functions in question differ substantially (of which, the logit and probit do not). For instance, consider the following simulation:  \\nset.seed(1)\\nprobLower = vector(length=1000)\\n\\nfor(i in 1:1000){      \\n    x = rnorm(1000)\\n    y = rbinom(n=1000, size=1, prob=pnorm(x))\\n\\n    logitModel  = glm(y~x, family=binomial(link=\"logit\"))\\n    probitModel = glm(y~x, family=binomial(link=\"probit\"))\\n\\n    probLower[i] = deviance(probitModel)<deviance(logitModel)\\n}\\n\\nsum(probLower)/1000\\n[1] 0.695\\n\\nEven when we know the data were generated by a probit model, and we have 1000 data points, the probit model only yields a better fit 70% of the time, and even then, often by only a trivial amount. Consider the last iteration:  \\ndeviance(probitModel)\\n[1] 1025.759\\ndeviance(logitModel)\\n[1] 1026.366\\ndeviance(logitModel)-deviance(probitModel)\\n[1] 0.6076806\\n\\nThe reason for this is simply that the logit and probit link functions yield very similar outputs when given the same inputs.  \\n \\nThe logit and probit functions are practically identical, except that the logit is slightly further from the bounds when they 'turn the corner', as @vinux stated. (Note that to get the logit and the probit to align optimally, the logit's $\\beta_1$ must be $\\approx 1.7$ times the corresponding slope value for the probit. In addition, I could have shifted the cloglog over slightly so that they would lay on top of each other more, but I left it to the side to keep the figure more readable.) Notice that the cloglog is asymmetrical whereas the others are not; it starts pulling away from 0 earlier, but more slowly, and approaches close to 1 and then turns sharply.  \\nA couple more things can be said about link functions. First, considering the identity function ($g(\\eta)=\\eta$) as a link function allows us to understand the standard linear model as a special case of the generalized linear model (that is, the response distribution is normal, and the link is the identity function). It's also important to recognize that whatever transformation the link instantiates is properly applied to the parameter governing the response distribution (that is, $\\mu$), not the actual response data. Finally, because in practice we never have the underlying parameter to transform, in discussions of these models, often what is considered to be the actual link is left implicit and the model is represented by the inverse of the link function applied to the structural component instead. That is:\\n$$\\n\\mu=g^{-1}(\\beta_0+\\beta_1X)\\n$$\\nFor instance, logistic regression is usually represented:\\n$$\\n\\pi(Y)=\\frac{\\exp(\\beta_0+\\beta_1X)}{1+\\exp(\\beta_0+\\beta_1X)}\\n$$\\ninstead of:\\n$$\\n\\ln\\left(\\frac{\\pi(Y)}{1-\\pi(Y)}\\right)=\\beta_0+\\beta_1X\\n$$  \\nFor a quick and clear, but solid, overview of the generalized linear model, see chapter 10 of Fitzmaurice, Laird, & Ware (2004), (on which I leaned for parts of this answer, although since this is my own adaptation of that--and other--material, any mistakes would be my own). For how to fit these models in R, check out the documentation for the function ?glm in the base package.  \\n(One final note added later:) I occasionally hear people say that you shouldn't use the probit, because it can't be interpreted. This is not true, although the interpretation of the betas is less intuitive. With logistic regression, a one unit change in $X_1$ is associated with a $\\beta_1$ change in the log odds of 'success' (alternatively, an $\\exp(\\beta_1)$-fold change in the odds), all else being equal. With a probit, this would be a change of $\\beta_1\\text{ }z$'s. (Think of two observations in a dataset with $z$-scores of 1 and 2, for example.) To convert these into predicted probabilities, you can pass them through the normal CDF, or look them up on a $z$-table.  \\n(+1 to both @vinux and @Elvis. Here I have tried to provide a broader framework within which to think about these things and then using that to address the choice between logit and probit.)\\n   \n",
       "1525   It's hard to ignore the wealth of statistical packages available in R/CRAN.  That said, I spend a lot of time in Python land and would never dissuade anyone from having as much fun as I do.  :)  Here are some libraries/links you might find useful for statistical work.  \\n\\nNumPy/Scipy You probably know about these already.  But let me point out the Cookbook where you can read about many statistical facilities already available and the Example List which is a great reference for functions (including data manipulation and other operations).  Another handy reference is John Cook's Distributions in Scipy.\\npandas This is a really nice library for working with statistical data -- tabular data, time series, panel data.  Includes many builtin functions for data summaries, grouping/aggregation, pivoting.  Also has a statistics/econometrics library.\\nlarry  Labeled array that plays nice with NumPy.  Provides statistical functions not present in NumPy and good for data manipulation.\\npython-statlib A fairly recent effort which combined a number of scattered statistics libraries.  Useful for basic and descriptive statistics if you're not using NumPy or pandas.\\nstatsmodels Statistical modeling: Linear models, GLMs, among others.  \\nscikits  Statistical and scientific computing packages -- notably smoothing, optimization and machine learning.\\nPyMC For your Bayesian/MCMC/hierarchical modeling needs. Highly recommended.\\nPyMix Mixture models.\\n\\nIf speed becomes a problem, consider Theano -- used with good success by the deep learning people.\\nThere's plenty of other stuff out there, but this is what I find the most useful along the lines you mentioned.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "39438  The short version is that the Beta distribution can be understood as representing a distribution of probabilities- that is, it represents all the possible values of a probability when we don't know what that probability is. Here is my favorite intuitive explanation of this:\\nAnyone who follows baseball is familiar with batting averages- simply the number of times a player gets a base hit divided by the number of times he goes up at bat (so it's just a percentage between 0 and 1). .266 is in general considered an average batting average, while .300 is considered an excellent one.\\nImagine we have a baseball player, and we want to predict what his season-long batting average will be. You might say we can just use his batting average so far- but this will be a very poor measure at the start of a season! If a player goes up to bat once and gets a single, his batting average is briefly 1.000, while if he strikes out or walks, his batting average is 0.000. It doesn't get much better if you go up to bat five or six times- you could get a lucky streak and get an average of 1.000, or an unlucky streak and get an average of 0, neither of which are a remotely good predictor of how you will bat that season.\\nWhy is your batting average in the first few hits not a good predictor of your eventual batting average? When a player's first at-bat is a strikeout, why does no one predict that he'll never get a hit all season? Because we're going in with prior expectations. We know that in history, most batting averages over a season have hovered between something like .215 and .360, with some extremely rare exceptions on either side. We know that if a player gets a few strikeouts in a row at the start, that might indicate he'll end up a bit worse than average, but we know he probably won't deviate from that range.\\nGiven our batting average problem, which can be represented with a binomial distribution (a series of successes and failures), the best way to represent these prior expectations (what we in statistics just call a prior) is with the Beta distribution- it's saying, before we've seen the player take his first swing, what we roughly expect his batting average to be. The domain of the Beta distribution is (0, 1), just like a probability, so we already know we're on the right track- but the appropriateness of the Beta for this task goes far beyond that.\\nWe expect that the player's season-long batting average will be most likely around .27, but that it could reasonably range from .21 to .35. This can be represented with a Beta distribution with parameters $\\alpha=81$ and $\\beta=219$:\\ncurve(dbeta(x, 81, 219))\\n\\n\\nI came up with these parameters for two reasons:\\n\\nThe mean is $\\frac{\\alpha}{\\alpha+\\beta}=\\frac{81}{81+219}=.270$\\nAs you can see in the plot, this distribution lies almost entirely within (.2, .35)- the reasonable range for a batting average.\\n\\nYou asked what the x axis represents in a beta distribution density plot- here it represents his batting average. Thus notice that in this case, not only is the y-axis a probability (or more precisely a probability density), but the x-axis is as well (batting average is just a probability of a hit, after all)! The Beta distribution is representing a probability distribution of probabilities.\\nBut here's why the Beta distribution is so appropriate. Imagine the player gets a single hit. His record for the season is now 1 hit; 1 at bat. We have to then update our probabilities- we want to shift this entire curve over just a bit to reflect our new information. While the math for proving this is a bit involved (it's shown here), the result is very simple. The new Beta distribution will be:\\n$\\mbox{Beta}(\\alpha_0+\\mbox{hits}, \\beta_0+\\mbox{misses})$\\nWhere $\\alpha_0$ and $\\beta_0$ are the parameters we started with- that is, 81 and 219. Thus, in this case, $\\alpha$  has increased by 1 (his one hit), while $\\beta$ has not increased at all (no misses yet). That means our new distribution is $\\mbox{Beta}(81+1, 219)$, or:\\ncurve(dbeta(x, 82, 219))\\n\\n\\nNotice that it has barely changed at all- the change is indeed invisible to the naked eye! (That's because one hit doesn't really mean anything).\\nHowever, the more the player hits over the course of the season, the more the curve will shift to accommodate the new evidence, and furthermore the more it will narrow based on the fact that we have more proof. Let's say halfway through the season he has been up to bat 300 times, hitting 100 out of those times. The new distribution would be $\\mbox{Beta}(81+100, 219+200)$, or:\\ncurve(dbeta(x, 81+100, 219+200))\\n\\n\\nNotice the curve is now both thinner and shifted to the right (higher batting average) than it used to be- we have a better sense of what the player's batting average is.\\nOne of the most interesting outputs of this formula is the expected value of the resulting Beta distribution, which is basically your new estimate. Recall that the expected value of the Beta distribution is $\\frac{\\alpha}{\\alpha+\\beta}$. Thus, after 100 hits of 300 real at-bats, the expected value of the new Beta distribution is $\\frac{81+100}{81+100+219+200}=.303$- notice that it is lower than the naive estimate of $\\frac{100}{100+200}=.333$, but higher than the estimate you started the season with ($\\frac{81}{81+219}=.270$). You might notice that this formula is equivalent to adding a \"head start\" to the number of hits and non-hits of a player- you're saying \"start him off in the season with 81 hits and 219 non hits on his record\").\\nThus, the Beta distribution is best for representing a probabilistic distribution of probabilities- the case where we don't know what a probability is in advance, but we have some reasonable guesses.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "2512   This manuscript really helped me grok PCA. I think it's still too complex for explaining to your grandmother, but it's not bad. You should skip first few bits on calculating eigens, etc. Jump down to the example in chapter 3 and look at the graphs. \\nI have some examples where I worked through some toy examples so I could understand PCA vs. OLS linear regression. I'll try to dig those up and post them as well. \\nedit:\\nYou didn't really ask about the difference between Ordinary Least Squares (OLS) and PCA but since I dug up my notes I did a blog post about it. The very short version is OLS of y ~ x minimizes error perpendicular to the independent axis like this (yellow lines are examples of two errors):\\n\\nIf you were to regress x ~ y (as opposed to y ~ x in the first example) it would minimize error like this:\\n\\nand PCA effectively minimizes error orthogonal to the model itself, like so:\\n\\nMore importantly, as others have said, in a situation where you have a WHOLE BUNCH of independent variables, PCA helps you figure out which ones matter the most. The examples above just help visualize what the first principal component looks like in a really simple case. \\nIn my blog post I have the R code for creating the above graphs and for calculating the first principal component. It might be worth playing with to build your intuition around PCA. I tend to not really own something until I write code that reproduces it.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "\n",
       "       Tags  \n",
       "99888  None  \n",
       "27635  None  \n",
       "1525   None  \n",
       "39438  None  \n",
       "2512   None  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_100ans[['Score','Body_Text','Tags']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 Answer\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p>What a great question- it's a chance to show how one would inspect the drawbacks and assumptions of any statistical method.  Namely: make up some data and try the algorithm on it!</p>\n",
       "\n",
       "<p>We'll consider two of your assumptions, and we'll see what happens to the k-means algorithm when those assumptions are broken. We'll stick to 2-dimensional data since it's easy to visualize. (Thanks to the <a href=\"http://en.wikipedia.org/wiki/Curse_of_dimensionality\">curse of dimensionality</a>, adding additional dimensions is likely to make these problems more severe, not less). We'll work with the statistical programming language R: you can find the full code <a href=\"https://github.com/dgrtwo/dgrtwo.github.com/blob/master/_R/2015-01-16-kmeans-free-lunch.Rmd\">here</a> (and the post in blog form <a href=\"http://varianceexplained.org/r/kmeans-free-lunch/\">here</a>).</p>\n",
       "\n",
       "<h3>Diversion: Anscombe's Quartet</h3>\n",
       "\n",
       "<p>First, an analogy. Imagine someone argued the following:</p>\n",
       "\n",
       "<blockquote>\n",
       "  <p>I read some material about the drawbacks of linear regression- that it expects a linear trend, that the residuals are normally distributed, and that there are no outliers. But all linear regression is doing is minimizing the sum of squared errors (SSE) from the predicted line. That's an optimization problem that can be solved no matter what the shape of the curve or the distribution of the residuals is. Thus, linear regression requires no assumptions to work.</p>\n",
       "</blockquote>\n",
       "\n",
       "<p>Well, yes, linear regression works by minimizing the sum of squared residuals. But that by itself is not the goal of a regression: what we're <em>trying</em> to do is draw a line that serves as a reliable, unbiased predictor of <em>y</em> based on <em>x</em>. The <a href=\"http://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem\">Gauss-Markov theorem</a> tells us that minimizing the SSE accomplishes that goal- but that theorem rests on some very specific assumptions. If those assumptions are broken, you can still minimize the SSE, but it might not <em>do</em> anything. Imagine saying \"You drive a car by pushing the pedal: driving is essentially a 'pedal-pushing process.' The pedal can be pushed no matter how much gas in the tank. Therefore, even if the tank is empty, you can still push the pedal and drive the car.\"</p>\n",
       "\n",
       "<p>But talk is cheap. Let's look at the cold, hard, data. Or actually, made-up data.</p>\n",
       "\n",
       "<p><img src=\"http://varianceexplained.org/figs/2015-01-16-kmeans-free-lunch/anscombe-1.png\" alt=\"center\"> </p>\n",
       "\n",
       "<p>This is in fact my <em>favorite</em> made-up data: <a href=\"http://en.wikipedia.org/wiki/Anscombe%27s_quartet\">Anscombe's Quartet</a>. Created in 1973 by statistician Francis Anscombe, this delightful concoction illustrates the folly of trusting statistical methods blindly. Each of the datasets has the same linear regression slope, intercept, p-value and $R^2$- and yet at a glance we can see that only one of them, <strong>I</strong>, is appropriate for linear regression. In <strong>II</strong> it suggests the wrong shape, in <strong>III</strong> it is skewed by a single outlier- and in <strong>IV</strong> there is clearly no trend at all!</p>\n",
       "\n",
       "<p>One could say \"Linear regression is still <em>working</em> in those cases, because it's minimizing the sum of squares of the residuals.\" But what a <a href=\"http://en.wikipedia.org/wiki/Pyrrhic_victory\">Pyrrhic victory</a>! Linear regression will always draw a line, but if it's a meaningless line, who cares?</p>\n",
       "\n",
       "<p>So now we see that just because an optimization can be performed doesn't mean we're accomplishing our goal. And we see that making up data, and visualizing it, is a good way to inspect the assumptions of a model. Hang on to that intuition, we're going to need it in a minute.</p>\n",
       "\n",
       "<h3>Broken Assumption: Non-Spherical Data</h3>\n",
       "\n",
       "<p>You argue that the k-means algorithm will work fine on non-spherical clusters. Non-spherical clusters like... these?</p>\n",
       "\n",
       "<p><img src=\"http://varianceexplained.org/figs/2015-01-16-kmeans-free-lunch/non_spherical-1.png\" alt=\"center\"> </p>\n",
       "\n",
       "<p>Maybe this isn't what you were expecting- but it's a perfectly reasonable way to construct clusters. Looking at this image, we humans <em>immediately</em> recognize two natural groups of points- there's no mistaking them. So let's see how k-means does: assignments are shown in color, imputed centers are shown as X's.</p>\n",
       "\n",
       "<p><img src=\"http://i.stack.imgur.com/SlpL1.png\" alt=\"enter image description here\"></p>\n",
       "\n",
       "<p>Well, <em>that</em>'s not right. K-means was trying to fit a <a href=\"http://en.wikipedia.org/wiki/Square_peg_in_a_round_hole\">square peg in a round hole</a>- trying to find nice centers with neat spheres around them- and it failed. Yes, it's still minimizing the within-cluster sum of squares- but just like in Anscombe's Quartet above, it's a Pyrrhic victory!</p>\n",
       "\n",
       "<p>You might say \"That's not a fair example... <em>no</em> clustering method could correctly find clusters that are that weird.\" Not true! Try <a href=\"http://en.wikipedia.org/wiki/Single-linkage_clustering\">single linkage</a> <a href=\"http://en.wikipedia.org/wiki/Hierarchical_clustering\">hierachical clustering</a>:</p>\n",
       "\n",
       "<p><img src=\"http://i.stack.imgur.com/vBuTf.png\" alt=\"enter image description here\"></p>\n",
       "\n",
       "<p>Nailed it! This is because single-linkage hierarchical clustering makes the <em>right</em> assumptions for this dataset. (There's a whole <em>other</em> class of situations where it fails).</p>\n",
       "\n",
       "<p>You might say \"That's a single, extreme, pathological case.\" But it's not! For instance, you can make the outer group a semi-circle instead of a circle, and you'll see k-means still does terribly (and hierarchical clustering still does well). I could come up with other problematic situations easily, and that's just in two dimensions. When you're clustering 16-dimensional data, there's all kinds of pathologies that could arise.</p>\n",
       "\n",
       "<p>Lastly, I should note that k-means is still salvagable! If you start by transforming your data into <a href=\"http://en.wikipedia.org/wiki/Polar_coordinate_system\">polar coordinates</a>, the clustering now works:</p>\n",
       "\n",
       "<p><img src=\"http://varianceexplained.org/figs/2015-01-16-kmeans-free-lunch/polar-1.png\" alt=\"center\"> </p>\n",
       "\n",
       "<p>That's why understanding the assumptions underlying a method is essential: <em>it doesn't just tell you when a method has drawbacks, it tells you how to fix them.</em></p>\n",
       "\n",
       "<h3>Broken Assumption: Unevenly Sized Clusters</h3>\n",
       "\n",
       "<p>What if the clusters have an uneven number of points- does that also break k-means clustering? Well, consider this set of clusters, of sizes 20, 100, 500. I've generated each from a multivariate Gaussian: </p>\n",
       "\n",
       "<p><img src=\"http://varianceexplained.org/figs/2015-01-16-kmeans-free-lunch/different_sizes-1.png\" alt=\"center\"> </p>\n",
       "\n",
       "<p>This looks like k-means could probably find those clusters, right? Everything seems to be generated into neat and tidy groups. So let's try k-means:</p>\n",
       "\n",
       "<p><img src=\"http://i.stack.imgur.com/zAI1g.png\" alt=\"enter image description here\"></p>\n",
       "\n",
       "<p>Ouch. What happened here is a bit subtler. In its quest to minimize the within-cluster sum of squares, the k-means algorithm gives more \"weight\" to larger clusters. In practice, that means it's happy to let that small cluster end up far away from any center, while it uses those centers to \"split up\" a much larger cluster.</p>\n",
       "\n",
       "<p>If you play with these examples a little (<a href=\"https://github.com/dgrtwo/dgrtwo.github.com/blob/master/_R/2015-01-16-kmeans-free-lunch.Rmd\">R code here!</a>), you'll see that you can construct far more scenarios where k-means gets it embarrassingly wrong.</p>\n",
       "\n",
       "<h3>Conclusion: No Free Lunch</h3>\n",
       "\n",
       "<p>There's a charming construction in mathematical folklore, formalized by <a href=\"http://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf\">Wolpert and Macready</a>, called the \"No Free Lunch Theorem.\" It's probably my favorite theorem in machine learning philosophy, and I relish any chance to bring it up (did I mention I love this question?) The basic idea is stated (non-rigorously) as this: <strong>\"When averaged across all possible situations, every algorithm performs equally well.\"</strong></p>\n",
       "\n",
       "<p>Sound counterintuitive? Consider that for every case where an algorithm works, I could construct a situation where it fails terribly. Linear regression assumes your data falls along a line- but what if it follows a sinusoidal wave? A t-test assumes each sample comes from a normal distribution: what if you throw in an outlier? Any gradient ascent algorithm can get trapped in local maxima, and any supervised classification can be tricked into overfitting.</p>\n",
       "\n",
       "<p>What does this mean? It means that assumptions <em>are where your power comes from!</em> When Netflix recommends movies to you, it's assuming that if you like one movie, you'll like similar ones (and vice versa). Imagine a world where that wasn't true, and your tastes are perfectly random- scattered haphazardly across genres, actors and directors. Their recommendation algorithm would fail terribly. Would it make sense to say \"Well, it's still minimizing some expected squared error, so the algorithm is still working\"? You can't make a recommendation algorithm without making some assumptions about users' tastes- just like you can't make a clustering algorithm without making some assumptions about the nature of those clusters.</p>\n",
       "\n",
       "<p>So don't just accept these drawbacks. Know them, so they can inform your choice of algorithms. Understand them, so you can tweak your algorithm and transform your data to solve them. And love them, because if your model could never be wrong, that means it will never be right.</p>\n",
       "\n",
       "<hr>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Top 2 Answer\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p>A standard linear model (e.g., a simple regression model) can be thought of as having two 'parts'. These are called the <em>structural component</em> and the <em>random component</em>. For example:<br>\n",
       "$$\n",
       "Y=\\beta_0+\\beta_1X+\\varepsilon  \\\\\n",
       "\\text{where }  \\varepsilon\\sim\\mathcal{N}(0,\\sigma^2)\n",
       "$$\n",
       "The first two terms (that is, $\\beta_0+\\beta_1X$) constitute the structural component, and the $\\varepsilon$ (which indicates a normally distributed error term) is the random component. When the response variable is not normally distributed (for example, if your response variable is binary) this approach may no longer be valid. The <a href=\"http://en.wikipedia.org/wiki/Generalized_linear_model\">generalized linear model</a> (GLiM) was developed to address such cases, and logit and probit models are special cases of GLiMs that are appropriate for binary variables (or multi-category response variables with some adaptations to the process). A GLiM has three parts, a <em>structural component</em>, a <em>link function</em>, and a <em>response distribution</em>. For example:<br>\n",
       "$$\n",
       "g(\\mu)=\\beta_0+\\beta_1X\n",
       "$$\n",
       "Here $\\beta_0+\\beta_1X$ is again the structural component, $g()$ is the link function, and $\\mu$ is a mean of a conditional response distribution at a given point in the covariate space. The way we think about the structural component here doesn't really differ from how we think about it with standard linear models; in fact, that's one of the great advantages of GLiMs. Because for many distributions the variance is a function of the mean, having fit a conditional mean (and given that you stipulated a response distribution), you have automatically accounted for the analog of the random component in a linear model (N.B.: this can be more complicated in practice).  </p>\n",
       "\n",
       "<p>The link function is the key to GLiMs: since the distribution of the response variable is non-normal, it's what lets us connect the structural component to the response--it 'links' them (hence the name). It's also the key to your question, since the logit and probit are links (as @vinux explained), and understanding link functions will allow us to intelligently choose when to use which one. Although there can be many link functions that can be acceptable, often there is one that is special. Without wanting to get too far into the weeds (this can get very technical) the predicted mean, $\\mu$, will not necessarily be mathematically the same as the response distribution's <em>canonical location parameter</em>; the link function that does equate them is the <em>canonical link function</em>. The advantage of this \"is that a minimal sufficient statistic for $\\beta$ exists\" (<a href=\"http://data.princeton.edu/wws509/notes/a2s1.html\">German Rodriguez</a>). The canonical link for binary response data (more specifically, the binomial distribution) is the logit.  However, there are lots of functions that can map the structural component onto the interval $(0,1)$, and thus be acceptable; the probit is also popular, but there are yet other options that are sometimes used (such as the complementary log log, $\\ln(-\\ln(1-\\mu))$, often called 'cloglog'). Thus, there are lots of possible link functions and the choice of link function can be very important.  The choice should be made based on some combination of:  </p>\n",
       "\n",
       "<ol>\n",
       "<li>Knowledge of the response distribution,  </li>\n",
       "<li>Theoretical considerations, and  </li>\n",
       "<li>Empirical fit to the data.  </li>\n",
       "</ol>\n",
       "\n",
       "<p>Having covered a little of conceptual background needed to understand these ideas more clearly (forgive me), I will explain how these considerations can be used to guide your choice of link. (Let me note that I think @David's <a href=\"http://stats.stackexchange.com/questions/20523/difference-between-logit-and-probit-models#comment37049_20529\">comment</a> accurately captures why different links are chosen <em>in practice</em>.)  To start with, if your response variable is the outcome of a Bernoulli trial (that is, $0$ or $1$), your response distribution will be binomial, and what you are actually modeling is the probability of an observation being a $1$ (that is, $\\pi(Y=1)$). As a result, any function that maps the real number line, $(-\\infty,+\\infty)$, to the interval $(0,1)$ will work.  </p>\n",
       "\n",
       "<p>From the point of view of your substantive theory, if you are thinking of your covariates as <em>directly</em> connected to the probability of success, then you would typically choose logistic regression because it is the canonical link. However, consider the following example: You are asked to model <code>high_Blood_Pressure</code> as a function of some covariates. Blood pressure itself is normally distributed in the population (I don't actually know that, but it seems reasonable prima fascie), nonetheless, clinicians dichotomized it during the study (that is, they only recorded 'high-BP' or 'normal'). In this case, probit would be preferable a-priori for theoretical reasons. This is what @Elvis meant by \"your binary outcome depends on a hidden Gaussian variable\". Another consideration is that both logit and probit are <em>symmetrical</em>, if you believe that the probability of success rises slowly from zero, but then tapers off more quickly as it approaches one, the cloglog is called for, etc.  </p>\n",
       "\n",
       "<p>Lastly, note that the empirical fit of the model to the data is unlikely to be of assistance in selecting a link, unless the shapes of the link functions in question differ substantially (of which, the logit and probit do not). For instance, consider the following simulation:  </p>\n",
       "\n",
       "<pre><code>set.seed(1)\n",
       "probLower = vector(length=1000)\n",
       "\n",
       "for(i in 1:1000){      \n",
       "    x = rnorm(1000)\n",
       "    y = rbinom(n=1000, size=1, prob=pnorm(x))\n",
       "\n",
       "    logitModel  = glm(y~x, family=binomial(link=\"logit\"))\n",
       "    probitModel = glm(y~x, family=binomial(link=\"probit\"))\n",
       "\n",
       "    probLower[i] = deviance(probitModel)&lt;deviance(logitModel)\n",
       "}\n",
       "\n",
       "sum(probLower)/1000\n",
       "[1] 0.695\n",
       "</code></pre>\n",
       "\n",
       "<p>Even when we know the data were generated by a probit model, and we have 1000 data points, the probit model only yields a better fit 70% of the time, and even then, often by only a trivial amount. Consider the last iteration:  </p>\n",
       "\n",
       "<pre><code>deviance(probitModel)\n",
       "[1] 1025.759\n",
       "deviance(logitModel)\n",
       "[1] 1026.366\n",
       "deviance(logitModel)-deviance(probitModel)\n",
       "[1] 0.6076806\n",
       "</code></pre>\n",
       "\n",
       "<p>The reason for this is simply that the logit and probit link functions yield very similar outputs when given the same inputs.  </p>\n",
       "\n",
       "<p><img src=\"http://i.stack.imgur.com/glarC.png\" alt=\"Enter image description here\">  </p>\n",
       "\n",
       "<p>The logit and probit functions are practically identical, except that the logit is slightly further from the bounds when they 'turn the corner', as @vinux stated. (Note that to get the logit and the probit to align optimally, the logit's $\\beta_1$ must be $\\approx 1.7$ times the corresponding slope value for the probit. In addition, I could have shifted the cloglog over slightly so that they would lay on top of each other more, but I left it to the side to keep the figure more readable.) Notice that the cloglog is asymmetrical whereas the others are not; it starts pulling away from 0 earlier, but more slowly, and approaches close to 1 and then turns sharply.  </p>\n",
       "\n",
       "<p>A couple more things can be said about link functions. First, considering the <em>identity function</em> ($g(\\eta)=\\eta$) as a link function allows us to understand the standard linear model as a special case of the generalized linear model (that is, the response distribution is normal, and the link is the identity function). It's also important to recognize that whatever transformation the link instantiates is properly applied to the <em>parameter</em> governing the response distribution (that is, $\\mu$), not the actual response <em>data</em>. Finally, because in practice we never have the underlying parameter to transform, in discussions of these models, often what is considered to be the actual link is left implicit and the model is represented by the <em>inverse</em> of the link function applied to the structural component instead. That is:<br>\n",
       "$$\n",
       "\\mu=g^{-1}(\\beta_0+\\beta_1X)\n",
       "$$\n",
       "For instance, logistic regression is usually represented:\n",
       "$$\n",
       "\\pi(Y)=\\frac{\\exp(\\beta_0+\\beta_1X)}{1+\\exp(\\beta_0+\\beta_1X)}\n",
       "$$\n",
       "instead of:\n",
       "$$\n",
       "\\ln\\left(\\frac{\\pi(Y)}{1-\\pi(Y)}\\right)=\\beta_0+\\beta_1X\n",
       "$$  </p>\n",
       "\n",
       "<p>For a quick and clear, but solid, overview of the generalized linear model, see chapter 10 of <a href=\"http://rads.stackoverflow.com/amzn/click/0471214876\">Fitzmaurice, Laird, &amp; Ware (2004)</a>, (on which I leaned for parts of this answer, although since this is my own adaptation of that--and other--material, any mistakes would be my own). For how to fit these models in R, check out the documentation for the function <a href=\"http://web.njit.edu/all_topics/Prog_Lang_Docs/html/library/base/html/glm.html\">?glm</a> in the base package.  </p>\n",
       "\n",
       "<p><em>(One final note added later:)</em> I occasionally hear people say that you shouldn't use the probit, because it can't be interpreted. This is not true, although the interpretation of the betas is less intuitive. With logistic regression, a one unit change in $X_1$ is associated with a $\\beta_1$ change in the log odds of 'success' (alternatively, an $\\exp(\\beta_1)$-fold change in the odds), all else being equal. With a probit, this would be a change of $\\beta_1\\text{ }z$'s. (Think of two observations in a dataset with $z$-scores of 1 and 2, for example.) To convert these into predicted <em>probabilities</em>, you can pass them through the normal <a href=\"https://en.wikipedia.org/wiki/Cumulative_distribution_function\">CDF</a>, or look them up on a $z$-table.  </p>\n",
       "\n",
       "<p>(+1 to both @vinux and @Elvis. Here I have tried to provide a broader framework within which to think about these things and then using that to address the choice between logit and probit.)</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Top 3 Answer\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p>It's hard to ignore the wealth of statistical packages available in R/CRAN.  That said, I spend a <em>lot</em> of time in Python land and would never dissuade anyone from having as much fun as I do.  :)  Here are some libraries/links you might find useful for statistical work.  </p>\n",
       "\n",
       "<ul>\n",
       "<li><p><a href=\"http://scipy.org\">NumPy/Scipy</a> You probably know about these already.  But let me point out the <a href=\"http://scipy.org/Cookbook\">Cookbook</a> where you can read about many statistical facilities already available and the <a href=\"http://www.scipy.org/Numpy_Example_List\">Example List</a> which is a great reference for functions (including data manipulation and other operations).  Another handy reference is John Cook's <a href=\"http://www.johndcook.com/distributions_scipy.html\">Distributions in Scipy</a>.</p></li>\n",
       "<li><p><a href=\"http://code.google.com/p/pandas/\">pandas</a> This is a really nice library for working with statistical data -- tabular data, time series, panel data.  Includes many builtin functions for data summaries, grouping/aggregation, pivoting.  Also has a statistics/econometrics library.</p></li>\n",
       "<li><p><a href=\"http://pypi.python.org/pypi/la\">larry</a>  Labeled array that plays nice with NumPy.  Provides statistical functions not present in NumPy and good for data manipulation.</p></li>\n",
       "<li><p><a href=\"http://code.google.com/p/python-statlib/\">python-statlib</a> A fairly recent effort which combined a number of scattered statistics libraries.  Useful for basic and descriptive statistics if you're not using NumPy or pandas.</p></li>\n",
       "<li><p><a href=\"http://statsmodels.sourceforge.net/\">statsmodels</a> Statistical modeling: Linear models, GLMs, among others.  </p></li>\n",
       "<li><p><a href=\"http://scikits.appspot.com/scikits\">scikits</a>  Statistical and scientific computing packages -- notably smoothing, optimization and machine learning.</p></li>\n",
       "<li><p><a href=\"http://pypi.python.org/pypi/pymc/\">PyMC</a> For your Bayesian/MCMC/hierarchical modeling needs. Highly recommended.</p></li>\n",
       "<li><p><a href=\"http://www.pymix.org/pymix/index.php?n=PyMix.Home\">PyMix</a> Mixture models.</p></li>\n",
       "</ul>\n",
       "\n",
       "<p>If speed becomes a problem, consider <a href=\"http://deeplearning.net/software/theano/\">Theano</a> -- used with good success by the deep learning people.</p>\n",
       "\n",
       "<p>There's plenty of other stuff out there, but this is what I find the most useful along the lines you mentioned.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Top 4 Answer\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p>The short version is that the Beta distribution can be understood as representing a distribution <em>of probabilities</em>- that is, it represents all the possible values of a probability when we don't know what that probability is. Here is my favorite intuitive explanation of this:</p>\n",
       "\n",
       "<p>Anyone who follows baseball is familiar with <a href=\"http://en.wikipedia.org/wiki/Batting_average#Major_League_Baseball\">batting averages</a>- simply the number of times a player gets a base hit divided by the number of times he goes up at bat (so it's just a percentage between <code>0</code> and <code>1</code>). <code>.266</code> is in general considered an average batting average, while <code>.300</code> is considered an excellent one.</p>\n",
       "\n",
       "<p>Imagine we have a baseball player, and we want to predict what his season-long batting average will be. You might say we can just use his batting average so far- but this will be a very poor measure at the start of a season! If a player goes up to bat once and gets a single, his batting average is briefly <code>1.000</code>, while if he strikes out or walks, his batting average is <code>0.000</code>. It doesn't get much better if you go up to bat five or six times- you could get a lucky streak and get an average of <code>1.000</code>, or an unlucky streak and get an average of <code>0</code>, neither of which are a remotely good predictor of how you will bat that season.</p>\n",
       "\n",
       "<p>Why is your batting average in the first few hits not a good predictor of your eventual batting average? When a player's first at-bat is a strikeout, why does no one predict that he'll never get a hit all season? Because we're going in with <em>prior expectations.</em> We know that in history, most batting averages over a season have hovered between something like <code>.215</code> and <code>.360</code>, with some extremely rare exceptions on either side. We know that if a player gets a few strikeouts in a row at the start, that might indicate he'll end up a bit worse than average, but we know he probably won't deviate from that range.</p>\n",
       "\n",
       "<p>Given our batting average problem, which can be represented with a <a href=\"http://en.wikipedia.org/wiki/Binomial_distribution\">binomial distribution</a> (a series of successes and failures), the best way to represent these prior expectations (what we in statistics just call a <a href=\"http://en.wikipedia.org/wiki/Prior_probability\">prior</a>) is with the Beta distribution- it's saying, before we've seen the player take his first swing, what we roughly expect his batting average to be. The domain of the Beta distribution is <code>(0, 1)</code>, just like a probability, so we already know we're on the right track- but the appropriateness of the Beta for this task goes far beyond that.</p>\n",
       "\n",
       "<p>We expect that the player's season-long batting average will be most likely around <code>.27</code>, but that it could reasonably range from <code>.21</code> to <code>.35</code>. This can be represented with a Beta distribution with parameters $\\alpha=81$ and $\\beta=219$:</p>\n",
       "\n",
       "<pre><code>curve(dbeta(x, 81, 219))\n",
       "</code></pre>\n",
       "\n",
       "<p><img src=\"http://i.stack.imgur.com/RJDrz.png\" alt=\"Beta(81, 219)\"></p>\n",
       "\n",
       "<p>I came up with these parameters for two reasons:</p>\n",
       "\n",
       "<ul>\n",
       "<li>The mean is $\\frac{\\alpha}{\\alpha+\\beta}=\\frac{81}{81+219}=.270$</li>\n",
       "<li>As you can see in the plot, this distribution lies almost entirely within <code>(.2, .35)</code>- the reasonable range for a batting average.</li>\n",
       "</ul>\n",
       "\n",
       "<p>You asked what the x axis represents in a beta distribution density plot- here it represents his batting average. Thus notice that in this case, not only is the y-axis a probability (or more precisely a probability density), but the x-axis is as well (batting average is just a probability of a hit, after all)! The Beta distribution is representing a probability distribution <em>of probabilities</em>.</p>\n",
       "\n",
       "<p>But here's why the Beta distribution is so appropriate. Imagine the player gets a single hit. His record for the season is now <code>1 hit; 1 at bat</code>. We have to then <em>update</em> our probabilities- we want to shift this entire curve over just a bit to reflect our new information. While the math for proving this is a bit involved (<a href=\"http://en.wikipedia.org/wiki/Conjugate_prior#Example\">it's shown here</a>), the result is <em>very simple</em>. The new Beta distribution will be:</p>\n",
       "\n",
       "<p>$\\mbox{Beta}(\\alpha_0+\\mbox{hits}, \\beta_0+\\mbox{misses})$</p>\n",
       "\n",
       "<p>Where $\\alpha_0$ and $\\beta_0$ are the parameters we started with- that is, 81 and 219. Thus, in this case, $\\alpha$  has increased by 1 (his one hit), while $\\beta$ has not increased at all (no misses yet). That means our new distribution is $\\mbox{Beta}(81+1, 219)$, or:</p>\n",
       "\n",
       "<pre><code>curve(dbeta(x, 82, 219))\n",
       "</code></pre>\n",
       "\n",
       "<p><img src=\"http://i.stack.imgur.com/aULXN.png\" alt=\"enter image description here\"></p>\n",
       "\n",
       "<p>Notice that it has barely changed at all- the change is indeed invisible to the naked eye! (That's because one hit doesn't really mean anything).</p>\n",
       "\n",
       "<p>However, the more the player hits over the course of the season, the more the curve will shift to accommodate the new evidence, and furthermore the more it will narrow based on the fact that we have more proof. Let's say halfway through the season he has been up to bat 300 times, hitting 100 out of those times. The new distribution would be $\\mbox{Beta}(81+100, 219+200)$, or:</p>\n",
       "\n",
       "<pre><code>curve(dbeta(x, 81+100, 219+200))\n",
       "</code></pre>\n",
       "\n",
       "<p><img src=\"http://i.stack.imgur.com/oBgYH.png\" alt=\"enter image description here\"></p>\n",
       "\n",
       "<p>Notice the curve is now both thinner and shifted to the right (higher batting average) than it used to be- we have a better sense of what the player's batting average is.</p>\n",
       "\n",
       "<p>One of the most interesting outputs of this formula is the expected value of the resulting Beta distribution, which is basically your new estimate. Recall that the expected value of the Beta distribution is $\\frac{\\alpha}{\\alpha+\\beta}$. Thus, after 100 hits of 300 <em>real</em> at-bats, the expected value of the new Beta distribution is $\\frac{81+100}{81+100+219+200}=.303$- notice that it is lower than the naive estimate of $\\frac{100}{100+200}=.333$, but higher than the estimate you started the season with ($\\frac{81}{81+219}=.270$). You might notice that this formula is equivalent to adding a \"head start\" to the number of hits and non-hits of a player- you're saying \"start him off in the season with 81 hits and 219 non hits on his record\").</p>\n",
       "\n",
       "<p>Thus, the Beta distribution is best for representing a probabilistic distribution <em>of probabilities</em>- the case where we don't know what a probability is in advance, but we have some reasonable guesses.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Top 5 Answer\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><a href=\"http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf\">This manuscript</a> really helped me grok PCA. I think it's still too complex for explaining to your grandmother, but it's not bad. You should skip first few bits on calculating eigens, etc. Jump down to the example in chapter 3 and look at the graphs. </p>\n",
       "\n",
       "<p>I have some examples where I worked through some toy examples so I could understand PCA vs. OLS linear regression. I'll try to dig those up and post them as well. </p>\n",
       "\n",
       "<p><strong>edit:</strong>\n",
       "You didn't really ask about the difference between Ordinary Least Squares (OLS) and PCA but since I dug up my notes I did a <a href=\"http://www.cerebralmastication.com/2010/09/principal-component-analysis-pca-vs-ordinary-least-squares-ols-a-visual-explination/\">blog post about it</a>. The very short version is OLS of y ~ x minimizes error perpendicular to the independent axis like this (yellow lines are examples of two errors):</p>\n",
       "\n",
       "<p><img src=\"http://i.stack.imgur.com/ozqBY.png\" alt=\"alt text\"></p>\n",
       "\n",
       "<p>If you were to regress x ~ y (as opposed to y ~ x in the first example) it would minimize error like this:</p>\n",
       "\n",
       "<p><img src=\"http://i.stack.imgur.com/pHbub.png\" alt=\"alt text\"></p>\n",
       "\n",
       "<p>and PCA effectively minimizes error orthogonal to the model itself, like so:</p>\n",
       "\n",
       "<p><img src=\"http://i.stack.imgur.com/bOl5N.png\" alt=\"alt text\"></p>\n",
       "\n",
       "<p>More importantly, as others have said, in a situation where you have a WHOLE BUNCH of independent variables, PCA helps you figure out which ones matter the most. The examples above just help visualize what the first principal component looks like in a really simple case. </p>\n",
       "\n",
       "<p>In my blog post I have the R code for creating the above graphs and for calculating the first principal component. It might be worth playing with to build your intuition around PCA. I tend to not really <em>own</em> something until I write code that reproduces it.&nbsp;</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_ans = top_100ans.Body.values\n",
    "i=1\n",
    "for ans in best_ans[:5]:\n",
    "    print 'Top '+str(i)+ ' Answer'\n",
    "    display(HTML(ans))\n",
    "    print '------------------------------------------------------'\n",
    "    i+=1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Who's really contributing to the community?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
