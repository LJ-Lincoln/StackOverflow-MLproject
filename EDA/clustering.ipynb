{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook,save\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import lda\n",
    "import matplotlib.pyplot as plt\n",
    "from bokeh.io import push_notebook\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post = pd.read_pickle('/Users/boyaliu/Documents/UCD/STA 208/final project/qpost_df_new.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>ClosedDate</th>\n",
       "      <th>ParentID</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>Score</th>\n",
       "      <th>OwnerDisplayName</th>\n",
       "      <th>Title</th>\n",
       "      <th>...</th>\n",
       "      <th>SpaceCnt</th>\n",
       "      <th>TagCnt</th>\n",
       "      <th>Sentimental_Polarity</th>\n",
       "      <th>Sentimental_Subjectivity</th>\n",
       "      <th>TitleLen</th>\n",
       "      <th>UrlCnt</th>\n",
       "      <th>CleanBody</th>\n",
       "      <th>CleanTitle</th>\n",
       "      <th>Body_rm</th>\n",
       "      <th>Title_rm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;p&gt;How should I elicit prior distributions fro...</td>\n",
       "      <td>1850.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>31</td>\n",
       "      <td>None</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>elicit prior distribution expert fit bayesian ...</td>\n",
       "      <td>elicit prior expert</td>\n",
       "      <td>How should I elicit prior distributions from e...</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;p&gt;In many different statistical methods there...</td>\n",
       "      <td>15519.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>26</td>\n",
       "      <td>None</td>\n",
       "      <td>What is normality?</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>statistical method assumption normality normal...</td>\n",
       "      <td>normality</td>\n",
       "      <td>In many different statistical methods there is...</td>\n",
       "      <td>What is normality?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;p&gt;What are some valuable Statistical Analysis...</td>\n",
       "      <td>5162.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>19.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>63</td>\n",
       "      <td>None</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>...</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>0.021875</td>\n",
       "      <td>0.596875</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>valuable statistical analysis open source proj...</td>\n",
       "      <td>valuable statistical analysis open source project</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;p&gt;I have two groups of data.  Each with a dif...</td>\n",
       "      <td>15443.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>15</td>\n",
       "      <td>None</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "      <td>...</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>0.129808</td>\n",
       "      <td>0.552885</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>group data distribution multiple variable dete...</td>\n",
       "      <td>assess significance difference distribution</td>\n",
       "      <td>I have two groups of data.  Each with a differ...</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;p&gt;Last year, I read a blog post from &lt;a href=...</td>\n",
       "      <td>65657.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>226</td>\n",
       "      <td>None</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learn...</td>\n",
       "      <td>...</td>\n",
       "      <td>204</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.150000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>year read blog post brendan connor entitle sta...</td>\n",
       "      <td>culture statistic machine learning</td>\n",
       "      <td>Last year, I read a blog post from Brendan O'C...</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Body  ViewCount ClosedDate  \\\n",
       "0  <p>How should I elicit prior distributions fro...     1850.0       None   \n",
       "1  <p>In many different statistical methods there...    15519.0       None   \n",
       "2  <p>What are some valuable Statistical Analysis...     5162.0       None   \n",
       "3  <p>I have two groups of data.  Each with a dif...    15443.0       None   \n",
       "5  <p>Last year, I read a blog post from <a href=...    65657.0       None   \n",
       "\n",
       "   ParentID  CommentCount  AnswerCount  AcceptedAnswerId  Score  \\\n",
       "0       NaN             1          5.0              15.0     31   \n",
       "1       NaN             1          7.0              59.0     26   \n",
       "2       NaN             4         19.0               5.0     63   \n",
       "3       NaN             2          5.0             135.0     15   \n",
       "5       NaN             6         17.0               NaN    226   \n",
       "\n",
       "  OwnerDisplayName                                              Title  \\\n",
       "0             None                      Eliciting priors from experts   \n",
       "1             None                                 What is normality?   \n",
       "2             None  What are some valuable Statistical Analysis op...   \n",
       "3             None  Assessing the significance of differences in d...   \n",
       "5             None  The Two Cultures: statistics vs. machine learn...   \n",
       "\n",
       "                         ...                          SpaceCnt  TagCnt  \\\n",
       "0                        ...                                15       3   \n",
       "1                        ...                                25       2   \n",
       "2                        ...                                36       2   \n",
       "3                        ...                                90       2   \n",
       "5                        ...                               204       1   \n",
       "\n",
       "  Sentimental_Polarity Sentimental_Subjectivity  TitleLen  UrlCnt  \\\n",
       "0             0.133333                 0.133333        29       0   \n",
       "1             0.000000                 0.000000        18       0   \n",
       "2             0.021875                 0.596875        65       0   \n",
       "3             0.129808                 0.552885        58       0   \n",
       "5            -0.150000                 0.300000        50       4   \n",
       "\n",
       "                                           CleanBody  \\\n",
       "0  elicit prior distribution expert fit bayesian ...   \n",
       "1  statistical method assumption normality normal...   \n",
       "2  valuable statistical analysis open source proj...   \n",
       "3  group data distribution multiple variable dete...   \n",
       "5  year read blog post brendan connor entitle sta...   \n",
       "\n",
       "                                          CleanTitle  \\\n",
       "0                                elicit prior expert   \n",
       "1                                          normality   \n",
       "2  valuable statistical analysis open source project   \n",
       "3        assess significance difference distribution   \n",
       "5                 culture statistic machine learning   \n",
       "\n",
       "                                             Body_rm  \\\n",
       "0  How should I elicit prior distributions from e...   \n",
       "1  In many different statistical methods there is...   \n",
       "2  What are some valuable Statistical Analysis op...   \n",
       "3  I have two groups of data.  Each with a differ...   \n",
       "5  Last year, I read a blog post from Brendan O'C...   \n",
       "\n",
       "                                            Title_rm  \n",
       "0                      Eliciting priors from experts  \n",
       "1                                 What is normality?  \n",
       "2  What are some valuable Statistical Analysis op...  \n",
       "3  Assessing the significance of differences in d...  \n",
       "5  The Two Cultures: statistics vs. machine learn...  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "body_title_process = post.CleanBody + ' ' + post.CleanTitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'statistical method assumption normality normality normality normality'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body_title_process[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag = post.Tags.map(lambda i: re.sub('<|>', ' ', i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             bayesian  prior  elicitation \n",
       "1                                 distributions  normality \n",
       "2                                    software  open-source \n",
       "3                  distributions  statistical-significance \n",
       "5                                         machine-learning \n",
       "6                    dataset  sample  population  teaching \n",
       "7                                                    humor \n",
       "9           scales  measurement  ordinal  interval  likert \n",
       "10                            multivariable  interpolation \n",
       "16            anova  chi-squared  generalized-linear-model \n",
       "20                         forecasting  population  census \n",
       "21                                   bayesian  frequentist \n",
       "22                                 distributions  pdf  cdf \n",
       "24                modeling  time-series  finance  software \n",
       "25                      standard-deviation  basic-concepts \n",
       "29         algorithms  hypothesis-testing  random-variab...\n",
       "30         hypothesis-testing  t-test  p-value  interpre...\n",
       "32                                          r  seasonality \n",
       "33         distributions  modeling  poisson  overdispers...\n",
       "34                                   correlation  teaching \n",
       "36               modeling  bayesian  logit  transportation \n",
       "37          algorithms  random-variable  random-generation \n",
       "41                           data-visualization  intuition \n",
       "44                                  clustering  large-data \n",
       "46                  random-variable  intuition  definition \n",
       "47                                     pattern-recognition \n",
       "49              pca  covariance-matrix  correlation-matrix \n",
       "50         standard-deviation  error  teaching  unbiased...\n",
       "54               algorithms  optimization  neural-networks \n",
       "58                                        statistical-bias \n",
       "                                ...                        \n",
       "150989                                nonlinear-regression \n",
       "150990     random-forest  missing-data  scikit-learn  no...\n",
       "150991     machine-learning  classification  neural-netw...\n",
       "150992                    feature-selection  random-forest \n",
       "150994                                  fisher-information \n",
       "150995                           logistic  matlab  entropy \n",
       "150996                                            accuracy \n",
       "150997     machine-learning  probability  distributions ...\n",
       "150998     machine-learning  neural-networks  deep-learn...\n",
       "151000     mathematical-statistics  gaussian-process  kr...\n",
       "151001                                            bayesian \n",
       "151002     machine-learning  neural-networks  matlab  co...\n",
       "151003                                       t-test  error \n",
       "151004                          machine-learning  learning \n",
       "151008                         delta-method  taylor-series \n",
       "151010     self-study  mathematical-statistics  bernoull...\n",
       "151013      maximum-likelihood  likelihood  log-likelihood \n",
       "151018     r  regression  fixed-effects-model  differenc...\n",
       "151020                                       r  pca  caret \n",
       "151022                                       r  self-study \n",
       "151023                                           pca  mean \n",
       "151026                                                 var \n",
       "151027                            r  logistic  interaction \n",
       "151028      multiple-regression  spss  partial-correlation \n",
       "151031                 bayesian  mcmc  metropolis-hastings \n",
       "151034                probability  distributions  aic  bic \n",
       "151038             probability  distributions  exponential \n",
       "151039                                       distributions \n",
       "151041                    probability  nonparametric-bayes \n",
       "151043     r  nonparametric  reporting  nonparametric-re...\n",
       "Name: Tags, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags =tag.map(lambda i:  \" \".join([word for word in word_tokenize(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                bayesian prior elicitation\n",
       "1                                   distributions normality\n",
       "2                                      software open-source\n",
       "3                    distributions statistical-significance\n",
       "5                                          machine-learning\n",
       "6                        dataset sample population teaching\n",
       "7                                                     humor\n",
       "9                scales measurement ordinal interval likert\n",
       "10                              multivariable interpolation\n",
       "16               anova chi-squared generalized-linear-model\n",
       "20                            forecasting population census\n",
       "21                                     bayesian frequentist\n",
       "22                                    distributions pdf cdf\n",
       "24                    modeling time-series finance software\n",
       "25                        standard-deviation basic-concepts\n",
       "29        algorithms hypothesis-testing random-variable ...\n",
       "30        hypothesis-testing t-test p-value interpretati...\n",
       "32                                            r seasonality\n",
       "33            distributions modeling poisson overdispersion\n",
       "34                                     correlation teaching\n",
       "36                   modeling bayesian logit transportation\n",
       "37             algorithms random-variable random-generation\n",
       "41                             data-visualization intuition\n",
       "44                                    clustering large-data\n",
       "46                     random-variable intuition definition\n",
       "47                                      pattern-recognition\n",
       "49                 pca covariance-matrix correlation-matrix\n",
       "50        standard-deviation error teaching unbiased-est...\n",
       "54                  algorithms optimization neural-networks\n",
       "58                                         statistical-bias\n",
       "                                ...                        \n",
       "150989                                 nonlinear-regression\n",
       "150990      random-forest missing-data scikit-learn nominal\n",
       "150991    machine-learning classification neural-network...\n",
       "150992                      feature-selection random-forest\n",
       "150994                                   fisher-information\n",
       "150995                              logistic matlab entropy\n",
       "150996                                             accuracy\n",
       "150997    machine-learning probability distributions bay...\n",
       "150998    machine-learning neural-networks deep-learning...\n",
       "151000     mathematical-statistics gaussian-process kriging\n",
       "151001                                             bayesian\n",
       "151002    machine-learning neural-networks matlab conv-n...\n",
       "151003                                         t-test error\n",
       "151004                            machine-learning learning\n",
       "151008                           delta-method taylor-series\n",
       "151010    self-study mathematical-statistics bernoulli-d...\n",
       "151013         maximum-likelihood likelihood log-likelihood\n",
       "151018    r regression fixed-effects-model difference-in...\n",
       "151020                                          r pca caret\n",
       "151022                                         r self-study\n",
       "151023                                             pca mean\n",
       "151026                                                  var\n",
       "151027                               r logistic interaction\n",
       "151028         multiple-regression spss partial-correlation\n",
       "151031                    bayesian mcmc metropolis-hastings\n",
       "151034                    probability distributions aic bic\n",
       "151038                probability distributions exponential\n",
       "151039                                        distributions\n",
       "151041                      probability nonparametric-bayes\n",
       "151043    r nonparametric reporting nonparametric-regres...\n",
       "Name: Tags, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "body_title_tag = body_title_process + ' ' + tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate the tf-idf scores\n",
    "# min_df: When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold\n",
    "# max_features: build a vocabulary that only consider the top 10000 max_features ordered by term frequency across the corpus.\n",
    "vectorizer = TfidfVectorizer(min_df=4, max_features = 10000)\n",
    "vz = vectorizer.fit_transform(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use gensim library to preprocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# only contain words that shows more than 1 time\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "for text in tags:\n",
    "    for word in word_tokenize(text):\n",
    "        frequency[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a function for a text only contains words that appear more than once\n",
    "def text_model(text):\n",
    "    output = []\n",
    "    for word in word_tokenize(text):\n",
    "        if frequency[word] > 1:\n",
    "            output.append(word)\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = tags.map(lambda i: text_model(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# created to store a special object from the corpora module of the gensim library. \n",
    "# This is responsible for indexing the words and assigning them to numbers.\n",
    "dictionary= corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('contoso.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The corpus is now transformed to match the trained TF/IDF model. \n",
    "# Before, the numeric representation was based on the bag of words model (how much times each word appears), \n",
    "# and after the transformation, the numeric representation will be based on the times each word appears in the \n",
    "# whole corpus and scaled by how many individual discussions use that word. \n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# discover topics \n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=5)\n",
    "corpus_lsi = lsi[corpus_tfidf]\n",
    "topics = lsi.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.703*\"regression\" + 0.531*\"r\" + 0.229*\"time-series\" + 0.186*\"logistic\" + 0.117*\"machine-learning\" + 0.102*\"correlation\" + 0.098*\"hypothesis-testing\" + 0.093*\"multiple-regression\" + 0.089*\"self-study\" + 0.074*\"anova\"'), (1, '-0.627*\"regression\" + 0.528*\"r\" + 0.425*\"time-series\" + 0.167*\"probability\" + 0.138*\"hypothesis-testing\" + -0.119*\"logistic\" + 0.119*\"forecasting\" + 0.110*\"distributions\" + 0.077*\"arima\" + 0.072*\"machine-learning\"'), (2, '0.718*\"probability\" + 0.361*\"self-study\" + -0.289*\"r\" + 0.283*\"distributions\" + 0.256*\"hypothesis-testing\" + -0.145*\"time-series\" + 0.131*\"mathematical-statistics\" + 0.128*\"normal-distribution\" + 0.110*\"statistical-significance\" + 0.089*\"conditional-probability\"'), (3, '-0.730*\"time-series\" + 0.440*\"r\" + 0.297*\"machine-learning\" + -0.182*\"hypothesis-testing\" + -0.150*\"forecasting\" + -0.137*\"regression\" + -0.125*\"correlation\" + 0.124*\"classification\" + 0.092*\"probability\" + -0.090*\"arima\"'), (4, '-0.657*\"hypothesis-testing\" + 0.431*\"machine-learning\" + 0.323*\"time-series\" + 0.249*\"probability\" + -0.235*\"statistical-significance\" + -0.164*\"r\" + 0.149*\"classification\" + -0.143*\"anova\" + 0.139*\"neural-networks\" + -0.093*\"t-test\"')]\n"
     ]
    }
   ],
   "source": [
    "print(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for k is 2 score is 0.772903201803\n",
      "for k is 3 score is 0.626117054927\n",
      "for k is 4 score is 0.597956774256\n",
      "for k is 5 score is 0.548168879471\n",
      "for k is 6 score is 0.540534799606\n",
      "for k is 7 score is 0.51457631307\n",
      "for k is 8 score is 0.502970259464\n",
      "for k is 9 score is 0.489731863842\n"
     ]
    }
   ],
   "source": [
    "for i in range(2,10):\n",
    "    kmeans_model = MiniBatchKMeans(n_clusters = i, init='k-means++', n_init=1, \n",
    "                         init_size=1000, batch_size=1000, verbose=False, max_iter=1000, random_state = 0)\n",
    "    kmeans = kmeans_model.fit(vz)\n",
    "    kmeans_clusters = kmeans.predict(vz)\n",
    "    kmeans_distances = kmeans.transform(vz)\n",
    "    X = kmeans_model.fit_transform(vz)\n",
    "    print('for k is', i, 'score is', metrics.silhouette_score(X, kmeans_model.labels_, sample_size=10000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set number of clusters as 8\n",
    "num_clusters = 8\n",
    "kmeans_model = MiniBatchKMeans(n_clusters=num_clusters, init='k-means++', n_init=1, \n",
    "                               init_size=1000, batch_size=1000, verbose=False, max_iter=1000, random_state = 0)\n",
    "kmeans = kmeans_model.fit(vz)\n",
    "kmeans_clusters = kmeans.predict(vz)\n",
    "kmeans_distances = kmeans.transform(vz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: testing hypothesis correlation\n",
      "Cluster 1: regression multiple logistic\n",
      "Cluster 2: data visualization categorical\n",
      "Cluster 3: series time forecasting\n",
      "Cluster 4: value expected hypothesis\n",
      "Cluster 5: model linear mixed\n",
      "Cluster 6: learning machine classification\n",
      "Cluster 7: probability self study\n"
     ]
    }
   ],
   "source": [
    "sorted_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for j in sorted_centroids[i, :3]:\n",
    "        print(' %s' % terms[j], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 10000\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] Error after 100 iterations with early exaggeration: 1.301112\n",
      "[t-SNE] Error after 300 iterations: 1.224848\n"
     ]
    }
   ],
   "source": [
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0)\n",
    "tsne_kmeans = tsne_model.fit_transform(kmeans_distances[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 7, 0], dtype=int32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyaliu/anaconda/lib/python3.5/site-packages/bokeh/io.py:393: UserWarning: save() called but no resources were supplied and output_file(...) was never called, defaulting to resources.CDN\n",
      "  warnings.warn(\"save() called but no resources were supplied and output_file(...) was never called, defaulting to resources.CDN\")\n",
      "/Users/boyaliu/anaconda/lib/python3.5/site-packages/bokeh/io.py:398: UserWarning: save() called but no title was supplied and output_file(...) was never called, using default title 'Bokeh Plot'\n",
      "  warnings.warn(\"save() called but no title was supplied and output_file(...) was never called, using default title 'Bokeh Plot'\")\n"
     ]
    }
   ],
   "source": [
    "# create the plot and save it\n",
    "colormap = np.array([\n",
    "    \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\", \n",
    "    \"#98df8a\", \"#d62728\", \"#ff9896\", \"#9467bd\", \"#c5b0d5\", \n",
    "    \"#8c564b\", \"#c49c94\", \"#e377c2\", \"#f7b6d2\", \"#7f7f7f\", \n",
    "    \"#c7c7c7\", \"#bcbd22\", \"#dbdb8d\", \"#17becf\", \"#9edae5\"\n",
    "])\n",
    "leg = {0:'testing hypothesis correlation',1:'regression multiple logistic', \n",
    "                           2:'data visualization categorical', 3:'series time forecasting',\n",
    "                             4:'value expected hypothesis', 5:'model linear mixed', \n",
    "       6:'learning machine classification',\n",
    "                             7:'probability self study'}\n",
    "\n",
    "plot_kmeans = bp.figure(plot_width=900, plot_height=700, title=\"Stack Overflow Questions (k-means)\",\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "    x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "plot_kmeans.scatter(x=tsne_kmeans[:,0], y=tsne_kmeans[:,1], \n",
    "                    color=colormap[kmeans_clusters][:10000], \n",
    "                    source=bp.ColumnDataSource({\n",
    "                       \"tags\": tags[:10000],\n",
    "                       \"cluster\": kmeans_clusters[:10000]\n",
    "                    }))\n",
    "\n",
    "hover = plot_kmeans.select(dict(type=HoverTool))\n",
    "hover.tooltips={\"tags\": \"@tags (tags: \\\"@tags\\\" - cluster: @cluster)\"}\n",
    "#hover.tooltips={\"questions\": \"@questions (processed: \\\"@processed\\\" - cluster: @cluster)\"}\n",
    "save(plot_kmeans, filename = '/Users/boyaliu/Documents/UCD/STA 208/final project/k8.html')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# uncover the latent topics in the post\n",
    "# and then wet're going to use the topic distributions for each document as a measure to group similar tweets together.\n",
    "# vectorize our data by representing each post as a 10k dimensional vector whose indices correspond to the \n",
    "# 10k most frequent terms in our corpus.\n",
    "cvectorizer = CountVectorizer(min_df=4, max_features=10000, stop_words='english')\n",
    "cvz = cvectorizer.fit_transform(body_title_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for i is 1 the score is -29520933.199726492\n",
      "for i is 2 the score is -32663333.57656804\n",
      "for i is 3 the score is -34585097.17157614\n",
      "for i is 4 the score is -36001729.08554066\n",
      "for i is 5 the score is -37137933.2914421\n",
      "for i is 6 the score is -38092794.60097875\n",
      "for i is 7 the score is -38923540.63248336\n",
      "for i is 8 the score is -39660030.238723025\n",
      "for i is 9 the score is -40324127.966968715\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    lda_model = lda.LDA(n_topics=i, n_iter = 2000, random_state = 0)\n",
    "    lda_model.fit(cvz)\n",
    "    score = lda_model.loglikelihoods_[0]\n",
    "    print('for i is', i, 'the score is', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_topics = 8\n",
    "n_iter = 2000\n",
    "lda_model = lda.LDA(n_topics=n_topics, n_iter=n_iter, random_state = 0)\n",
    "X_topics = lda_model.fit_transform(cvz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: network question problem learning algorithm neural machine data\n",
      "Topic 1: distribution probability random function parameter normal variable likelihood\n",
      "Topic 2: data feature model set classification class validation cross\n",
      "Topic 3: group data analysis variable factor test measure effect\n",
      "Topic 4: model regression variable linear effect data logistic fit\n",
      "Topic 5: time series data model day year event number\n",
      "Topic 6: data correlation matrix cluster point plot distance set\n",
      "Topic 7: test sample mean interval hypothesis standard confidence size\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 8\n",
    "topic_summaries = []\n",
    "\n",
    "topic_word = lda_model.topic_word_  # get the topic words\n",
    "vocab = cvectorizer.get_feature_names()\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 10000\n",
      "[t-SNE] Mean sigma: 0.022845\n",
      "[t-SNE] Error after 100 iterations with early exaggeration: 1.425486\n",
      "[t-SNE] Error after 375 iterations: 1.355540\n"
     ]
    }
   ],
   "source": [
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0)\n",
    "tsne_lda = tsne_model.fit_transform(X_topics[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  get the main topic for each post, which we'll use to colorize them later on:\n",
    "doc_topic = lda_model.doc_topic_\n",
    "lda_keys = []\n",
    "for i, post in enumerate(tags):\n",
    "    lda_keys += [doc_topic[i].argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyaliu/anaconda/lib/python3.5/site-packages/bokeh/io.py:393: UserWarning: save() called but no resources were supplied and output_file(...) was never called, defaulting to resources.CDN\n",
      "  warnings.warn(\"save() called but no resources were supplied and output_file(...) was never called, defaulting to resources.CDN\")\n",
      "/Users/boyaliu/anaconda/lib/python3.5/site-packages/bokeh/io.py:398: UserWarning: save() called but no title was supplied and output_file(...) was never called, using default title 'Bokeh Plot'\n",
      "  warnings.warn(\"save() called but no title was supplied and output_file(...) was never called, using default title 'Bokeh Plot'\")\n"
     ]
    }
   ],
   "source": [
    "colormap = np.array([\n",
    "    \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\", \n",
    "    \"#98df8a\", \"#d62728\", \"#ff9896\", \"#9467bd\", \"#c5b0d5\", \n",
    "    \"#8c564b\", \"#c49c94\", \"#e377c2\", \"#f7b6d2\", \"#7f7f7f\", \n",
    "    \"#c7c7c7\", \"#bcbd22\", \"#dbdb8d\", \"#17becf\", \"#9edae5\"\n",
    "])\n",
    "# plot the picture and save it.\n",
    "plot_lda = bp.figure(plot_width=900, plot_height=700, title=\"Stack Overflow (LDA)\",\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "    x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "plot_lda.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], \n",
    "                 color=colormap[lda_keys][:10000], \n",
    "                 source=bp.ColumnDataSource({\n",
    "                      \"tags\": tags[:10000], \n",
    "                      \"topic_key\": lda_keys[:10000]\n",
    "                }))\n",
    "hover = plot_lda.select(dict(type=HoverTool))\n",
    "hover.tooltips={\"tags\": \"@tags (topic: \\\"@topic_key)\"}\n",
    "save(plot_lda, filename = '/Users/boyaliu/Documents/UCD/STA 208/final project/lda8.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud for Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a list contain all the tags\n",
    "T = tags.tolist()\n",
    "tag_l = []\n",
    "for i in T:\n",
    "    for j in i.split():\n",
    "        tag_l.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bayesian',\n",
       " 'prior',\n",
       " 'elicitation',\n",
       " 'distributions',\n",
       " 'normality',\n",
       " 'software',\n",
       " 'open-source',\n",
       " 'distributions',\n",
       " 'statistical-significance',\n",
       " 'machine-learning',\n",
       " 'dataset',\n",
       " 'sample',\n",
       " 'population',\n",
       " 'teaching',\n",
       " 'humor',\n",
       " 'scales',\n",
       " 'measurement',\n",
       " 'ordinal',\n",
       " 'interval',\n",
       " 'likert',\n",
       " 'multivariable',\n",
       " 'interpolation',\n",
       " 'anova',\n",
       " 'chi-squared',\n",
       " 'generalized-linear-model',\n",
       " 'forecasting',\n",
       " 'population',\n",
       " 'census',\n",
       " 'bayesian',\n",
       " 'frequentist',\n",
       " 'distributions',\n",
       " 'pdf',\n",
       " 'cdf',\n",
       " 'modeling',\n",
       " 'time-series',\n",
       " 'finance',\n",
       " 'software',\n",
       " 'standard-deviation',\n",
       " 'basic-concepts',\n",
       " 'algorithms',\n",
       " 'hypothesis-testing',\n",
       " 'random-variable',\n",
       " 'random-generation',\n",
       " 'hypothesis-testing',\n",
       " 't-test',\n",
       " 'p-value',\n",
       " 'interpretation',\n",
       " 'intuition',\n",
       " 'r',\n",
       " 'seasonality',\n",
       " 'distributions',\n",
       " 'modeling',\n",
       " 'poisson',\n",
       " 'overdispersion',\n",
       " 'correlation',\n",
       " 'teaching',\n",
       " 'modeling',\n",
       " 'bayesian',\n",
       " 'logit',\n",
       " 'transportation',\n",
       " 'algorithms',\n",
       " 'random-variable',\n",
       " 'random-generation',\n",
       " 'data-visualization',\n",
       " 'intuition',\n",
       " 'clustering',\n",
       " 'large-data',\n",
       " 'random-variable',\n",
       " 'intuition',\n",
       " 'definition',\n",
       " 'pattern-recognition',\n",
       " 'pca',\n",
       " 'covariance-matrix',\n",
       " 'correlation-matrix',\n",
       " 'standard-deviation',\n",
       " 'error',\n",
       " 'teaching',\n",
       " 'unbiased-estimator',\n",
       " 'algorithms',\n",
       " 'optimization',\n",
       " 'neural-networks',\n",
       " 'statistical-bias',\n",
       " 'r',\n",
       " 'r',\n",
       " 'references',\n",
       " 'code',\n",
       " 'nonparametric',\n",
       " 'survival',\n",
       " 'hazard',\n",
       " 'time-series',\n",
       " 'garch',\n",
       " 'volatility-forecasting',\n",
       " 'finance',\n",
       " 'summary-statistics',\n",
       " 'likert',\n",
       " 'ordinal',\n",
       " 'signal-processing',\n",
       " 'data-visualization',\n",
       " 'blog',\n",
       " 'multiple-comparisons',\n",
       " 'power',\n",
       " 'machine-learning',\n",
       " 'methodology',\n",
       " 'theory',\n",
       " 'blog',\n",
       " 'standard-deviation',\n",
       " 'definition',\n",
       " 'classification',\n",
       " 'information-retrieval',\n",
       " 'text-mining',\n",
       " 'bayesian',\n",
       " 'references',\n",
       " 'data-visualization',\n",
       " 'r',\n",
       " 'algorithms',\n",
       " 'median',\n",
       " 'r',\n",
       " 'references',\n",
       " 'dataset',\n",
       " 'regression',\n",
       " 'pca',\n",
       " 'bayesian',\n",
       " 'mcmc',\n",
       " 'mixture',\n",
       " 'teaching',\n",
       " 'communication',\n",
       " 'regression',\n",
       " 'time-series',\n",
       " 'bayesian',\n",
       " 'mcmc',\n",
       " 'teaching',\n",
       " 'sample-size',\n",
       " 'polling',\n",
       " 'smoothing',\n",
       " 'kernel-smoothing',\n",
       " 'teaching',\n",
       " 'references',\n",
       " 'r',\n",
       " 'time-series',\n",
       " 'poisson',\n",
       " 'count-data',\n",
       " 'epidemiology',\n",
       " 'regression',\n",
       " 'outliers',\n",
       " 'model-selection',\n",
       " 'neural-networks',\n",
       " 'clustering',\n",
       " 'categorical-data',\n",
       " 'association-measure',\n",
       " 'data-mining',\n",
       " 'hypothesis-testing',\n",
       " 'fitting',\n",
       " 'data-visualization',\n",
       " 'open-source',\n",
       " 't-test',\n",
       " 'scales',\n",
       " 'ordinal',\n",
       " 'likert',\n",
       " 'interval',\n",
       " 'generalized-linear-model',\n",
       " 'optimization',\n",
       " 'continuous-data',\n",
       " 'discrete-data',\n",
       " 'statistical-significance',\n",
       " 'multivariable',\n",
       " 'outliers',\n",
       " 'data-visualization',\n",
       " 'library',\n",
       " 'protovis',\n",
       " 'distributions',\n",
       " 'random-variable',\n",
       " 'minimum',\n",
       " 'pca',\n",
       " 'definition',\n",
       " 'scores',\n",
       " 'references',\n",
       " 'data-visualization',\n",
       " 'software',\n",
       " 'r',\n",
       " 'distributions',\n",
       " 'maximum',\n",
       " 'time-series',\n",
       " 'forecasting',\n",
       " 'standard-deviation',\n",
       " 'variance',\n",
       " 'anova',\n",
       " 'random-effects-model',\n",
       " 'machine-learning',\n",
       " 'boosting',\n",
       " 'data-visualization',\n",
       " 'csv-file',\n",
       " 'machine-learning',\n",
       " 'classification',\n",
       " 'application',\n",
       " 'statistical-significance',\n",
       " 'standard-deviation',\n",
       " 'variance',\n",
       " 'sample',\n",
       " 'population',\n",
       " 'modeling',\n",
       " 'poisson',\n",
       " 'large-data',\n",
       " 'sample-size',\n",
       " 'estimation',\n",
       " 'least-squares',\n",
       " 'maximum-likelihood',\n",
       " 'modeling',\n",
       " 'spatial',\n",
       " 'clustering',\n",
       " 'modeling',\n",
       " 'regression',\n",
       " 'estimation',\n",
       " 'method-of-moments',\n",
       " 'generalized-moments',\n",
       " 'estimation',\n",
       " 'beta-binomial',\n",
       " 'references',\n",
       " 'stata',\n",
       " 'regression',\n",
       " 'distributions',\n",
       " 'data-transformation',\n",
       " 'logarithm',\n",
       " 'regression-strategies',\n",
       " 'variance',\n",
       " 't-test',\n",
       " 'homogeneity',\n",
       " 'distributions',\n",
       " 'modeling',\n",
       " 'poisson',\n",
       " 'binomial',\n",
       " 'disease',\n",
       " 'machine-learning',\n",
       " 'boosting',\n",
       " 'entropy',\n",
       " 'references',\n",
       " 'finance',\n",
       " 'k-nearest-neighbour',\n",
       " 'unbalanced-classes',\n",
       " 'algorithms',\n",
       " 'median',\n",
       " 'large-data',\n",
       " 'standard-deviation',\n",
       " 'least-squares',\n",
       " 'hypothesis-testing',\n",
       " 'maximum-likelihood',\n",
       " 'distributions',\n",
       " 'statistical-significance',\n",
       " 'normality',\n",
       " 'kolmogorov-smirnov',\n",
       " 'references',\n",
       " 'clustering',\n",
       " 'probability',\n",
       " 'data-mining',\n",
       " 'cart',\n",
       " 'probability',\n",
       " 'puzzle',\n",
       " 'multivariate-analysis',\n",
       " 'pivot-table',\n",
       " 'random-generation',\n",
       " 'proof',\n",
       " 'randomness',\n",
       " 'variance',\n",
       " 'monitoring',\n",
       " 'data-visualization',\n",
       " 'best-practices',\n",
       " 'time-series',\n",
       " 'standard-deviation',\n",
       " 'mean',\n",
       " 'uncertainty',\n",
       " 'distributions',\n",
       " 'probability',\n",
       " 'hypothesis-testing',\n",
       " 'mathematical-statistics',\n",
       " 'references',\n",
       " 'outliers',\n",
       " 'bootstrap',\n",
       " 'robust',\n",
       " 'references',\n",
       " 'science',\n",
       " 'humor',\n",
       " 'regression',\n",
       " 'error',\n",
       " 'standard-error',\n",
       " 'time-series',\n",
       " 'data-visualization',\n",
       " 'machine-learning',\n",
       " 'r',\n",
       " 'algorithms',\n",
       " 'cart',\n",
       " 'random-forest',\n",
       " 'time-series',\n",
       " 'forecasting',\n",
       " 'mathematics',\n",
       " 'references',\n",
       " 'mathematical-statistics',\n",
       " 'mixed-model',\n",
       " 'model-selection',\n",
       " 'aic',\n",
       " 'machine-learning',\n",
       " 'classification',\n",
       " 'multiple-comparisons',\n",
       " 'multivariate-analysis',\n",
       " 'feature-selection',\n",
       " 'time-series',\n",
       " 'data-mining',\n",
       " 'signal-processing',\n",
       " 'trend',\n",
       " 'sas',\n",
       " 'regression',\n",
       " 'frequentist',\n",
       " 'multiple-comparisons',\n",
       " 'bayesian',\n",
       " 'mcmc',\n",
       " 'machine-learning',\n",
       " 'mcmc',\n",
       " 'code',\n",
       " 'machine-learning',\n",
       " 'cross-validation',\n",
       " 'hypothesis-testing',\n",
       " 'measurement-error',\n",
       " 'method-comparison',\n",
       " 'correlation',\n",
       " 'causality',\n",
       " 'categorical-data',\n",
       " 'continuous-data',\n",
       " 'data-transformation',\n",
       " 'categorical-data',\n",
       " 'data-transformation',\n",
       " 'optimal-scaling',\n",
       " 'regression',\n",
       " 'anova',\n",
       " 'loss-functions',\n",
       " 'time-series',\n",
       " 'modeling',\n",
       " 'model-comparison',\n",
       " 'regression',\n",
       " 'econometrics',\n",
       " 'instrumental-variables',\n",
       " 'regression',\n",
       " 'econometrics',\n",
       " 'difference-in-difference',\n",
       " 'sem',\n",
       " 'data-visualization',\n",
       " 'classification',\n",
       " 'intuition',\n",
       " 'unsupervised-learning',\n",
       " 'r',\n",
       " 'repeated-measures',\n",
       " 'multiple-comparisons',\n",
       " 'post-hoc',\n",
       " 'sphericity',\n",
       " 'modeling',\n",
       " 'aic',\n",
       " 'cross-validation',\n",
       " 'bic',\n",
       " 'model-selection',\n",
       " 'machine-learning',\n",
       " 'image-processing',\n",
       " 'distributions',\n",
       " 'discrete-data',\n",
       " 'expectation-maximization',\n",
       " 'intuition',\n",
       " 'machine-learning',\n",
       " 'expectation-maximization',\n",
       " 'machine-learning',\n",
       " 'svm',\n",
       " 'regression',\n",
       " 'discriminant-analysis',\n",
       " 'r',\n",
       " 'aic',\n",
       " 'cross-validation',\n",
       " 'bic',\n",
       " 'r',\n",
       " 'spss',\n",
       " 'pca',\n",
       " 'factor-analysis',\n",
       " 'factor-rotation',\n",
       " 'references',\n",
       " 'open-source',\n",
       " 'estimation',\n",
       " 'maximum-likelihood',\n",
       " 'forecasting',\n",
       " 'standard-deviation',\n",
       " 'modeling',\n",
       " 'categorical-data',\n",
       " 'model-selection',\n",
       " 'binary-data',\n",
       " 'machine-learning',\n",
       " 'best-practices',\n",
       " 'dataset',\n",
       " 'survey',\n",
       " 'theory',\n",
       " 'central-limit-theorem',\n",
       " 'data-mining',\n",
       " 'dataset',\n",
       " 'eda',\n",
       " 'machine-learning',\n",
       " 'bayesian',\n",
       " 'references',\n",
       " 'finance',\n",
       " 'correlation',\n",
       " 'text-mining',\n",
       " 'probability',\n",
       " 'teaching',\n",
       " 'mathematical-statistics',\n",
       " 'probability',\n",
       " 'bayesian',\n",
       " 'theory',\n",
       " 'theory',\n",
       " 'survival',\n",
       " 'missing-data',\n",
       " 'software',\n",
       " 'classification',\n",
       " 'dataset',\n",
       " 'multivariable',\n",
       " 'clustering',\n",
       " 'statistical-significance',\n",
       " 'binary-data',\n",
       " 'machine-learning',\n",
       " 'multivariate-analysis',\n",
       " 'image-processing',\n",
       " 'references',\n",
       " 'history',\n",
       " 'bayesian',\n",
       " 'search-theory',\n",
       " 'mixed-model',\n",
       " 'theory',\n",
       " 'machine-learning',\n",
       " 'teaching',\n",
       " 'operations-research',\n",
       " 'data-transformation',\n",
       " 'normality',\n",
       " 'distributions',\n",
       " 'logarithm',\n",
       " 'mean',\n",
       " 'mean',\n",
       " 'rule-of-thumb',\n",
       " 'histogram',\n",
       " 'error',\n",
       " 'standard-error',\n",
       " 'normality',\n",
       " 'data-mining',\n",
       " 'signal-processing',\n",
       " 'curve-fitting',\n",
       " 'wavelet',\n",
       " 'algorithms',\n",
       " 'r',\n",
       " 'parallel-computing',\n",
       " 'multicore',\n",
       " 'stata',\n",
       " 'aic',\n",
       " 'r',\n",
       " 'logistic',\n",
       " 'distributions',\n",
       " 'hypothesis-testing',\n",
       " 'standard-deviation',\n",
       " 'normal-distribution',\n",
       " 'regression',\n",
       " 'computational-statistics',\n",
       " 'model',\n",
       " 'econometrics',\n",
       " 'generalized-linear-model',\n",
       " 'hypothesis-testing',\n",
       " 'anova',\n",
       " 'regression',\n",
       " 'lasso',\n",
       " 'ridge-regression',\n",
       " 'logit',\n",
       " 'logistic',\n",
       " 'hypothesis-testing',\n",
       " 'hypothesis-testing',\n",
       " 'econometrics',\n",
       " 'machine-learning',\n",
       " 'classification',\n",
       " 'cross-validation',\n",
       " 'probability',\n",
       " 'mixture',\n",
       " 'pdf',\n",
       " 'degrees-of-freedom',\n",
       " 'machine-learning',\n",
       " 'maximum-likelihood',\n",
       " 'loss-functions',\n",
       " 'standard-deviation',\n",
       " 'variance',\n",
       " 'normality',\n",
       " 'sample',\n",
       " 'unbiased-estimator',\n",
       " 'logistic',\n",
       " 'machine-learning',\n",
       " 'modeling',\n",
       " 'software',\n",
       " 'real-time',\n",
       " 'dataset',\n",
       " 'outliers',\n",
       " 'expectation-maximization',\n",
       " 'regression',\n",
       " 'hypothesis-testing',\n",
       " 'hypothesis-testing',\n",
       " 'large-data',\n",
       " 'references',\n",
       " 'measurement',\n",
       " 'probability',\n",
       " 'contingency-tables',\n",
       " 'yates-correction',\n",
       " 'time-series',\n",
       " 'outliers',\n",
       " 'signal-processing',\n",
       " 'wavelet',\n",
       " 'r',\n",
       " 'r',\n",
       " 'mean',\n",
       " 'statistical-significance',\n",
       " 'spss',\n",
       " 'logistic',\n",
       " 'generalized-linear-model',\n",
       " 'distributions',\n",
       " 'poisson',\n",
       " 'logistic',\n",
       " 'scales',\n",
       " 'survey',\n",
       " 'ranking',\n",
       " 'careers',\n",
       " 'contingency-tables',\n",
       " 'time-series',\n",
       " 'data-transformation',\n",
       " 'machine-learning',\n",
       " 'classification',\n",
       " 'dataset',\n",
       " 'large-data',\n",
       " 'variance',\n",
       " 'measurement',\n",
       " 'data-visualization',\n",
       " 'distributions',\n",
       " 'spearman',\n",
       " 'spearman-rho',\n",
       " 'paired-data',\n",
       " 'distributions',\n",
       " 'uniform',\n",
       " 'normal-distribution',\n",
       " 'psychometrics',\n",
       " 'reliability',\n",
       " 'elicitation',\n",
       " 'regression',\n",
       " 'teaching',\n",
       " 'distributions',\n",
       " 'kullback-leibler',\n",
       " 'modeling',\n",
       " 'estimation',\n",
       " 'model-selection',\n",
       " 'confidence-interval',\n",
       " 'confidence-interval',\n",
       " 'credible-interval',\n",
       " 'hypothesis-testing',\n",
       " 'correlation',\n",
       " 'mutual-information',\n",
       " 'r',\n",
       " 'survival',\n",
       " 'references',\n",
       " 'r',\n",
       " 'survival',\n",
       " 'stata',\n",
       " 'r',\n",
       " 'time-series',\n",
       " 'modeling',\n",
       " 'aic',\n",
       " 'multivariable',\n",
       " 'probability',\n",
       " 'distributions',\n",
       " 'normality',\n",
       " 'nonparametric',\n",
       " 'multidimensional-scaling',\n",
       " 'correlation',\n",
       " 'data-visualization',\n",
       " 'smoothing',\n",
       " 'distributions',\n",
       " 'algorithms',\n",
       " 'categorical-data',\n",
       " 'count-data',\n",
       " 'normalization',\n",
       " 'regression',\n",
       " 'distributions',\n",
       " 'normality',\n",
       " 'modeling',\n",
       " 'regression',\n",
       " 'categorical-data',\n",
       " 'logistic',\n",
       " 'multiple-comparisons',\n",
       " 'chi-squared',\n",
       " 'time-series',\n",
       " 'outliers',\n",
       " 'mathematical-statistics',\n",
       " 'real-time',\n",
       " 'regression',\n",
       " 'intuition',\n",
       " 'multicollinearity',\n",
       " 'r',\n",
       " 'outliers',\n",
       " 'model-selection',\n",
       " 'nonparametric',\n",
       " 'robust',\n",
       " 'confidence-interval',\n",
       " 'data-visualization',\n",
       " 'hypothesis-testing',\n",
       " 'distributions',\n",
       " 'poisson',\n",
       " 'goodness-of-fit',\n",
       " 'index-decomposition',\n",
       " 'predictive-models',\n",
       " 'pareto-distribution',\n",
       " 'data-imputation',\n",
       " 'distributions',\n",
       " 't-test',\n",
       " 'time-series',\n",
       " 'algorithms',\n",
       " 'real-time',\n",
       " 'data-visualization',\n",
       " 'time-series',\n",
       " 'outliers',\n",
       " 'hypothesis-testing',\n",
       " 'clinical-trials',\n",
       " 'data-visualization',\n",
       " 'control-chart',\n",
       " 'regression',\n",
       " 'roc',\n",
       " 'probability',\n",
       " 'binomial',\n",
       " 'mathematical-statistics',\n",
       " 'modeling',\n",
       " 'best-practices',\n",
       " 'distributions',\n",
       " 'data-mining',\n",
       " 'r',\n",
       " 'correlation',\n",
       " 'efficiency',\n",
       " 'likelihood',\n",
       " 'data-generating-process',\n",
       " 'modeling',\n",
       " 'categorical-data',\n",
       " 'chi-squared',\n",
       " 'r',\n",
       " 'nonparametric',\n",
       " 'anova',\n",
       " 'time-series',\n",
       " 'machine-learning',\n",
       " 'pca',\n",
       " 'data-transformation',\n",
       " 'multivariate-analysis',\n",
       " 'time-series',\n",
       " 'algorithms',\n",
       " 'categorical-data',\n",
       " 'classification',\n",
       " 'correlation',\n",
       " 'count-data',\n",
       " 'epidemiology',\n",
       " 'r',\n",
       " 'nonparametric',\n",
       " 'permutation',\n",
       " 'combination',\n",
       " 'data-visualization',\n",
       " 'pca',\n",
       " 'histogram',\n",
       " 'machine-learning',\n",
       " 'nonparametric',\n",
       " 'cart',\n",
       " 'references',\n",
       " 'probability',\n",
       " 'confidence-interval',\n",
       " 'confidence-interval',\n",
       " 'statistical-significance',\n",
       " 'software',\n",
       " 'epidemiology',\n",
       " 'probability',\n",
       " 'bioinformatics',\n",
       " 'distributions',\n",
       " 'normality',\n",
       " 'sample',\n",
       " 'sample-size',\n",
       " 'r',\n",
       " 'time-series',\n",
       " 'data-visualization',\n",
       " 'ggplot2',\n",
       " 'repeated-measures',\n",
       " 'references',\n",
       " 'humor',\n",
       " 'large-data',\n",
       " 'stata',\n",
       " 'computing',\n",
       " 'experiment-design',\n",
       " 'r',\n",
       " 'probability',\n",
       " 'games',\n",
       " 'correlation',\n",
       " 'mutual-information',\n",
       " 'mathematical-statistics',\n",
       " 'moments',\n",
       " 'intuition',\n",
       " 'directional-statistics',\n",
       " 'sample',\n",
       " 'robust',\n",
       " 'repeated-measures',\n",
       " 'variability',\n",
       " 'forecasting',\n",
       " 'contingency-tables',\n",
       " 'modeling',\n",
       " 'compression',\n",
       " 'missing-data',\n",
       " 'hypothesis-testing',\n",
       " 't-test',\n",
       " 'finance',\n",
       " 'robust',\n",
       " 'estimation',\n",
       " 'maximum-likelihood',\n",
       " 'data-visualization',\n",
       " 'funnel-plot',\n",
       " 'java',\n",
       " 'confidence-interval',\n",
       " 'bootstrap',\n",
       " 'hypothesis-testing',\n",
       " 'logistic',\n",
       " 'link-function',\n",
       " 'r',\n",
       " 'regression',\n",
       " 'mixed-model',\n",
       " 'link-function',\n",
       " 'probability',\n",
       " 'games',\n",
       " 'combination',\n",
       " 'time-series',\n",
       " 'modeling',\n",
       " 'change-point',\n",
       " 'time-series',\n",
       " 'stochastic-processes',\n",
       " 'stationarity',\n",
       " 'r',\n",
       " 'logistic',\n",
       " 'aic',\n",
       " 'error',\n",
       " 'pca',\n",
       " 'stochastic-processes',\n",
       " 'data-transformation',\n",
       " 'large-data',\n",
       " 'regression',\n",
       " 'correlation',\n",
       " 'variance',\n",
       " 'clustering',\n",
       " 'mixture',\n",
       " 'distributions',\n",
       " 'odds-ratio',\n",
       " 'hypothesis-testing',\n",
       " 'multiple-comparisons',\n",
       " 'time-series',\n",
       " 'forecasting',\n",
       " 'dynamic-regression',\n",
       " 'data-transformation',\n",
       " 'econometrics',\n",
       " 'standard-deviation',\n",
       " 'estimation',\n",
       " 'efficiency',\n",
       " 'r',\n",
       " 'categorical-data',\n",
       " 'data-transformation',\n",
       " 'data-visualization',\n",
       " 'clustering',\n",
       " 'software',\n",
       " 'hypothesis-testing',\n",
       " 'code',\n",
       " 'uniform',\n",
       " 'machine-learning',\n",
       " 'svm',\n",
       " 'pca',\n",
       " 'experiment-design',\n",
       " 'normalization',\n",
       " 'microarray',\n",
       " 'distributions',\n",
       " 'probability',\n",
       " 'stochastic-processes',\n",
       " 'data-visualization',\n",
       " 'communication',\n",
       " 'dataset',\n",
       " 'validation',\n",
       " 'hypothesis-testing',\n",
       " 'binomial',\n",
       " 'business-intelligence',\n",
       " 'outliers',\n",
       " 'hypothesis-testing',\n",
       " 'probability',\n",
       " 'stochastic-processes',\n",
       " 'data-mining',\n",
       " 'probability',\n",
       " 'regression',\n",
       " 'residuals',\n",
       " 'diagnostic',\n",
       " 'confidence-interval',\n",
       " 'epidemiology',\n",
       " 'relative-risk',\n",
       " 'multiple-comparisons',\n",
       " 'equivalence',\n",
       " 'correlation',\n",
       " 'mutual-information',\n",
       " 'distributions',\n",
       " 'hypothesis-testing',\n",
       " 'mathematical-statistics',\n",
       " 'stochastic-processes',\n",
       " 'uniform',\n",
       " 'order-statistics',\n",
       " 'minimum',\n",
       " 'time-series',\n",
       " 'multiple-comparisons',\n",
       " 'statistical-significance',\n",
       " 'anova',\n",
       " 'chi-squared',\n",
       " 'statistical-significance',\n",
       " 'independence',\n",
       " 'estimation',\n",
       " 'error',\n",
       " 'shrinkage',\n",
       " 'application',\n",
       " 'steins-phenomenon',\n",
       " 'proportion',\n",
       " 'statistical-significance',\n",
       " 'chi-squared',\n",
       " 'conditional-probability',\n",
       " 'r',\n",
       " 'bayesian',\n",
       " 'maximum-likelihood',\n",
       " 'mcmc',\n",
       " 'pca',\n",
       " 'factor-analysis',\n",
       " 'regression',\n",
       " 'predictor',\n",
       " 'logistic',\n",
       " 'logit',\n",
       " 'link-function',\n",
       " 'correlation',\n",
       " 'r',\n",
       " 'spss',\n",
       " 'stata',\n",
       " 'python',\n",
       " 'data-transformation',\n",
       " 'normality',\n",
       " 'variance-stabilizing',\n",
       " 'cross-validation',\n",
       " 'smoothing',\n",
       " 'splines',\n",
       " 'terminology',\n",
       " 'type-i-errors',\n",
       " 'type-ii-errors',\n",
       " 'bayesian',\n",
       " 'frequentist',\n",
       " 'model-selection',\n",
       " 'information-geometry',\n",
       " 'finance',\n",
       " 'hypothesis-testing',\n",
       " 'mean',\n",
       " 'confidence-interval',\n",
       " 'z-statistic',\n",
       " 'distributions',\n",
       " 'regression',\n",
       " 'normality',\n",
       " 't-test',\n",
       " 'anova',\n",
       " 'hypothesis-testing',\n",
       " 'goodness-of-fit',\n",
       " 'normality',\n",
       " 'small-sample',\n",
       " 'correlation',\n",
       " 'intraclass-correlation',\n",
       " 'aggregation',\n",
       " 'interpretation',\n",
       " 'effect-size',\n",
       " 'variance',\n",
       " 'mixed-model',\n",
       " 'r',\n",
       " 'censoring',\n",
       " 'negative-binomial',\n",
       " 'hypothesis-testing',\n",
       " 'independence',\n",
       " 'definition',\n",
       " 't-test',\n",
       " 'references',\n",
       " 'engineering-statistics',\n",
       " 'references',\n",
       " 'computational-statistics',\n",
       " 'computing',\n",
       " 'information-theory',\n",
       " 'r',\n",
       " 'probability',\n",
       " 'probability',\n",
       " 'standard-deviation',\n",
       " 'r',\n",
       " 'finance',\n",
       " 'maximum',\n",
       " 'pca',\n",
       " 'genetics',\n",
       " 'distributions',\n",
       " 'data-visualization',\n",
       " 'data-transformation',\n",
       " 'confidence-interval',\n",
       " 't-test',\n",
       " 'interpretation',\n",
       " 'bayesian',\n",
       " 'logistic',\n",
       " 'r',\n",
       " 't-test',\n",
       " 'post-hoc',\n",
       " 'latex',\n",
       " 'regression',\n",
       " 'survival',\n",
       " 'python',\n",
       " 'r',\n",
       " 'categorical-data',\n",
       " 'aggregation',\n",
       " 'plyr',\n",
       " 'probability',\n",
       " 'binomial',\n",
       " 'negative-binomial',\n",
       " 'data-visualization',\n",
       " 'standard-deviation',\n",
       " 'r',\n",
       " 'networks',\n",
       " 'references',\n",
       " 'data-visualization',\n",
       " 'logarithm',\n",
       " 'precision-recall',\n",
       " 'regression',\n",
       " 'aic',\n",
       " 'feature-selection',\n",
       " 'r',\n",
       " 'text-mining',\n",
       " 'correlation',\n",
       " 'estimation',\n",
       " 'censoring',\n",
       " 'covariance-matrix',\n",
       " 'unbiased-estimator',\n",
       " 'r',\n",
       " 'confidence-interval',\n",
       " 'variance',\n",
       " 'generalized-linear-model',\n",
       " 'bootstrap',\n",
       " 'standard-deviation',\n",
       " 'statistical-significance',\n",
       " 'standard-error',\n",
       " 'r',\n",
       " 'confidence-interval',\n",
       " 'mixed-model',\n",
       " 'repeated-measures',\n",
       " 'sas',\n",
       " 'non-independent',\n",
       " 'spss',\n",
       " 'stata',\n",
       " 'contingency-tables',\n",
       " 'fishersexact',\n",
       " 't-test',\n",
       " 'standard-deviation',\n",
       " 'small-sample',\n",
       " 'r',\n",
       " 'algorithms',\n",
       " 'factor-analysis',\n",
       " 'psychometrics',\n",
       " 'matlab',\n",
       " 'hypothesis-testing',\n",
       " 'genetic-algorithms',\n",
       " 'references',\n",
       " 'experiment-design',\n",
       " 'anova',\n",
       " 'repeated-measures',\n",
       " 'sample-size',\n",
       " 'power',\n",
       " 'survey',\n",
       " 'cross-validation',\n",
       " 'regression',\n",
       " 'least-squares',\n",
       " 'r',\n",
       " 'time-series',\n",
       " 'data-visualization',\n",
       " 'entropy',\n",
       " 'r',\n",
       " 'time-series',\n",
       " 'hypothesis-testing',\n",
       " 'spss',\n",
       " 'entropy',\n",
       " 'survival',\n",
       " 'hazard',\n",
       " 'bayesian',\n",
       " 'effect-size',\n",
       " 'cohens-d',\n",
       " 'hypothesis-testing',\n",
       " 'distributions',\n",
       " 'bootstrap',\n",
       " 'moments',\n",
       " 'l-moments',\n",
       " 'machine-learning',\n",
       " 'feature-selection',\n",
       " 'regression',\n",
       " 'data-transformation',\n",
       " 'categorical-data',\n",
       " 'variance',\n",
       " 'unbalanced-classes',\n",
       " 'repeated-measures',\n",
       " 'probability',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_join = ' '.join(tag_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take relative word frequencies into account, lower max_font_size\n",
    "wordcloud = WordCloud(max_font_size=40, relative_scaling=.5, background_color=\"white\").generate(tag_join)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
